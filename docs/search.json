[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Python Blog",
    "section": "",
    "text": "Sıralama Öğesi\n       Varsayılan\n         \n          Tarih - En eski\n        \n         \n          Tarih - En yeni\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nTemel Bileşenler Analizi ile 2024 Avrupa Futbol Şampiyonası İlk Grup Maçları Sonrası Takım Konumlarının Belirlenmesi\n\n\n\n\n\n\n\n\nHaziran 19, 2024\n\n\n5 dakika\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Dağılımı ile 2024 Avrupa Futbol Şampiyonası Grup Maçları Öngörüleri\n\n\n\n\n\n\n\n\nHaziran 17, 2024\n\n\n10 dakika\n\n\n\n\n\n\n\n\n\n\n\n\nTürkçe Finans Haberlerinde Duygu Sınıflandırması\n\n\n\n\n\n\n\n\nHaziran 14, 2024\n\n\n3 dakika\n\n\n\n\n\n\n\n\n\n\n\n\nİndikatör Ağırlıklarının Optimize Edilerek Teknik Reytinglerin Oluşturulması ve Al-Tut Stratejisinin Alt Edilmesi\n\n\n\n\n\n\n\n\nMayıs 25, 2024\n\n\n44 dakika\n\n\n\n\n\n\n\n\n\n\n\n\nTCMB’nin Faiz Oranlarına İlişkin Duyuru Metinlerinin Okunabilirliği\n\n\n\n\n\n\n\n\nMayıs 14, 2024\n\n\n4 dakika\n\n\n\n\n\n\n\n\n\n\n\n\nTCMB’nin Faiz Oranlarına İlişkin Duyuru Metinlerinin İçerik Analizi\n\n\n\n\n\n\n\n\nMayıs 4, 2024\n\n\n7 dakika\n\n\n\n\n\n\n\n\n\n\n\n\nTCMB Başkan Yardımcısı Cevdet Akçay’ın ‘Link Kopmuş’ Dediği Konuşmasının Duygu Analizi\n\n\n\n\n\n\n\n\nNisan 25, 2024\n\n\n2 dakika\n\n\n\n\n\n\n\n\n\n\n\n\nRutin Görevlerin Otomatize Edilmesi: Outlook ve Görev Zamanlayıcının Kullanılması\n\n\n\n\n\n\n\n\nNisan 23, 2024\n\n\n2 dakika\n\n\n\n\n\n\n\n\n\n\n\n\nTCMB/EVDS Verilerine Erişim (05/04/2024 Değişikliği Sonrası)\n\n\n\n\n\n\n\n\nNisan 21, 2024\n\n\n2 dakika\n\n\n\n\n\n\n\n\n\n\n\n\nTürkiye ve Çevresinde Gerçekleşen Depremlerin Animasyonlu Harita ile Gösterilmesi\n\n\n\n\n\n\n\n\nNisan 20, 2024\n\n\n4 dakika\n\n\n\n\n\n\n\n\n\n\n\n\nReddit Başlıklarının Duygu Analizi: r/worldnews Örneği\n\n\n\n\n\n\n\n\nNisan 14, 2024\n\n\n3 dakika\n\n\n\n\n\n\n\n\n\n\n\n\nİstanbul İlçe Belediye Başkanlarının X (Twitter) Profillerindeki Duygu Dağılımı\n\n\n\n\n\n\n\n\nNisan 13, 2024\n\n\n3 dakika\n\n\n\n\n\n\n\n\n\n\n\n\nPartilere Göre Milletvekillerinin Ortalama Yüzü: Türkiye Örneği\n\n\n\n\n\n\n\n\nNisan 10, 2024\n\n\n2 dakika\n\n\n\n\n\n\n\n\n\n\n\n\nSeçim Verilerinin Türkiye Haritasında Görselleştirilmesi\n\n\n\n\n\n\n\n\nNisan 8, 2024\n\n\n3 dakika\n\n\n\n\n\n\n\n\n\n\n\n\nAnlık Veri Akışı ile Oy Öngörüsü\n\n\n\n\n\n\n\n\nNisan 7, 2024\n\n\n9 dakika\n\n\n\n\n\n\n\n\n\n\n\n\nKorelasyon Tahmin Oyunu\n\n\n\n\n\n\n\n\nMart 24, 2024\n\n\n4 dakika\n\n\n\n\n\n\n\n\n\n\n\n\nPython Dünyama Hoş Geldin!\n\n\n\n\n\n\n\n\nMart 23, 2024\n\n\n1 dakika\n\n\n\n\n\n\nEşleşen öğe yok"
  },
  {
    "objectID": "posts/post_8/index.html",
    "href": "posts/post_8/index.html",
    "title": "Türkiye ve Çevresinde Gerçekleşen Depremlerin Animasyonlu Harita ile Gösterilmesi",
    "section": "",
    "text": "Giriş\nAnimasyonlu haritalar, zaman içinde değişen verileri görselleştiren etkili bir yöntemdir. Örneğin, Türkiye ve çevresinde gerçekleşen depremleri animasyonlu bir harita üzerinde göstermek, deprem aktivitesinin zamanla nasıl değiştiğini ve hangi bölgelerin daha fazla risk altında olduğunu görsel olarak görmemize yardımcı olabilir.\nAnimasyonlu haritayı oluşturmak için kullanacağımız verilere USGS (United States Geological Survey) API ile ulaşacağız.\n\n\nKullanılacak Kütüphaneler\n\nimport requests\nimport pandas as pd\nimport os\nimport time\nimport folium\nfrom folium.plugins import HeatMap\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\nfrom PIL import Image, ImageDraw, ImageFont\nfrom pathlib import Path\nimport imageio\nimport shutil\n\n\n\nAPI ile Verilerin Çekilmesi ve Kullanılabilir Formata Dönüştürülmesi\n\nstart = '2014-01-01'\nmin_mag = 3\nlat = 39.1458\nlon = 34.1614\nmax_rad_km = 1000\n\nurl = f'https://earthquake.usgs.gov/fdsnws/event/1/query?format=geojson&starttime={start}&minmagnitude={min_mag}&latitude={lat}&longitude={lon}&maxradiuskm={max_rad_km}'\nresponse = requests.get(url)\ndata = response.json()\n\ndf = pd.DataFrame({\n    'Place': [feature['properties']['place'] for feature in data['features']],\n    'Magnitude': [feature['properties']['mag'] for feature in data['features']],\n    'Time': [pd.to_datetime(feature['properties']['time'], unit='ms').strftime('%Y-%m-%d') for feature in data['features']],\n    'Latitude': [feature['geometry']['coordinates'][1] for feature in data['features']],\n    'Longitude': [feature['geometry']['coordinates'][0] for feature in data['features']]\n})\n\nYukarıdaki kodda, verilen parametrelerle USGS API’ını kullanarak deprem verilerini çekiyoruz. Başlangıç tarihini (start) '2014-01-01' ve minimum deprem büyüklüğünü (min_mag) 3 olarak belirledik. Ayrıca, enlem (lat) 39.1458 ve boylam (lon) 34.1614 olacak şekilde Türkiye’nin koordinatlarına yakın ve maksimum 1000 kilometrelik yarıçapa (max_rad_km) sahip bir bölgeyi sorguluyoruz. Son olarak, API’dan gelen verileri işleyerek bir DataFrame oluşturuyoruz. Bu DataFrame, depremlerin yerini (Place), büyüklüğünü (Magnitude), zamanını (Time) ve enlem-boylam koordinatlarını (Latitude-Longitude) içeriyor.\nAylık bir seri ile çalışacağımız için tarihlerdeki (Time sütunu) günleri 01 ile değiştiriyoruz.\n\ndf['Time'] = df['Time'].str.replace(r'-\\d{2}$', '-01', regex=True)\n\n\n\nHaritaların Yapılması ve HTML-PNG Formatlarında Kaydedilmesi\n\nchrome_options = Options()\nchrome_options.add_argument('--start-fullscreen')\n\nunique_dates = df['Time'].unique()\nunique_dates.sort()\n\nturkey_latlon = [39, 35]\n\ndelay = 5\n\nfont = ImageFont.load_default()\nfont_size = 36\n\nfor unique_date in unique_dates:\n\n    filtered_df = df[df['Time'] == unique_date]\n    filtered_df = filtered_df[['Latitude', 'Longitude', 'Magnitude']]\n    turkey_map = folium.Map(location=turkey_latlon, zoom_start=6, tiles='cartodbdark_matter')\n    HeatMap(data=filtered_df, radius=15).add_to(turkey_map)\n\n    html_filename = f'turkey_heatmap_{unique_date}.html'\n    turkey_map.save(html_filename)\n\n    browser = webdriver.Chrome(options=chrome_options)\n    browser.get(os.path.abspath(html_filename))\n    time.sleep(delay)\n\n    screenshot_filename = f'turkey_heatmap_{unique_date}.png'\n    browser.save_screenshot(screenshot_filename)\n\n    browser.quit()\n\n    img = Image.open(screenshot_filename)\n    draw = ImageDraw.Draw(img)\n    font = ImageFont.truetype('arial.ttf', font_size)\n    draw.text((10, img.height - 50), pd.to_datetime(unique_date).strftime('%B %Y'), font=font, fill=(255, 255, 255))\n    img.save(screenshot_filename)\n\n    os.remove(html_filename)\n    print(f'{html_filename} loaded in the browser and screenshot {screenshot_filename} captured.')\n\nYukarıdaki kodda, her bir tarih döngüde kullanılacağı için tekil tarihleri unique_dates değişkenine gönderip bu tarihleri eskiden yeniye doğru olacak şekilde sıralıyoruz. Türkiye’nin koordinatlarını tanımladığımız ve haritaların merkezi olacak değerler turkey_latlon değişkeninde bulunuyor. Her işlemde 5 saniyelik bir bekleme süresi olacak. Bunu delay değişkeninde tutuyoruz. Resimlerin üzerinde görünecek yazılara ait varsayılan font ve font büyüklüğü değerleri olan arial.ttf ve 36 sırasıyla font ve font_size değişkenlerinde bulunuyor. Her döngüde açılacak tarayıcıların ekranı kapsayacak şekilde olmasını istediğimiz için '--start-fullscreen' olacak şekilde ayarlama da yaptık.\nDöngüde 7 kod grubu bulunmaktadır. İlk grup, tarih filtresi yapıp haritayı oluşturuyor. İkinci grup, haritayı içinde bulunduğu dizine HTML formatında kaydediyor. Üçüncü grup, HTML formatında kaydedilen dosyayı tarayıcıda açıyor ve açtıktan sonra belirlenen süre kadar bekletiyor. Dördüncü grup, ekran görüntüsü alıyor ve içinde bulunduğu dizine PNG formatında kaydediyor. Beşinci grup, açılan tarayıcıyı kapatıyor. Altıncı grup, resim üzerinde yazı işlemlerini yapıyor. Yedinci ve son grup, kaydedilen HTML dosyalarını siliyor ve ekrana bilgi veriyor.\n\n\nAnimasyonlu Harita Yapımı ve GIF Formatında Kaydedilmesi\nÇalışmanın odak noktasını bu başlık altında göreceğiz.\n\nimage_path = Path()\nimages = list(image_path.glob('*.png'))\nimage_list = [imageio.v3.imread(file_name) for file_name in images]\nimageio.mimwrite('Turkey_Earthquake.gif', image_list, fps=2)\n_ = [file.unlink() for file in images]\n\nshutil.move('Turkey_Earthquake.gif', 'imgs/Turkey_Earthquake.gif')\n\nYukarıdaki kodda ilk olarak, Path() fonksiyonunu çağırarak bir dosya yolu nesnesi oluşturuyor ve bunu image_path değişkenine atıyoruz. Ardından, bu dosya yolu nesnesi üzerinde .glob() yöntemini kullanarak tüm PNG dosyalarını alıyor ve images listesine atıyoruz. imageio modülünü kullanarak her bir PNG dosyasını imageio.v3.imread() fonksiyonuyla okuyor ve bu okunan görüntüleri image_list listesine ekliyoruz. imageio.mimwrite() fonksiyonuyla image_list içindeki görüntüleri kullanarak bir GIF dosyası oluşturuyoruz. Oluşturduğumuz GIF dosyasının ismini 'Turkey_Earthquake.gif' olarak belirliyor ve saniyede 2 kare (fps=2) hızında olacak şekilde ayarlıyoruz. Daha sonra, artık gereksiz hale gelmiş olan PNG dosyalarını tek tek sildiriyoruz. Bu işlem için bir liste dönülüyor ve her bir dosya unlink() yöntemi kullanılarak siliniyor. Son olarak, shutil.move() fonksiyonunu kullanarak oluşturduğumuz GIF dosyasını 'imgs/' dizini altına taşıyoruz.\n\nGelecek içeriklerde görüşmek dileğiyle."
  },
  {
    "objectID": "posts/post_6/index.html",
    "href": "posts/post_6/index.html",
    "title": "İstanbul İlçe Belediye Başkanlarının X (Twitter) Profillerindeki Duygu Dağılımı",
    "section": "",
    "text": "Giriş\nYapılan çalışmaların yanında içinde profil fotoğrafının da olduğu sosyal medya kullanımı da önemli olabiliyor.\nBu uygulamada, İstanbul ilçe belediye başkanlarının X (Twitter) profillerindeki duygularını inceleyeceğiz. Duyguların tespitinde Py-Feat kütüphanesinden faydalanacağız.\n\n\nKullanılacak Kütüphaneler\n\nimport os\nimport pandas as pd\nfrom feat import Detector\nfrom feat.plotting import imshow\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n\nFotoğrafların İçe Aktarılması\nBuradan ulaşabileceğiniz fotoğraflar ilçe belediye başkanlarının X (Twitter) hesaplarından indirilmiştir.\n\nimage_folder = 'ilce_bb_profil/'\nimage_files = [f for f in os.listdir(image_folder) if f.endswith('.jpg')]\n\n\n\nDuyguların Tespit Edilmesi ve Görselleştirilmesi\nPy-Feat, görüntülerden ve videolardan yüz ifadelerini kolayca tespit etmek, yüz ifadesi verilerini önceden işlemek ve analiz etmek ve yüz ifadesi verilerini görselleştirmek için kapsamlı bir araç ve model seti sağlar. Py-Feat önceden eğitilmiş çeşitli modelleri içeriyor.\n\ndetector = Detector(\n    face_model='retinaface', # face detection\n    landmark_model='mobilefacenet', # facial landmark detection\n    facepose_model='img2pose', # facial pose estimation\n    au_model='xgb', # action unit detection\n    emotion_model='resmasknet', # emotion detection\n)\n\n\nYüz tespiti (Face detection, face_model): Bir görüntüdeki veya bir videodaki insan yüzlerini algılama sürecidir. Yüz tespiti, bir görüntü içindeki yüz bölgelerini belirlemek için kullanılır. Parametre seçenekleri şunlardır: retinaface, mtcnn, faceboxes, img2pose ve img2pose-c.\nYüz belirleme (Facial landmark detection, landmark_model): Yüzün belirli noktalarını (örneğin, gözler, burun, ağız) tespit etme sürecidir. Bu noktalar genellikle gözbebekleri, burun ucundaki nokta, dudakların kenarları gibi önemli anatomik yerlerdir. Parametre seçenekleri şunlardır: mobilefacenet, mobilenet ve pfld.\nYüz poz tahmini (Facial Pose estimation, facepose_model): Bir yüzün konumunu ve/veya dönüşünü belirleme sürecidir. Yüzün kaç derece eğik olduğunu, hangi yönde baktığını tahmin etmeyi içerir. Parametre seçenekleri şunlardır: img2pose ve img2pose-c.\nHareket Birimi tespiti (Action Unit detection, au_model): Yüz ifadelerindeki belirli kas gruplarını temsil eden hareket birimlerini (action units) tanımlama sürecidir. Örneğin, kaşların kaldırılması, dudakların büzülmesi gibi. Parametre seçenekleri şunlardır: xgb ve svm.\nDuygu tespiti (Emotion detection, emotion_model): Bir kişinin yüz ifadesinden duygusal durumunu belirleme sürecidir. Örneğin, mutlu, üzgün, kızgın gibi duyguları tanımlama. Parametre seçenekleri şunlardır: resmasknet ve svm.\n\nBir döngü ile tüm fotoğrafları dahil edeceğiz ama öncesinde örnek bir fotoğraf ile sürece bakalım.\n\nexample_img = 'uskudar_chp.jpg'\nimshow(image_folder + example_img)\n\n\ndetect_image() ile yüklenen modelleri kullanalım.\n\nsingle_face_prediction = detector.detect_image(image_folder + example_img)\n\n\nBuradan duyguları çekelim.\n\nemotions = single_face_prediction.emotions\n\n\nPy-Feat görselleştirme imkanı da sunuyor. Bunu iki farklı şekilde yapabiliriz.\n\nfigs = single_face_prediction.plot_detections(poses=True)\n\n\n\nfigs = single_face_prediction.plot_detections(faces='aus', muscles=True)\n\n\nTüm fotoğraflar için duyguları tespit edelim. Görseli ısı haritası ile yapacağız.\nÖncesinde image_files değişkenine tüm fotoğrafları aktarmıştık. Bunu kullanabiliriz.\n\nemotions_df = pd.DataFrame()\n\nfor image_file in image_files:\n    face_prediction = detector.detect_image(image_folder + image_file)\n    face_prediction_final = face_prediction[face_prediction['FaceScore'] == face_prediction['FaceScore'].max()]\n    emotions = face_prediction_final.emotions\n    ilce, parti = image_file.split('_')\n    parti = parti.split('.')[0]\n    emotions['image_file'] = ilce.upper() + '-' + parti.upper()\n    emotions_df = pd.concat([emotions_df, emotions], ignore_index=True)\n\nemotions_df = emotions_df.set_index('image_file')\nemotions_df.columns = emotions_df.columns.str.upper()\n\nDöngüde bulunan face_prediction_final değişkenini birden fazla yüz tespiti olduğu için oluşturdum. Büyükçekmece belediye başkanının profilinde kedi de bulunmaktadır.\n\nexample_img = 'buyukcekmece_chp.jpg'\nimshow(image_folder + example_img)\n\n\nIsı haritası ile duyguları gösterebiliriz.\n\nplt.figure(figsize=(10, 12))\nsns.heatmap(emotions_df, cmap='Reds', annot=False, linewidths=.5)\nplt.title('İstanbul İlçe Belediye Başkanları X (Twitter) Profilleri Duygu Dağılımı')\nplt.tick_params(left=False, bottom=False)\nplt.ylabel('')\nplt.show()\n\n\nProfil fotoğraflarında doğal olarak en güçlü duygunun happiness olduğunu görüyoruz. neutral pozlar da kendini göstermektedir.\nGelecek içeriklerde görüşmek dileğiyle."
  },
  {
    "objectID": "posts/post_4/index.html",
    "href": "posts/post_4/index.html",
    "title": "Seçim Verilerinin Türkiye Haritasında Görselleştirilmesi",
    "section": "",
    "text": "Giriş\n31 Mart 2024 yerel seçimlerini geride bıraktık ancak biz Türkiye’de yaşayanlar için iki seçim arası kısa bir reklam arası gibi. Önümüzdeki seçimlere hazırlıklı olmak için bu arayı değerlendirmeye devam ediyoruz.\nBu uygulamada, CHP’nin iller bazında aldığı oyları harita üzerinde göstereceğiz. local_elections_province_20240331 isimli JSON dosyasında bulunan verilere buradan ulaşabilirsiniz.\nHarita üzerinde görselleştirme yapmak için geopandas kütüphanesini kullanacağız. geopandas, isminden de anlaşılacağı üzere, popüler veri bilimi kütüphanesi pandas’ı jeo-uzamsal veriler ile destekleyip genişletiyor.\nHaritada görselleştirmek için öncelikle Türkiye’nin .shp uzantılı dosyasını bulmamız gerekiyor. .shp, coğrafi verileri depolamak için kullanılan bir vektör veri formatıdır. Buradan turkey_administrativelevels0_1_2.zip isimli dosyayı indirebilirsiniz. Eğer bir problem ile karşılaşırsanız burada bulunan dosyayı da indirebilirsiniz.\n\n\nKullanılacak Kütüphaneler\n\nimport json\nimport pandas as pd\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\n\n\nVerilerin Hazırlanması\nread_file() fonksiyonu ile .shp uzantılı dosyayı içeri aktaralım.\n\nshapefile_tr = gpd.read_file('turkey_administrativelevels0_1_2/tur_polbnda_adm1.shp')\n\nprint(shapefile_tr)\n\n\nshapefile_tr ile birleştireceğimiz seçim verilerinin yer aldığı JSON dosyasını içe aktaralım.\n\nwith open('local_elections_province_20240331.json', 'r', encoding='utf-8') as file:\n    data = json.load(file)\n\ndf = pd.DataFrame(data['Data'])\n\nprint(df)\n\n\nshapefile_tr tablosundaki adm1_tr sütunu ile df tablosundaki PROVINCE sütununu kullanarak iki tabloyu birleştireceğiz.\n\nmerged_df = pd.merge(\n  shapefile_tr,\n  df,\n  left_on='adm1_tr',\n  right_on='PROVINCE',\n  how='left'\n)\n\n\nBirleştirdikten sonra harita aşamasına geçebiliriz.\n\n\nHaritanın Oluşturulması\nHaritayı en temiz haliyle görelim.\n\nfig, ax = plt.subplots(figsize = (10,10))\nmerged_df.plot(ax=ax)\nplt.show()\n\n\nŞimdi verileri haritaya gönderelim ve görselin daha profesyonel görünmesini sağlayalım.\n\nfig, ax = plt.subplots(figsize = (10,10))\nmerged_df.plot(ax=ax, column='CHP', cmap='Reds')\nax.axis('off')\nax.set_title(\n    '31 Mart 2024 Yerel Seçimleri - Cumhuriyet Halk Partisi Oy Dağılımı',\n    fontdict = {'fontsize': 8}\n)\nax.text(\n    0.95,\n    0.01,\n    \"Veriler Yeni Şafak'ın web sitesinden alınmıştır.\",\n    color='gray',\n    fontsize=6,\n    fontstyle='italic',\n    ha='right',\n    va='bottom',\n    transform=ax.transAxes\n)\nplt.show()\n\n\nNeler yaptık? İnceleyelim.\nÖncelikle, plt.subplots() ile bir Figure ve Axes nesnesi oluşturuyoruz. Sonrasında, merged_df isimli DataFrame’den gelen verileri kullanarak harita oluşturuyoruz ve bu işlemi plot() ile gerçekleştiriyoruz. column parametresi haritada renk kodlaması yapılacak sütunu belirtirken, cmap parametresi renk haritasını belirler. Sadece haritanın görüntülenmesini sağlamak için ax.axis() ile eksenleri kapalı hale getiriyoruz. ax.set_title() ile haritanın başlığını ayarlıyoruz. Son olarak, ax.text() ile sağ alt köşeye bir metin ekliyoruz ve plt.show() ile grafiği görüntülüyoruz.\nYukarıda sürekli verileri kullandık. Peki, kategorik verileri harita üzerinde nasıl gösterebiliriz?\nCHP’nin %50’den az ve çok aldığı illeri görselleştirmek istediğimizi varsayalım.\n\nmerged_df['CHP_50'] = merged_df['CHP'].apply(\n  lambda x: '%50\\'den az' if x &lt; 50 else '%50\\'den çok'\n)\n\n\n\ncolors = {\n    \"%50'den az\": \"gray\",\n    \"%50'den çok\": \"black\"\n}\n\ncmap = ListedColormap(list(colors.values()))\n\nfig, ax = plt.subplots(figsize = (10,10))\nmerged_df.plot(\n  ax=ax,\n  column='CHP_50',\n  cmap=cmap,\n  legend=True,\n  legend_kwds={'loc': 'lower left', 'fontsize': 8}\n)\nax.axis('off')\nax.set_title(\n    \"31 Mart 2024 Yerel Seçimleri - Cumhuriyet Halk Partisi'nin %50'den Az ve Çok Aldığı İller\",\n    fontdict = {'fontsize': 8}\n)\nax.text(\n    0.95,\n    0.01,\n    \"Veriler Yeni Şafak'ın web sitesinden alınmıştır.\",\n    color='gray',\n    fontsize=6,\n    fontstyle='italic',\n    ha='right',\n    va='bottom',\n    transform=ax.transAxes\n)\nplt.show()\n\n\nFarklı neler yaptık? İnceleyelim.\ncolors isminde bir sözlük tanımladık. Bu sözlük, görselleştirmede kullanılacak renkleri ve bunlara karşılık gelen kategorileri içeriyor. Ayrıca, cmap isminde bir ListedColormap nesnesi oluşturduk. Bu, renk paletini belirtiyor ve colors sözlüğünden alınan renklerle oluşturuluyor. Önceki haritada olmayan, bu haritada sol alt köşede bulunan lejant ise kategorilerin tanımlarını içeriyor.\nGelecek içeriklerde görüşmek dileğiyle."
  },
  {
    "objectID": "posts/post_2/index.html",
    "href": "posts/post_2/index.html",
    "title": "Korelasyon Tahmin Oyunu",
    "section": "",
    "text": "Giriş\nKorelasyon tahmin oyunu yapımında nelere ihtiyacımız olabilir?\nBirincisi, iki adet rassal seri üretmeliyiz ve bu serileri üretirken korelasyon katsayısını dikkate almalıyız. Buradaki korelasyon katsayısı Pearson’ı ifade etmektedir. Rassal serileri üretmek için numpy kütüphanesinin np.random.multivariate_normal fonksiyonundan faydalanabiliriz.\nİkincisi, kullanıcı için bir skorlama yapmalıyız. Bunun için RMSE (Root Mean Squared Error, Kök Ortalama Kare Hatası) metriğini kullanabiliriz. RMSE skoru düştükçe başarı artacaktır.\nÜçüncüsü, kullanıcı ile etkileşimde olmalıyız. Etkileşim için Streamlit ile bir web uygulaması yapabiliriz. Uygulamayı lokalde çalıştıracağız.\n\n\nKullanılacak Kütüphaneler\n\nimport streamlit as st\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\nKorelasyonlu Rassal Serilerin Üretilmesi\n\ndef generate_correlated_data():\n    num_points = int(np.random.uniform(low=100, high=1000))\n    rho = round(np.random.uniform(low=-1, high=1), 2)\n    cov_matrix = np.array([[1, rho], [rho, 1]])\n    mu = [0, 0]\n    correlated_data = np.random.multivariate_normal(mean=mu, cov=cov_matrix, size=num_points)\n    return correlated_data, rho\n\nFonksiyonumuzun ismini generate_correlated_data olarak belirledik ve fonksiyonumuzun herhangi bir parametresi bulunmamaktadır.\nnum_points değişkeni, 100 ile 1000 arasında olmak üzere rassal olarak üretilecek serilere ait verilerin uzunluğunu temsil etmektedir. Bu değeri üretebilmek için numpy kütüphanesinin np.random.uniform fonksiyonunu kullandık ve int() ile değerin tam sayı veri tipinde olmasını sağladık.\nrho değişkeni, -1 ile 1 arasında rassal olarak olarak üretilmiş bir korelasyon katsayısını temsil etmektedir. Bu değeri üretebilmek için np.random.uniform fonksiyonunu kullandık. rho değerini noktadan sonra iki rakam gelecek şekilde ayarladık.\ncov_matrix değişkeni, 2x2’lik bir kovaryans matrisini temsil etmektedir.\nmu değişkeni, veri kümesinin her bir boyutu için belirlenen ortalama değeri temsil etmektedir. Bu değer 0 olacağı için X ve Y [0, 0]’dır.\ncorrelated_data değişkeni, np.random.multivariate_normal fonksiyonu yardımıyla üretilen verileri temsil etmektedir. Bu fonksiyon, çok değişkenli bir normal dağılımdan rassal örnekler üretir. Fonksiyonun içerisine parametre olarak ortalama, kovaryans matrisi ve örnek büyüklüğü girilir.\ngenerate_correlated_data fonksiyonu bize correlated_data ve rho değerlerini dönüyor. correlated_data değişkenindeki ilk seriye correlated_data[:,0]; ikinci seriye correlated_data[:,1] ile ulaşılabilir.\nFonksiyonun döndüğü değerleri kullanarak bir görselleştirme yapalım.\n\ncorrelated_data, rho = generate_correlated_data()\n\n# print(f'Randomly selected correlation: {rho}')\n\nplt.figure(figsize=(8, 6))\nplt.scatter(correlated_data[:,0], correlated_data[:,1], alpha=0.7)\nplt.title('Scatter Plot of Correlated Data')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.grid(True)\nplt.show()\n\n\nBir diğer fonksiyonumuz olan ve RMSE değerini hesaplayan calculate_rmse fonksiyonuna bakalım.\n\\(\\mathrm{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\\)\n\ndef calculate_rmse(predictions, actuals):\n    mse = np.mean((predictions - actuals) ** 2)\n    rmse = round(np.sqrt(mse), 4)\n    return rmse\n\ncalculate_rmse fonksiyonu, predictions ve actuals olmak üzere 2 adet parametre alıyor. Önce mse değişkenine tahminler ile gerçek değerler arasındaki farkların karelerinin ortalamasını gönderiyoruz. Ardından da bu ortalamanın karekökünü rmse değişkenine atıyoruz ve bu değeri noktadan sonra 4 rakam olacak şekilde döndürüyoruz.\nKodların ana yapısını oluşturduk.\n\n\nStreamlit Web Uygulamasının Yapımı\nWeb uygulaması tarafında kullanılan kodlar aşağıdadır.\n\n# Oturum durumu değişkenlerinin tanımlanması ve varsayılan değerlerin ayarlanması\nst.session_state.setdefault('rhos', []) # Korelasyon katsayıları\nst.session_state.setdefault('guesses', []) # Kullanıcının tahminleri\nst.session_state.setdefault('rmse_values', []) # RMSE değeri\nst.session_state.setdefault('plots', []) # Görseller\n\n# Korelasyonlu veri oluşturan fonksiyon\ndef generate_correlated_data():\n    num_points = int(np.random.uniform(low=100, high=1000))\n    rho = round(np.random.uniform(low=-1, high=1), 2)\n    cov_matrix = np.array([[1, rho], [rho, 1]])\n    mu = [0, 0]\n    correlated_data = np.random.multivariate_normal(mean=mu, cov=cov_matrix, size=num_points)\n    return correlated_data, rho\n\n# RMSE hesaplayan fonksiyon\ndef calculate_rmse(predictions, actuals):\n    mse = np.mean((predictions - actuals) ** 2)\n    rmse = round(np.sqrt(mse), 4)\n    return rmse\n\n# 'rhos' listesi boş ise yeni bir korelasyonlu veri oluşturulması ve korelasyon katsayısının kaydedilmesi\nif len(st.session_state['rhos']) == 0:\n    correlated_data, rho = generate_correlated_data()\n    st.session_state['rhos'].append(rho)\n\n# 'plots' listesi boşsa ilk görselin oluşturulması ve kaydedilmesi\nif len(st.session_state['plots']) == 0:\n    plt.figure(figsize=(8, 6))\n    plt.scatter(correlated_data[:,0], correlated_data[:,1], alpha=0.7)\n    plt.title('Scatter Plot of Correlated Data')\n    plt.xlabel('X')\n    plt.ylabel('Y')\n    plt.grid(True)\n    st.session_state['plots'].append(plt)\n    st.pyplot(st.session_state['plots'][0])\n\n# Kullanıcının tahmini\nuser_guess = st.sidebar.text_input(\n    label='Your Guess:',\n    value='0.0'\n)\n\n# Kullanıcı tahmininin ondalık sayıya dönüştürülmesi\nuser_guess = float(user_guess)\n\n# Tahmin butonunun oluşturulması\nguess_button = st.sidebar.button(label='Guess')\n\n# Tahmin butonuna basıldığında yapılacaklar\nif guess_button:\n    correlated_data, rho = generate_correlated_data()\n    st.session_state['rhos'].append(rho)\n    st.session_state['guesses'].append(user_guess)\n    rmse = calculate_rmse(np.array(st.session_state['rhos'][:-1]), np.array(st.session_state['guesses']))\n    st.session_state['rmse_values'].append(rmse)\n\n    st.subheader(f'Guess: {st.session_state[\"guesses\"][-1]}, Actual: {st.session_state[\"rhos\"][-2]}, RMSE: {st.session_state[\"rmse_values\"][-1]}')\n\n    plt.figure(figsize=(8, 6))\n    plt.scatter(correlated_data[:,0], correlated_data[:,1], alpha=0.7)\n    plt.title('Scatter Plot of Correlated Data')\n    plt.xlabel('X')\n    plt.ylabel('Y')\n    plt.grid(True)\n    st.session_state['plots'].append(plt)\n    st.pyplot(st.session_state['plots'][-1])\n\nWindows/Visual Studio Code için bir not: Streamlit uygulamasının yapımında kullanılacak olan kodları .py uzantılı app.py isminde bir script’e kaydedip terminalden streamlit run app.py komutu ile çalıştırabilirsiniz.\nGelecek içeriklerde görüşmek dileğiyle."
  },
  {
    "objectID": "posts/post_16/index.html",
    "href": "posts/post_16/index.html",
    "title": "Poisson Dağılımı ile 2024 Avrupa Futbol Şampiyonası Grup Maçları Öngörüleri",
    "section": "",
    "text": "Poisson dağılımının nasıl çalıştığını ve 2024 Avrupa Futbol Şampiyonası grup maçları ya da genel olarak futbol maçları için nasıl öngörülerde bulunabileceğimizi öğreneceğiz."
  },
  {
    "objectID": "posts/post_16/index.html#poisson-dağılımının-tanımı",
    "href": "posts/post_16/index.html#poisson-dağılımının-tanımı",
    "title": "Poisson Dağılımı ile 2024 Avrupa Futbol Şampiyonası Grup Maçları Öngörüleri",
    "section": "Poisson Dağılımının Tanımı",
    "text": "Poisson Dağılımının Tanımı\nPoisson dağılımı, istatistik ve olasılık teorisinde sıkça kullanılan bir olasılık dağılımıdır. Adını Fransız matematikçi Siméon Denis Poisson’dan alan bu dağılım, belirli bir zaman dilimi içinde belirli bir olayın meydana gelme olasılığını modellemek için kullanılır. Özellikle nadir olayların incelenmesinde etkilidir.\nPoisson dağılımı, aşağıdaki gibi çeşitli alanlarda geniş bir uygulama yelpazesine sahiptir.\n\nTelekomünikasyon: Bir telefon hattında belirli bir süre içinde alınan çağrıların sayısını modellemek.\nWeb Trafiği: Bir web sitesinde belirli bir süre içinde gerçekleşen tıklama sayısını tahmin etmek.\nKalite Kontrol: Üretim hattında belirli bir zaman diliminde meydana gelen hata veya kusur sayısını izlemek.\nSağlık Hizmetleri: Acil servise belirli bir zaman diliminde gelen hasta sayısını tahmin etmek.\nDoğal Afetler: Belirli bir bölgede meydana gelen deprem veya sel gibi olayların sayısını modellemek.\n\nPoisson dağılımını kullanmak için aşağıdaki koşulların sağlanıp sağlanmadığına bakmak gerekiyor.\n\nOlaylar belirli bir zaman diliminde veya alanda bağımsız olarak meydana gelir. Bir maçta atılan gollerin sayısı, maçtaki diğer olaylardan (örneğin, kart gösterme, faul yapma, köşe vuruşu kullanma) etkilenmez. Her gol atışı birbirinden bağımsızdır ve diğer olayları etkilemez.\nBir olayın meydana gelme olasılığı, belirli bir zaman dilimi veya alan içinde sabittir. Örneğin, bir takımın ortalama olarak maç başına 2 gol attığını düşünelim. Bu, belirli bir zaman aralığında (örneğin, sezon boyunca) sabit bir gol oranı olduğunu gösterir.\nİki veya daha fazla olayın aynı anda meydana gelme olasılığı ihmal edilecek kadar küçüktür. Örneğin, aynı an içinde iki takımın da birer gol atması çok nadir bir durumdur. Bu durum, Poisson dağılımının uygulanabilirliği için gerekli olan nadirlik koşulunu sağlar.\n\nPoisson dağılımında ortalama ve varyans birbirine eşittir. Yani, \\(E(X) = Var(X)\\) olacaktır.\nPoisson dağılımının olasılık kütle fonksiyonu (PMF) aşağıdaki gibi ifade edilir.\n\\(P(X=k) = \\frac{\\lambda^ke^{-\\lambda}}{k!}\\)\n\n\\(X\\): Belirli bir zaman diliminde meydana gelen olayların sayısı\n\\(k\\): Olay sayısı (0, 1, 2, …)\n\\(\\lambda\\): Belirli bir zaman diliminde beklenen olay sayısı (ortalama)\n\\(e\\): Euler sabiti (~2.71828)"
  },
  {
    "objectID": "posts/post_16/index.html#futbol-maçlarında-poisson-dağılımının-kullanımı",
    "href": "posts/post_16/index.html#futbol-maçlarında-poisson-dağılımının-kullanımı",
    "title": "Poisson Dağılımı ile 2024 Avrupa Futbol Şampiyonası Grup Maçları Öngörüleri",
    "section": "Futbol Maçlarında Poisson Dağılımının Kullanımı",
    "text": "Futbol Maçlarında Poisson Dağılımının Kullanımı\nPoisson dağılımı, futbol maçlarında bir takımın belirli bir maçta atacağı gol sayısını öngörmek ve sonuç olasılıklarını hesaplamak için kullanılabilir. Örneğin, belirli bir takımın ortalama olarak 2.5 gol attığını varsayalım (\\(\\lambda\\)=2.5). Bu durumda, bu takımın 0, 1, 2, 3, … gol atma olasılıklarını hesaplayabiliriz.\nBir takımın ortalama olarak 2.5 gol attığını düşünelim ve 0’dan 10’a kadar gol atma olasılıklarını hesaplayalım.\n\nlambda_goals = 2.5\ngoal_counts = np.arange(0, 11)\npoisson_probabilities = poisson.pmf(goal_counts, lambda_goals)\n\nfor i, p in zip(goal_counts, poisson_probabilities):\n    print(f\"Probability of scoring {i} goals: {p:.4f}\")\n\nplt.figure(figsize=(10, 6))\nplt.bar(goal_counts, poisson_probabilities, color='blue', alpha=0.7)\nplt.xlabel('Number of Goals')\nplt.ylabel('Probability')\nplt.title('Goal Scoring Probabilities Using Poisson Distribution')\nplt.xticks(goal_counts)\nplt.show()\n\n# Çıktı:\n\n# Probability of scoring 0 goals: 0.0821\n# Probability of scoring 1 goals: 0.2052\n# Probability of scoring 2 goals: 0.2565\n# Probability of scoring 3 goals: 0.2138\n# Probability of scoring 4 goals: 0.1336\n# Probability of scoring 5 goals: 0.0668\n# Probability of scoring 6 goals: 0.0278\n# Probability of scoring 7 goals: 0.0099\n# Probability of scoring 8 goals: 0.0031\n# Probability of scoring 9 goals: 0.0009\n# Probability of scoring 10 goals: 0.0002"
  },
  {
    "objectID": "posts/post_16/index.html#hesaplamanın-matematiği",
    "href": "posts/post_16/index.html#hesaplamanın-matematiği",
    "title": "Poisson Dağılımı ile 2024 Avrupa Futbol Şampiyonası Grup Maçları Öngörüleri",
    "section": "Hesaplamanın Matematiği",
    "text": "Hesaplamanın Matematiği\nTurnuvadaki ortalama gol beklentisini (average goal expectancy), her iki taraf için de saldırı gücü (attack strength) ve savunma gücünü (defence strength) hesaplamamız gerekiyor. Takımın gol beklentisi, hem takımın saldırı gücü ve savunma gücüne hem de karşı takımın saldırı ve savunma gücüne bağlıdır.\ncalculate_strengths ve calculate_goal_expectancy fonksiyonları ile yukarıda belirttiğimiz hesaplamaları yapacağız.\n\ndef calculate_strengths(historical_results_df):\n    def calculate_strengths_for_team(historical_results_df, column_team, column_score):\n        team_strength = {}\n        total_goals = historical_results_df[column_score].sum()\n        total_matches = len(historical_results_df[column_team])\n        total_avg = total_goals / total_matches\n\n        for team in historical_results_df[column_team].unique():\n            team_goals = historical_results_df.loc[historical_results_df[column_team] == team, column_score].sum()\n            team_matches = len(historical_results_df[historical_results_df[column_team] == team])\n            team_avg = team_goals / team_matches\n            team_strength[team] = team_avg / total_avg\n\n        return team_strength\n\n    home_attack_strength = calculate_strengths_for_team(historical_results_df, 'home_team', 'home_score')\n    away_attack_strength = calculate_strengths_for_team(historical_results_df, 'away_team', 'away_score')\n    home_defence_strength = calculate_strengths_for_team(historical_results_df, 'home_team', 'away_score')\n    away_defence_strength = calculate_strengths_for_team(historical_results_df, 'away_team', 'home_score')\n\n    return home_attack_strength, away_attack_strength, home_defence_strength, away_defence_strength\n\ndef calculate_goal_expectancy(matches, strengths, historical_results_df):\n    home_attack_strength, away_attack_strength, home_defence_strength, away_defence_strength = strengths\n    goal_expectancy = []\n\n    for index, match in matches.iterrows():\n        team1 = match['team1']\n        team2 = match['team2']\n\n        team1_attack_strength = home_attack_strength.get(team1, 1.0)\n        team2_defence_strength = away_defence_strength.get(team2, 1.0)\n        home_goals_avg = historical_results_df['home_score'].sum() / len(historical_results_df['home_team'])\n        team1_expectancy = team1_attack_strength * team2_defence_strength * home_goals_avg\n\n        team2_attack_strength = away_attack_strength.get(team2, 1.0)\n        team1_defence_strength = home_defence_strength.get(team1, 1.0)\n        away_goals_avg = historical_results_df['away_score'].sum() / len(historical_results_df['away_team'])\n        team2_expectancy = team2_attack_strength * team1_defence_strength * away_goals_avg\n\n        goal_expectancy.append((team1, team2, team1_expectancy, team2_expectancy))\n\n    return pd.DataFrame(goal_expectancy, columns=['Team1', 'Team2', 'Team1_Expectancy', 'Team2_Expectancy'])\n\nSon olarak, match_probabilities fonksiyonu ile olasılıkları hesaplayıp calculate_match_probabilities ile veri çerçevesini ortaya çıkaracağız.\n\ndef match_probabilities(lambda1, lambda2, max_goals=5):\n    probs = np.zeros((max_goals+1, max_goals+1))\n    for i in range(max_goals+1):\n        for j in range(max_goals+1):\n            probs[i, j] = (np.exp(-lambda1) * lambda1**i / np.math.factorial(i)) * (np.exp(-lambda2) * lambda2**j / np.math.factorial(j))\n    return probs / np.sum(probs)\n\ndef calculate_match_probabilities(goal_expectancy_df):\n    goal_expectancy_df['Team1_Win_Prob'] = 0.0\n    goal_expectancy_df['Team2_Win_Prob'] = 0.0\n    goal_expectancy_df['Draw_Prob'] = 0.0\n\n    for index, row in goal_expectancy_df.iterrows():\n        lambda1 = row['Team1_Expectancy']\n        lambda2 = row['Team2_Expectancy']\n\n        match_probs = match_probabilities(lambda1, lambda2)\n\n        goal_expectancy_df.at[index, 'Team1_Win_Prob'] = np.sum(np.tril(match_probs, -1))\n        goal_expectancy_df.at[index, 'Team2_Win_Prob'] = np.sum(np.triu(match_probs, 1))\n        goal_expectancy_df.at[index, 'Draw_Prob'] = np.sum(np.diag(match_probs))\n\n    return goal_expectancy_df"
  },
  {
    "objectID": "posts/post_16/index.html#avrupa-futbol-şampiyonası-grup-maçları-öngörüleri",
    "href": "posts/post_16/index.html#avrupa-futbol-şampiyonası-grup-maçları-öngörüleri",
    "title": "Poisson Dağılımı ile 2024 Avrupa Futbol Şampiyonası Grup Maçları Öngörüleri",
    "section": "2020 Avrupa Futbol Şampiyonası Grup Maçları Öngörüleri",
    "text": "2020 Avrupa Futbol Şampiyonası Grup Maçları Öngörüleri\n2020 yılına ait verileri içe aktaralım.\n\neuro_2020_countries = pd.read_excel('euro_2020_countries.xlsx')\nmatches_2020 = pd.read_excel('matches_2020.xlsx')\n\ncutoff_date = '2021-06-11'\nhistorical_results_2020 = filter_historical_data(historical_results, cutoff_date, euro_2020_countries)\n\nstrengths_2020 = calculate_strengths(historical_results_2020)\n\ngoal_expectancy_df_2020 = calculate_goal_expectancy(matches_2020, strengths_2020, historical_results_2020)\n\ngoal_expectancy_df_2020 = calculate_match_probabilities(goal_expectancy_df_2020)\n\n\nÖrnek (Türkiye-İtalya)\nDaha net anlamak adına Türkiye-İtalya maçı örneğine bakalım.\n\nEv sahibi takım Türkiye ve deplasman takımı İtalya’nın saldırı gücü nedir?\n\nEv sahibi takım için saldırı gücü, ev sahibi takımın attığı ortalama gol sayısının turnuvadaki ev sahibi takımların attığı ortalama gol sayısına bölümüdür.\nTürkiye 39 maçta 44 gol atarken, tüm takımlar 859 maçta 1284 gol atmıştır. Türkiye’nin saldırı gücü = (44/39) / (1284/859) = 0.7547727454269509’dur.\nDeplasman takımı için saldırı gücü, deplasman takımının attığı ortalama gol sayısının turnuvadaki deplasman takımlarının attığı ortalama gol sayısına bölümüdür.\nİtalya 48 maçta 45 gol atarken, tüm takımlar 859 maçta 911 gol atmıştır. İtalya’nın saldırı gücü = (45/48) / (911/859) = 0.8839873765093305’tir.\n\nEv sahibi takım Türkiye ve deplasman takımı İtalya’nın savunma gücü nedir?\n\nEv sahibi takım için savunma gücü, ev sahibi takımın yediği ortalama gol sayısının turnuvadaki ev sahibi takımların yediği ortalama gol sayısına bölümüdür.\nTürkiye 39 maçta 46 gol yerken, tüm takımlar 859 maçta 911 gol yemiştir. Türkiye’nin savunma gücü = (46/39) / (911/859) = 1.1121618959160122’dir.\nDeplasman takımı için savunma gücü, deplasman takımının yediği ortalama gol sayısının turnuvadaki deplasman takımlarının yediği ortalama gol sayısına bölümüdür.\nİtalya 48 maçta 44 gol yerken, tüm takımlar 859 maçta 1284 gol yemiştir. İtalya’nın savunma gücü = (44/48) / (1284/859) = 0.6132528556593977’dir.\n\nEv sahibi takım Türkiye ve deplasman takımı İtalya’nın gol beklentisi nedir?\n\nEv sahibi takım için gol beklentisi, takımın saldırı gücü, rakibin savunma gücü ve ev sahibi takımların ortalama gol sayısının çarpımıdır.\nTürkiye’nin saldırı gücü 0.7547727454269509, İtalya’nın savunma gücü 0.6132528556593977 ve ev sahibi takımların ortalama gol sayısı 1.4947613504074506’dır. Bu durumda Türkiye’nin gol beklentisi 0.6918750166413717’dir.\nDeplasman takımı için gol beklentisi, takımın saldırı gücü, rakibin savunma gücü ve deplasman takımlarının ortalama gol sayısının çarpımıdır.\nİtalya’nın saldırı gücü 0.8839873765093305, Türkiye’nin savunma gücücü 1.1121618959160122 ve ev sahibi takımların ortalama gol sayısı 1.060535506402794’tür. Bu durumda İtalya’nın gol beklentisi 1.0426517774212616’dır.\nPeki, sonuçlar gerçekten de hesapladığımız gibi mi?\n\nSağlamasını yapmış olduk.\nSonrasında, maksimum 5 gol için olasılıkları hesaplayacağız.\nTürkiye’nin gol beklentisine \\(\\lambda_T\\); İtalya’nın gol beklentisine \\(\\lambda_I\\) diyelim.\n\\(\\lambda_T\\) = 0.6918750166413717 ve \\(\\lambda_I\\) = 1.0426517774212616’dır.\n\\(P(k; \\lambda_T) = \\frac{\\lambda^ke^{-\\lambda}}{k!}\\)\nTürkiye için gol olasılıkları:\n\\(P(0; \\lambda_T) = \\frac{0.6918750166413717^0 e^{-0.6918750166413717}}{0!}\\)\n\\(P(1; \\lambda_T) = \\frac{0.6918750166413717^1 e^{-0.6918750166413717}}{1!}\\)\n\\(P(2; \\lambda_T) = \\frac{0.6918750166413717^2 e^{-0.6918750166413717}}{2!}\\)\n\\(P(3; \\lambda_T) = \\frac{0.6918750166413717^3 e^{-0.6918750166413717}}{3!}\\)\n\\(P(4; \\lambda_T) = \\frac{0.6918750166413717^4 e^{-0.6918750166413717}}{4!}\\)\n\\(P(5; \\lambda_T) = \\frac{0.6918750166413717^5 e^{-0.6918750166413717}}{5!}\\)\nİtalya için gol olasılıkları:\n\\(P(0; \\lambda_I) = \\frac{1.0426517774212616^0 e^{-1.0426517774212616}}{0!}\\)\n\\(P(1; \\lambda_I) = \\frac{1.0426517774212616^1 e^{-1.0426517774212616}}{1!}\\)\n\\(P(2; \\lambda_I) = \\frac{1.0426517774212616^2 e^{-1.0426517774212616}}{2!}\\)\n\\(P(3; \\lambda_I) = \\frac{1.0426517774212616^3 e^{-1.0426517774212616}}{3!}\\)\n\\(P(4; \\lambda_I) = \\frac{1.0426517774212616^4 e^{-1.0426517774212616}}{4!}\\)\n\\(P(5; \\lambda_I) = \\frac{1.0426517774212616^5 e^{-1.0426517774212616}}{5!}\\)\nMaç için gol olasılıkları:\nHer iki takımın olasılıklarının çarpımıdır. Örneğin, sonucun 1-2 olma olasılığına bakalım.\n\\(P(Türkiye=1) * P(İtalya=2) = 0.34637788 * 0.19161551 = 0.0663713741289188\\)\nKod kullanarak aşağıdaki gibi hesaplnabilir.\n\nlambda_T = 0.6918750166413717\nlambda_I = 1.0426517774212616\n\ngoals = np.arange(0, 6)\n\nprobabilities_T = poisson.pmf(goals, lambda_T)\nprobabilities_I = poisson.pmf(goals, lambda_I)\n\nprob_matrix = np.outer(probabilities_T, probabilities_I)\n\nplt.figure(figsize=(10, 7))\nplt.imshow(prob_matrix, cmap='viridis', aspect='auto')\n\nfor i in range(prob_matrix.shape[0]):\n    for j in range(prob_matrix.shape[1]):\n        plt.text(j, i, f'{prob_matrix[i, j]:.4f}', ha='center', va='center', color='white')\n\nplt.xlabel('Italy Goals')\nplt.ylabel('Turkey Goals')\nplt.title('Goal Probability Matrix Heatmap')\nplt.xticks(np.arange(len(goals)), goals)\nplt.yticks(np.arange(len(goals)), goals)\nplt.colorbar(label='Probability')\nplt.show()\n\n\n0-0, 1-1, 2-2, 3-3, 4-4 ve 5-5 birleşiminin altında kalan alanın toplamı Türkiye’nin, üstünde kalan alanın toplamı ise İtalya’nın kazanma olasılıklarıdır. Aynı gol sayılarının kesişimlerinin toplamı ise beraberlik olasılığıdır.\nÖrneğimizde, Türkiye’nin kazanma olasılığı 0.239113, İtalya’nın kazanma olasılığı 0.431935 ve beraberlik olasılığı 0.328952 çıkacaktır. Bu maçın sonucunda İtalya 3-0 kazanmıştı.\n\n\n2020 Öngörüleri Ne Kadar Başarılı Olurdu?\n\ndef determine_winner_prob(row):\n    if row['Team1_Win_Prob'] &gt; row['Team2_Win_Prob'] and row['Team1_Win_Prob'] &gt; row['Draw_Prob']:\n        return f\"{row['Team1']} wins\"\n    elif row['Team2_Win_Prob'] &gt; row['Team1_Win_Prob'] and row['Team2_Win_Prob'] &gt; row['Draw_Prob']:\n        return f\"{row['Team2']} wins\"\n    else:\n        return \"No Winner\"\n\ngoal_expectancy_df_2020['result'] = goal_expectancy_df_2020.apply(determine_winner_prob, axis=1)\n\ndef determine_winner(row):\n    if row['team1_score'] &gt; row['team2_score']:\n        return f\"{row['team1']} wins\"\n    elif row['team2_score'] &gt; row['team1_score']:\n        return f\"{row['team2']} wins\"\n    else:\n        return \"No Winner\"\n\nmatches_2020['result'] = matches_2020.apply(determine_winner, axis=1)\nmatches_2020['prob_result'] = goal_expectancy_df_2020['result']\n\nmatches_equal_prob = matches_2020[matches_2020['result'] == matches_2020['prob_result']]\nmatching_count = len(matches_equal_prob)\nresult = matching_count / len(matches_2020)\n\nBaşarı oranı %56 olmuştur ki bu da neredeyse yazı-tura atmak ile aynıdır."
  },
  {
    "objectID": "posts/post_16/index.html#avrupa-futbol-şampiyonası-grup-maçları-öngörüleri-1",
    "href": "posts/post_16/index.html#avrupa-futbol-şampiyonası-grup-maçları-öngörüleri-1",
    "title": "Poisson Dağılımı ile 2024 Avrupa Futbol Şampiyonası Grup Maçları Öngörüleri",
    "section": "2024 Avrupa Futbol Şampiyonası Grup Maçları Öngörüleri",
    "text": "2024 Avrupa Futbol Şampiyonası Grup Maçları Öngörüleri\nHer ne kadar 2020 öngörüleri tatmin edici bir sonuç vermese de 2024 yılı için de öngörülerimizi yapalım.\n\neuro_2024_countries = pd.read_excel('euro_2024_countries.xlsx')\nmatches_2024 = pd.read_excel('matches_2024.xlsx')\n\ncutoff_date = '2024-06-14'\nhistorical_results_2024 = filter_historical_data(historical_results, cutoff_date, euro_2024_countries)\n\nstrengths_2024 = calculate_strengths(historical_results_2024)\n\ngoal_expectancy_df_2024 = calculate_goal_expectancy(matches_2024, strengths_2024, historical_results_2024)\n\ngoal_expectancy_df_2024 = calculate_match_probabilities(goal_expectancy_df_2024)\n\ngoal_expectancy_df_2024['Result'] = goal_expectancy_df_2024.apply(determine_winner_prob, axis=1)\n\nSonuçları daha güzel gösterebiliriz.\n\ndef highlight_max_prob(row):\n    max_prob = row[['Team1_Win_Prob', 'Team2_Win_Prob', 'Draw_Prob']].max()\n    return ['background-color: red' if v == max_prob else '' for v in row]\n\nstyled_matches = goal_expectancy_df_2024.style.apply(highlight_max_prob, axis=1)\n\nhtml_styled_matches = styled_matches.to_html()\n\nwith open('styled_matches.html', 'w') as f:\n    f.write(html_styled_matches)\n\n\nYapılabilecek ilk eleştiri, beraberlik sonucunun çıkmaması olabilir. İkinci bir eleştiri, Poisson dağılıma uygun olduğu varsayımı yapılıp herhangi bir testin yapılmaması olabilir.\nBaşarısızlıktan öte, Poisson dağılımını anlamak var olanı ya da yeni yöntemleri geliştirmede etkili olacaktır. Bu, eğer atmadıysanız ilk adımınız olsun.\nGelecek içeriklerde görüşmek dileğiyle."
  },
  {
    "objectID": "posts/post_14/index.html",
    "href": "posts/post_14/index.html",
    "title": "İndikatör Ağırlıklarının Optimize Edilerek Teknik Reytinglerin Oluşturulması ve Al-Tut Stratejisinin Alt Edilmesi",
    "section": "",
    "text": "Tradingview platformundaki tanıma göre teknik reytingler, işlemcinin ve yatırımcıların karlı işlemler bulmasını kolaylaştırmak için çeşitli teknik göstergelerin derecelendirmelerini birleştiren teknik bir analiz aracıdır şeklindedir. Örneğin, BTC-USD’nin teknik reytingi aşağıdaki gibidir.\n\nOluşturacağımız teknik reytinglerin mantığı Tradingview platformundaki gibi olsa da bazı değişiklikler yapacağız. Bu değişiklikleri aşağıdaki gibi özetleyebiliriz.\n\nİndikatörlerin seçilmesi: Tradingview platformunda kullanılan indikatörlerden farklılaşacağız. Bu noktada, yatırımcı kendi indikatörlerini seçebilir.\nStratejilerin oluşturulması: Her bir indikatör için bir alım-satım stratejisi belirleyeceğiz. Bu noktada, yatırımcı kendi alım-satım stratejilerini belirleyebilir.\nStratejide kullanılacak değerlerin optimize edilmesi: Tarihsel verileri kullanarak ilgili indikatöre ait alım-satım stratejisinde kullanılacak değerlerden en iyi kümülatif getiri sağlayanları kullanacağız.\nReytinglerin oluşturulmasında ağırlıkların optimize edilmesi: Alım-satım kararında her indikatöre eşit ağırlık vermek yerine en iyi kümülatif getiri performansı gösteren ağırlık dağılımını kullanacağız.\n\nÇalışmada, tüm etkilerin sıfır varsayıldığının ve sadece getiriye odaklanıldığının altını çizelim. Ayrıca bu çalışma, aktif bir şekilde alım-satım stratejisi takip etmenin al-tut stratejisini her zaman yenebileceğini iddia etmemektedir."
  },
  {
    "objectID": "posts/post_14/index.html#kümülatif-getiri",
    "href": "posts/post_14/index.html#kümülatif-getiri",
    "title": "İndikatör Ağırlıklarının Optimize Edilerek Teknik Reytinglerin Oluşturulması ve Al-Tut Stratejisinin Alt Edilmesi",
    "section": "Kümülatif Getiri",
    "text": "Kümülatif Getiri\nÇalışmada kullanacağımız kümülatif getiri, tüm değerleme dönemi boyunca bileşik günlük getiriyi verir ve aşağıdaki gibi formülize edilebilir.\n\\(R_c = \\prod_{i=1}^{n} (1 + R_i) - 1 = (1 + R_1)(1 + R_2)...(1 + R_n) - 1\\)\nBir örnek ile nasıl hesaplandığına bakalım.\n\n# Veri\n\ndata = {\n    'Date': pd.date_range(start='2024-05-20', periods=5, freq='D'),\n    'Prices': [100, 101, 100.5, 102.5, 101.3]\n}\ndata_df = pd.DataFrame(data)\n\n# Getiri\n\ndata_df['Return'] = data_df['Prices'].pct_change()\n\n# Kümülatif Getiri\n\ndata_df['Cumulative Return'] = (1 + data_df['Return']).cumprod() - 1\ncumulative_return = data_df['Cumulative Return'].iloc[-1]\nprint(f\"Cumulative Return: {cumulative_return:.2%}\")\n\n# Başlangıç Değer Değişimi\n\ninitial_value = 100\nfinal_value = initial_value * (1 + cumulative_return)\nprint(f\"Final Value: {final_value:.2f} TL\")"
  },
  {
    "objectID": "posts/post_14/index.html#hesaplamanın-tasarımı",
    "href": "posts/post_14/index.html#hesaplamanın-tasarımı",
    "title": "İndikatör Ağırlıklarının Optimize Edilerek Teknik Reytinglerin Oluşturulması ve Al-Tut Stratejisinin Alt Edilmesi",
    "section": "Hesaplamanın Tasarımı",
    "text": "Hesaplamanın Tasarımı\nPopüler bir teknik analiz kütüphanesi olan ta kütüphanesini ve aşağıdaki trend, momentum ve volatilite indikatörlerini kullanacağız.\n\nTrend\n\nWeighted Moving Average: WMAIndicator\nMoving Average Convergence Divergence: MACD\nAverage Directional Movement Index: ADXIndicator\nIchimoku Kinkō Hyō: IchimokuIndicator\n\nMomentum\n\nRelative Strength Index: RSIIndicator\nRate of Change: ROCIndicator\n\nVolatilite\n\nBollinger Bands: BollingerBands\n\n\nHer bir indikatörde al sinyallerine 1, sat sinyallerine -1 ve nötr sinyallerine 0 atayacağız. Tüm optimizasyon işlemleri bittikten sonra aşağıdaki kriterleri baz alarak alım-satım kararını uygulayacağız.\n\n-1.0 \\(\\leq\\) reyting \\(\\leq\\) -0.5 ise Sat\n-0.5 \\(&lt;\\) reyting \\(&lt;\\) 0.5 ise Nötr\n0.5 \\(\\leq\\) reyting \\(\\leq\\) 1.0 ise Al"
  },
  {
    "objectID": "posts/post_14/index.html#indikatörlerin-hesaplanması",
    "href": "posts/post_14/index.html#indikatörlerin-hesaplanması",
    "title": "İndikatör Ağırlıklarının Optimize Edilerek Teknik Reytinglerin Oluşturulması ve Al-Tut Stratejisinin Alt Edilmesi",
    "section": "İndikatörlerin Hesaplanması",
    "text": "İndikatörlerin Hesaplanması\nta_rating_df adında bir pandas DataFrame oluşturalım. Bu DataFrame, tarih (Date), indikatör (Indicator) ve sinyal (Signal) bilgilerini içerecek. İndikatörlerden gelen bilgiler ta_rating_df’te saklanacak.\n\nta_rating_df = pd.DataFrame(columns=['Date', 'Indicator', 'Signal'])\n\n\nSinyallerin Güncellenmesi\nİndikatörlerin alım-satım stratejilerinde kullanılacak değerlerini optimize ederken alım-satım sinyallerini güncelleyeceğiz. Bu güncelleme ile beraber indikatörün art arda aynı sinyali üretmesi yerine sırayla sinyal üretmesini sağlayacağız.\n\ndef update_signals(signals):\n    updated_signals = []\n    previous_signal = None\n    for signal in signals:\n        if signal == previous_signal:\n            updated_signals.append(0)\n        else:\n            updated_signals.append(signal)\n            previous_signal = signal if signal in (1, -1) else previous_signal\n\n    return updated_signals\n\nupdate_signals fonksiyonu signals adında bir parametre alıyor. Güncellenmiş sinyalleri saklamak için updated_signals adında boş bir liste kullanılıyor. Bir önceki sinyali saklamak için de başlangıçta None olarak ayarlanmış previous_signal adında bir değişken kullanılıyor. Sonra süreç her bir sinyal için for döngüsü ile devam ediyor.\nif bloğu: Eğer mevcut sinyal önceki sinyal ile aynıysa updated_signals listesine 0 (nötr sinyali) ekleniyor.\nelse bloğu: Eğer mevcut sinyal önceki sinyal ile aynı değilse updated_signals listesine mevcut sinyal ekleniyor. Devamında ise mevcut sinyal 1 (al sinyali) veya -1 (sat sinyali) ise previous_signal değişkenine signal değişkeninin değeri atanıyor, aksi halde previous_signal değişkeninin değeri korunuyor.\nYukarıdaki işlemler bittikten sonra fonksiyon, updated_signals içerisinde bulunan güncellenmiş sinyaller listesini döndürür.\n\n\nİndikatörlerin Optimize Edilmesinde Genel Yaklaşım\nİndikatörlerin alım-satım stratejilerinde kullanılacak değerler optimize edilirken hepsinde ortak olan bir süreç bulunmaktadır. Alım-satım sinyali verecek değerler belli bir aralık içine alınır ve tarihsel verilerle kümülatif getirileri hesaplanır. En iyi kümülatif getiriyi sağlayan değerler kullanılarak sinyaller üretilir ve veriler ta_rating_df’te saklanır.\nKodsal olarak süreç şöyledir:\nbest_cum_return, best_params ve best_strategy_df değişkenleri tanımlanır. best_cum_return değişkeninin -np.inf değeri negatif sonsuz demektir ve henüz en iyi kümülatif getirinin hesaplanmadığı; best_params değişkeninin None değeri henüz en iyi getiriyi sağlayan stratejiye ait parametrelerin olmadığı; best_strategy_df değişkeninin None değeri ise henüz en iyi getiriyi sağlayan stratejinin işlem verilerinin olmadığı anlamına gelir.\nproduct fonksiyonu ile stratejide kullanılacak değerler oluşturulur ve bu değerler döngüye girer.\nSinyaller üretilir ve bu sinyaller güncellenir. Güncellenen sinyaller üzerinden kümülatif getiriler hesaplanır. Kümülatif getirilerden önce hesaplanan getiri hesaplamasında bir al-sat sinyalinden diğer al-sat sinyaline olan geçişe dikkat edilir.\nstrategy_cum_return’ün best_cum_return’den büyük olup olmadığına bakılır. Eğer büyükse başta oluşturulan değişkenler güncellenir.\nStratejinin en iyi değerleri ekrana basılır ve DataFrame’e son hali verilerek döndürülür.\n\n\nİndikatörlerin Optimize Edilmesi\n\nRelative Strength Index\nRelative Strength Index, bir varlığın geçmiş fiyat değişimlerine dayalı olarak aşırı alım veya aşırı satım durumlarını belirlemeye yardımcı olan bir momentum indikatörüdür.\nRelative Strength Index için seçilen strateji, indikatörü kullanarak alım-satım sinyalleri üretir ve bu sinyalleri optimize ederek en iyi kümülatif getiriyi elde etmeyi amaçlar. Belirli bir pencere boyutu, aşırı alım ve aşırı satım seviyeleri aralığını kullanarak fiyatların aşırı alım veya aşırı satım durumunda olup olmadığını belirler. Ardından, bu sinyalleri alım ve satım pozisyonlarına dönüştürür ve stratejinin getirisini hesaplar. En iyi parametrelerle belirlenen bu sinyaller daha sonra alım-satım stratejisi için kullanılır.\n\ndef optimize_and_calculate_rsi(df, window_range, overbought_range, oversold_range):\n    best_cum_return = -np.inf\n    best_params = None\n    best_strategy_df = None\n\n    for window, overbought, oversold in product(window_range, overbought_range, oversold_range):\n        strategy_df = df.copy().reset_index(drop=True)\n\n        rsi = momentum.RSIIndicator(strategy_df['Close'], window=window).rsi()\n        signal = np.where(rsi &lt; oversold, 1, np.where(rsi &gt; overbought, -1, 0))\n\n        strategy_df['Signal'] = signal\n        strategy_df['Signal Updated'] = update_signals(strategy_df['Signal'].tolist())\n\n        if not (1 in strategy_df['Signal Updated'].values and -1 in strategy_df['Signal Updated'].values):\n            continue\n\n        strategy_df_ret_calc = strategy_df.copy()\n        strategy_df_ret_calc = strategy_df_ret_calc[strategy_df_ret_calc['Signal Updated'] != 0]\n        strategy_df_ret_calc['Strategy Return'] = strategy_df_ret_calc['Close'].pct_change()\n\n        for index, row in strategy_df_ret_calc.iterrows():\n            signal = row['Signal Updated']\n            if signal == 1:\n                strategy_df_ret_calc.loc[index, 'Strategy Return'] *= -1\n\n        strategy_df_ret_calc = strategy_df_ret_calc[['Strategy Return']]\n        strategy_df = pd.merge(strategy_df, strategy_df_ret_calc, how='left', left_index=True, right_index=True)\n        strategy_df['Strategy Return'] = strategy_df['Strategy Return'].fillna(0)\n\n        strategy_df['Cumulative Return'] = (1 + strategy_df['Strategy Return']).cumprod() - 1\n        strategy_cum_return = strategy_df['Cumulative Return'].iloc[-1]\n\n        if strategy_cum_return &gt; best_cum_return:\n            best_cum_return = strategy_cum_return\n            best_params = (window, overbought, oversold)\n            best_strategy_df = strategy_df.copy()\n\n    print(f\"Best Params (Relative Strength Index): Window={best_params[0]}, Overbought={best_params[1]}, Oversold={best_params[2]}\")\n\n    best_strategy_df['Date'] = df.index\n    best_strategy_df['Indicator'] = 'Relative Strength Index'\n    rsi_signals_df = best_strategy_df[['Date', 'Signal', 'Indicator']]\n\n    return rsi_signals_df\n\nwindow_range = range(10, 21, 1)\noverbought_range = range(65, 86, 5)\noversold_range = range(15, 36, 5)\n\nrsi_signals_df = optimize_and_calculate_rsi(df=df, window_range=window_range, overbought_range=overbought_range, oversold_range=oversold_range)\nta_rating_df = pd.concat([ta_rating_df, rsi_signals_df], ignore_index=True)\n\nFonksiyondan alınan aşağıdaki kod bloğunu inceleyelim.\n\nrsi = momentum.RSIIndicator(strategy_df['Close'], window=window).rsi()\nsignal = np.where(rsi &lt; oversold, 1, np.where(rsi &gt; overbought, -1, 0))\n\nBu kod bloğu, indikatörün hesaplanmasını ve buna göre alım-satım sinyallerinin üretilmesini sağlar. İlk olarak, belirli bir pencere boyutu kullanılarak RSI hesaplanır. Ardından, RSI değerleri belirlenen aşırı alım ve aşırı satım seviyeleri ile karşılaştırılır. Eğer RSI değeri belirlenen aşırı satım seviyesinin altındaysa, bu durum aşırı satım olarak kabul edilir ve alım sinyali üretilir. Benzer şekilde, RSI değeri belirlenen aşırı alım seviyesinin üzerindeyse, bu durum aşırı alım olarak kabul edilir ve satım sinyali üretilir. RSI değeri bu seviyelerin arasındaysa, herhangi bir alım-satım sinyali üretilmez. Bu şekilde, RSI indikatörü kullanılarak belirli bir varlığın alım-satım kararları otomatik olarak belirlenmiş olur.\nRSI stratejisinin optimize edilmesi için kullanılan parametre aralıkları şöyledir:\n\nwindow_range: Pencere boyutunun aralığıdır. Değerler 10 ile 20 arasında her biri 1’er birim artacak şekilde belirlenmiştir.\noverbought_range: Aşırı alım seviyesinin belirlenmesi için kullanılan değerlerin aralığıdır. Değerler 65 ile 85 arasında her biri 5’er birim artacak şekilde belirlenmiştir.\noversold_range: Aşırı satım seviyesinin belirlenmesi için kullanılan değerlerin aralığıdır. Değerler 15 ile 35 arasında her biri 5’er birim artacak şekilde belirlenmiştir.\n\nÖrneğe göre en iyi kümülatif getiriyi sağlayan parametreler şunlardır:\nBest Params (Relative Strength Index): Window=20, Overbought=85, Oversold=25\n\n\nWeighted Moving Average\nWeighted Moving Average, belirli bir zaman diliminde fiyatların ağırlıklı ortalamasını hesaplayarak trendi belirlemeye yardımcı olan bir indikatördür.\nWeighted Moving Average için seçilen strateji, indikatörü kullanarak alım-satım sinyalleri üretir ve bu sinyalleri optimize ederek en iyi kümülatif getiriyi elde etmeyi amaçlar. Kısa ve uzun vadeli hareketli ortalamalar arasındaki ilişkiyi değerlendirir. Kısa vadeli WMA’in uzun vadeli WMA’yi aştığı durumlarda alım, kısa vadeli WMA’in uzun vadeli WMA’den düşük olduğu durumlarda satış sinyali üretir. Bu sinyalleri kullanarak stratejinin getirisini hesaplar ve en iyi parametrelerle belirlenen sinyallerle işlem yapar.\n\ndef optimize_and_calculate_wma(df, short_range, long_range):\n    best_cum_return = -np.inf\n    best_params = None\n    best_strategy_df = None\n\n    for short, long in product(short_range, long_range):\n        if short &gt;= long:\n            continue\n\n        strategy_df = df.copy().reset_index(drop=True)\n\n        wma_short = trend.WMAIndicator(strategy_df['Close'], window=short).wma()\n        wma_long = trend.WMAIndicator(strategy_df['Close'], window=long).wma()\n\n        signal = np.where(wma_short &gt; wma_long, 1, np.where(wma_short &lt; wma_long, -1, 0))\n\n        strategy_df['Signal'] = signal\n        strategy_df['Signal Updated'] = update_signals(strategy_df['Signal'].tolist())\n\n        if not (1 in strategy_df['Signal Updated'].values and -1 in strategy_df['Signal Updated'].values):\n            continue\n\n        strategy_df_ret_calc = strategy_df.copy()\n        strategy_df_ret_calc = strategy_df_ret_calc[strategy_df_ret_calc['Signal Updated'] != 0]\n        strategy_df_ret_calc['Strategy Return'] = strategy_df_ret_calc['Close'].pct_change()\n\n        for index, row in strategy_df_ret_calc.iterrows():\n            signal = row['Signal Updated']\n            if signal == 1:\n                strategy_df_ret_calc.loc[index, 'Strategy Return'] *= -1\n\n        strategy_df_ret_calc = strategy_df_ret_calc[['Strategy Return']]\n        strategy_df = pd.merge(strategy_df, strategy_df_ret_calc, how='left', left_index=True, right_index=True)\n        strategy_df['Strategy Return'] = strategy_df['Strategy Return'].fillna(0)\n\n        strategy_df['Cumulative Return'] = (1 + strategy_df['Strategy Return']).cumprod() - 1\n        strategy_cum_return = strategy_df['Cumulative Return'].iloc[-1]\n\n        if strategy_cum_return &gt; best_cum_return:\n            best_cum_return = strategy_cum_return\n            best_params = (short, long)\n            best_strategy_df = strategy_df.copy()\n\n    print(f\"Best Params (Weighted Moving Average): Short={best_params[0]}, Long={best_params[1]}\")\n\n    best_strategy_df['Date'] = df.index\n    best_strategy_df['Indicator'] = 'Weighted Moving Average'\n    wma_signals_df = best_strategy_df[['Date', 'Signal', 'Indicator']]\n\n    return wma_signals_df\n\nshort_range = range(20, 101, 5)\nlong_range = range(50, 201, 5)\n\nwma_signals_df = optimize_and_calculate_wma(df=df, short_range=short_range, long_range=long_range)\nta_rating_df = pd.concat([ta_rating_df, wma_signals_df], ignore_index=True)\n\nFonksiyondan alınan aşağıdaki kod bloğunu inceleyelim.\n\nwma_short = trend.wma_indicator(strategy_df['Close'], window=short)\nwma_long = trend.wma_indicator(strategy_df['Close'], window=long)\nsignal = np.where(wma_short &gt; wma_long, 1, np.where(wma_short &lt; wma_long, -1, 0))\n\nBu kod bloğu, indikatörün hesaplanmasını ve buna göre alım-satım sinyallerinin üretilmesini sağlar. İlk olarak, kısa vadeli WMA ve uzun vadeli WMA değerleri, belirli pencere boyutları (short ve long) kullanılarak hesaplanır. Daha sonra, kısa vadeli WMA’in uzun vadeli WMA’yi aştığı durumlarda alım sinyali üretilir, kısa vadeli WMA’in uzun vadeli WMA’den düşük olduğu durumlarda ise satım sinyali üretilir. Eğer kısa vadeli WMA ve uzun vadeli WMA birbirine eşitse veya kesişirse, herhangi bir sinyal üretilmez. Bu şekilde, WMA indikatörü kullanılarak belirli bir varlığın alım-satım kararları otomatik olarak belirlenmiş olur.\nKısa vadeli değerin uzun vadeli değere eşit veya uzun vadeli değerden büyük olmasının önüne geçilmiştir.\nWMA stratejisinin optimize edilmesi için kullanılan parametre aralıkları şöyledir:\n\nshort_range: Kısa vadeli ağırlıklı hareketli ortalama için kullanılacak pencere boyutu aralığıdır. Değerler 20 ile 100 arasında 5’er birim artacak şekilde belirlenmiştir.\nlong_range: Uzun vadeli ağırlıklı hareketli ortalama için kullanılacak pencere boyutu aralığıdır. Değerler 50 ile 200 arasında 5’er birim artacak şekilde belirlenmiştir.\n\nÖrneğe göre en iyi kümülatif getiriyi sağlayan parametreler şunlardır:\nBest Params (Weighted Moving Average): Short=45, Long=65\n\n\nMoving Average Convergence Divergence\nMoving Average Convergence Divergence, iki farklı üssel hareketli ortalamanın (EMA) farkını hesaplayarak trendin yönünü ve gücünü belirlemeye yardımcı olur\nMoving Average Convergence Divergence için seçilen strateji, indikatörü kullanarak alım-satım sinyalleri üretir ve bu sinyalleri optimize ederek en iyi kümülatif getiriyi elde etmeyi hedefler. Kısa vadeli ve uzun vadeli hareketli ortalamalar arasındaki farkı ve hareketli ortalamaların ortalama yakınsama ve diverjansını hesaplar. Ardından, sinyal hattıyla karşılaştırarak alım ve satım sinyallerini üretir. Bu sinyallerle stratejinin getirisini hesaplar ve en iyi parametrelerle belirlenen sinyallerle işlem yapar.\n\ndef optimize_and_calculate_macd(df, window_short_range, window_long_range, window_signal_range):\n    best_cum_return = -np.inf\n    best_params = None\n    best_strategy_df = None\n\n    for window_short, window_long, window_signal in product(window_short_range, window_long_range, window_signal_range):\n        strategy_df = df.copy().reset_index(drop=True)\n\n        macd_obj = trend.MACD(close=strategy_df['Close'], window_fast=window_short, window_slow=window_long, window_sign=window_signal)\n        macd = macd_obj.macd()\n        signal_line = macd_obj.macd_signal()\n\n        signal = np.where(macd &gt; signal_line, 1, np.where(macd &lt; signal_line, -1, 0))\n\n        strategy_df['Signal'] = signal\n        strategy_df['Signal Updated'] = update_signals(strategy_df['Signal'].tolist())\n\n        if not (1 in strategy_df['Signal Updated'].values and -1 in strategy_df['Signal Updated'].values):\n            continue\n\n        strategy_df_ret_calc = strategy_df.copy()\n        strategy_df_ret_calc = strategy_df_ret_calc[strategy_df_ret_calc['Signal Updated'] != 0]\n        strategy_df_ret_calc['Strategy Return'] = strategy_df_ret_calc['Close'].pct_change()\n\n        for index, row in strategy_df_ret_calc.iterrows():\n            signal = row['Signal Updated']\n            if signal == 1:\n                strategy_df_ret_calc.loc[index, 'Strategy Return'] *= -1\n\n        strategy_df_ret_calc = strategy_df_ret_calc[['Strategy Return']]\n        strategy_df = pd.merge(strategy_df, strategy_df_ret_calc, how='left', left_index=True, right_index=True)\n        strategy_df['Strategy Return'] = strategy_df['Strategy Return'].fillna(0)\n\n        strategy_df['Cumulative Return'] = (1 + strategy_df['Strategy Return']).cumprod() - 1\n        strategy_cum_return = strategy_df['Cumulative Return'].iloc[-1]\n\n        if strategy_cum_return &gt; best_cum_return:\n            best_cum_return = strategy_cum_return\n            best_params = (window_short, window_long, window_signal)\n            best_strategy_df = strategy_df.copy()\n\n    print(f\"Best Params (Moving Average Convergence Divergence): Short Window={best_params[0]}, Long Window={best_params[1]}, Signal Window={best_params[2]}\")\n\n    best_strategy_df['Date'] = df.index\n    best_strategy_df['Indicator'] = 'Moving Average Convergence Divergence'\n    macd_signals_df = best_strategy_df[['Date', 'Signal', 'Indicator']]\n\n    return macd_signals_df\n\nwindow_short_range = range(6, 13)\nwindow_long_range = range(20, 41)\nwindow_signal_range = range(4, 11)\n\nmacd_signals_df = optimize_and_calculate_macd(df=df, window_short_range=window_short_range, window_long_range=window_long_range, window_signal_range=window_signal_range)\nta_rating_df = pd.concat([ta_rating_df, macd_signals_df], ignore_index=True)\n\nFonksiyondan alınan aşağıdaki kod bloğunu inceleyelim.\n\nmacd_obj = trend.MACD(close=strategy_df['Close'], window_fast=window_short, window_slow=window_long, window_sign=window_signal)\nmacd = macd_obj.macd()\nsignal_line = macd_obj.macd_signal()\nsignal = np.where(macd &gt; signal_line, 1, np.where(macd &lt; signal_line, -1, 0))\n\nBu kod bloğu, indikatörün hesaplanmasını ve buna göre alım-satım sinyallerinin üretilmesini sağlar. İlk olarak, hızlı, yavaş ve sinyal pencere boyutları kullanılarak bir MACD nesnesi oluşturulur. MACD hesaplaması, hızlı hareketli ortalamanın yavaş hareketli ortalamadan çıkarılmasıyla elde edilir. Ardından, MACD hattı ve sinyal hattı hesaplanır. MACD hattı, hızlı EMA ile yavaş EMA arasındaki farkı gösterirken, sinyal hattı ise MACD hattının belirli bir zaman aralığındaki hareketli ortalamasıdır. Son olarak, MACD hattının sinyal hattını aştığı durumlarda alım sinyali üretilirken, MACD hattının sinyal hattının aşağısına geçtiği durumlarda satım sinyali üretilir. Eğer MACD hattı ve sinyal hattı birbirine eşitse veya kesişirse, herhangi bir sinyal üretilmez. Bu şekilde, MACD indikatörü kullanılarak belirli bir varlığın alım-satım kararları otomatik olarak belirlenir.\nMACD stratejisinin optimize edilmesi için kullanılan parametre aralıkları şöyledir:\n\nwindow_short_range: Kısa vadeli hareketli ortalama için kullanılacak pencere boyutu aralığıdır. Değerler 6 ile 12 arasında 1’er birim artacak şekilde belirlenmiştir.\nwindow_long_range: Uzun vadeli hareketli ortalama için kullanılacak pencere boyutu aralığıdır. Değerler 20 ile 40 arasında 1’er birim artacak şekilde belirlenmiştir.\nwindow_signal_range: MACD hattının sinyal hattını oluşturmak için kullanılacak pencere boyutu aralığıdır. Değerler 4 ile 10 arasında 1’er birim artacak şekilde belirlenmiştir.\n\nÖrneğe göre en iyi kümülatif getiriyi sağlayan parametreler şunlardır:\nBest Params (Moving Average Convergence Divergence): Short Window=6, Long Window=20, Signal Window=4\n\n\nAverage Directional Index\nAverage Directional Index, bir varlığın trendinin gücünü ölçen bir indikatördür.\nAverage Directional Index için seçilen strateji, indikatörü kullanarak alım-satım sinyalleri üretir ve bu sinyalleri optimize ederek en iyi kümülatif getiriyi hedefler. ADX, trendin gücünü ölçerken, +DI ve -DI göstergeleri trendin yönünü belirler. Belirli bir pencere boyutu ve eşik değeri ile bu göstergeleri değerlendirir ve belirli bir eşik değerinin üzerindeyse ve +DI, -DI’dan büyükse alım, tersi durumda satım sinyali üretir. Bu sinyallerle stratejinin getirisini hesaplar ve en iyi parametrelerle belirlenen sinyallerle işlem yapar.\n\ndef optimize_and_calculate_adx(df, window_range, threshold_range):\n    best_cum_return = -np.inf\n    best_params = None\n    best_strategy_df = None\n\n    for window in window_range:\n        for threshold in threshold_range:\n            strategy_df = df.copy().reset_index(drop=True)\n\n            adx_obj = trend.ADXIndicator(high=strategy_df['High'], low=strategy_df['Low'], close=strategy_df['Close'], window=window)\n            adx = adx_obj.adx()\n            plus_di = adx_obj.adx_pos()\n            minus_di = adx_obj.adx_neg()\n\n            for i in range(len(strategy_df)):\n                last_adx = adx.iloc[i]\n                last_plus_di = plus_di.iloc[i]\n                last_minus_di = minus_di.iloc[i]\n\n                if last_adx &gt; threshold:\n                    if last_plus_di &gt; last_minus_di:\n                        signal = 1\n                    elif last_plus_di &lt; last_minus_di:\n                        signal = -1\n                    else:\n                        signal = 0\n                else:\n                    signal = 0\n\n                strategy_df.loc[i, 'Signal'] = signal\n\n            strategy_df['Signal Updated'] = update_signals(strategy_df['Signal'].tolist())\n\n            if not (1 in strategy_df['Signal Updated'].values and -1 in strategy_df['Signal Updated'].values):\n                continue\n\n            strategy_df_ret_calc = strategy_df.copy()\n            strategy_df_ret_calc = strategy_df_ret_calc[strategy_df_ret_calc['Signal Updated'] != 0]\n            strategy_df_ret_calc['Strategy Return'] = strategy_df_ret_calc['Close'].pct_change()\n\n            for index, row in strategy_df_ret_calc.iterrows():\n                signal = row['Signal Updated']\n                if signal == 1:\n                    strategy_df_ret_calc.loc[index, 'Strategy Return'] *= -1\n\n            strategy_df_ret_calc = strategy_df_ret_calc[['Strategy Return']]\n            strategy_df = pd.merge(strategy_df, strategy_df_ret_calc, how='left', left_index=True, right_index=True)\n            strategy_df['Strategy Return'] = strategy_df['Strategy Return'].fillna(0)\n\n            strategy_df['Cumulative Return'] = (1 + strategy_df['Strategy Return']).cumprod() - 1\n            strategy_cum_return = strategy_df['Cumulative Return'].iloc[-1]\n\n            if strategy_cum_return &gt; best_cum_return:\n                best_cum_return = strategy_cum_return\n                best_params = (window, threshold)\n                best_strategy_df = strategy_df.copy()\n\n    print(f\"Best Params (Average Directional Index): Window={best_params[0]}, Threshold={best_params[1]}\")\n\n    best_strategy_df['Date'] = df.index\n    best_strategy_df['Indicator'] = 'Average Directional Index'\n    adx_signals_df = best_strategy_df[['Date', 'Signal', 'Indicator']]\n\n    return adx_signals_df\n\nwindow_range = range(10, 21)\nthreshold_range = range(10, 31)\nadx_signals_df = optimize_and_calculate_adx(df=df, window_range=window_range, threshold_range=threshold_range)\nta_rating_df = pd.concat([ta_rating_df, adx_signals_df], ignore_index=True)\n\nFonksiyondan alınan aşağıdaki kod bloğunu inceleyelim.\n\nadx_obj = trend.ADXIndicator(high=strategy_df['High'], low=strategy_df['Low'], close=strategy_df['Close'], window=window)\nadx = adx_obj.adx()\nplus_di = adx_obj.adx_pos()\nminus_di = adx_obj.adx_neg()\n\nfor i in range(len(strategy_df)):\n    last_adx = adx.iloc[i]\n    last_plus_di = plus_di.iloc[i]\n    last_minus_di = minus_di.iloc[i]\n\n    if last_adx &gt; threshold:\n        if last_plus_di &gt; last_minus_di:\n            signal = 1\n        elif last_plus_di &lt; last_minus_di:\n            signal = -1\n        else:\n            signal = 0\n    else:\n        signal = 0\n\nBu kod bloğu, indikatörün hesaplanmasını ve buna göre alım-satım sinyallerinin üretilmesini sağlar. İlk olarak, yüksek, düşük ve kapanış fiyatlarından oluşan veri seti ile birlikte belirli bir pencere boyutu kullanılarak bir ADX nesnesi oluşturulur. Daha sonra, ADX’in yanı sıra +DI ve -DI endeksler de hesaplanır. +DI, alım hareketinin gücünü, -DI ise satım hareketinin gücünü ölçer. Son olarak, belirlenen bir eşik değeri (threshold) kullanılarak alım-satım sinyalleri üretilir. Eğer ADX değeri belirli eşik değerini aşarsa, +DI ve -DI yönlü endeksler karşılaştırılır. Eğer +DI, -DI’dan büyükse, alım sinyali üretilir. Eğer -DI, +DI’dan büyükse, satım sinyali üretilir. Eğer artı ve eksi endeksler birbirine eşitse veya eşik değerini aşan bir ADX değeri yoksa, herhangi bir sinyal üretilmez. Bu şekilde, ADX indikatörü kullanılarak belirli bir varlığın trendinin gücüne bağlı olarak alım-satım kararları otomatik olarak belirlenir.\nADX stratejisinin optimize edilmesi için kullanılan parametre aralıkları şöyledir:\n\nwindow_range: Pencere boyutu aralığıdır. Değerler 10 ile 20 arasında 1’er birim artacak şekilde belirlenmiştir.\nthreshold_range: ADX indikatörünün değerini değerlendirirken kullanılacak olan eşik değerdir. Değerler 10 ile 30 arasında 1’er birim artacak şekilde belirlenmiştir.\n\nÖrneğe göre en iyi kümülatif getiriyi sağlayan parametreler şunlardır:\nBest Params (Average Directional Index): Window=11, Threshold=12\n\n\nBollinger Bands\nBollinger Bands, fiyatların ortalamadan sapmasını ölçerek volatiliteyi gösteren bir indikatördür.\nBollinger Bands için seçilen strateji, indikatörü kullanarak alım-satım sinyalleri üretir ve bu sinyalleri optimize ederek en iyi kümülatif getiriyi hedefler. Bollinger Bands, fiyatın ortalamadan ne kadar uzakta olduğunu ölçer ve standart sapma kullanarak üst ve alt bantları hesaplar. Belirli bir pencere boyutu ve standart sapma aralığı ile bu bantları hesaplar ve fiyatın bu bantların içinde veya dışında olduğu durumları değerlendirir. Fiyatın alt bant altında olduğu durumlarda alım, üst bant üstünde olduğu durumlarda satım sinyali üretir. Bu sinyallerle stratejinin getirisini hesaplar ve en iyi parametrelerle belirlenen sinyallerle işlem yapar.\n\ndef optimize_and_calculate_bollinger_bands(df, window_range, std_dev_range):\n    best_cum_return = -np.inf\n    best_params = None\n    best_strategy_df = None\n\n    for window, std_dev in product(window_range, std_dev_range):\n        strategy_df = df.copy().reset_index(drop=True)\n\n        bb = volatility.BollingerBands(close=strategy_df['Close'], window=window, window_dev=std_dev)\n        bb_lband = bb.bollinger_lband()\n        bb_hband = bb.bollinger_hband()\n\n        for i in range(len(strategy_df)):\n            lband = bb_lband.iloc[i]\n            hband = bb_hband.iloc[i]\n            close_price = strategy_df['Close'].iloc[i]\n\n            if close_price &lt; lband:\n                signal = 1\n            elif close_price &gt; hband:\n                signal = -1\n            else:\n                signal = 0\n\n            strategy_df.loc[i, 'Signal'] = signal\n\n        strategy_df['Signal Updated'] = update_signals(strategy_df['Signal'].tolist())\n\n        if not (1 in strategy_df['Signal Updated'].values and -1 in strategy_df['Signal Updated'].values):\n            continue\n\n        strategy_df_ret_calc = strategy_df.copy()\n        strategy_df_ret_calc = strategy_df_ret_calc[strategy_df_ret_calc['Signal Updated'] != 0]\n        strategy_df_ret_calc['Strategy Return'] = strategy_df_ret_calc['Close'].pct_change()\n\n        for index, row in strategy_df_ret_calc.iterrows():\n            signal = row['Signal Updated']\n            if signal == 1:\n                strategy_df_ret_calc.loc[index, 'Strategy Return'] *= -1\n\n        strategy_df_ret_calc = strategy_df_ret_calc[['Strategy Return']]\n        strategy_df = pd.merge(strategy_df, strategy_df_ret_calc, how='left', left_index=True, right_index=True)\n        strategy_df['Strategy Return'] = strategy_df['Strategy Return'].fillna(0)\n\n        strategy_df['Cumulative Return'] = (1 + strategy_df['Strategy Return']).cumprod() - 1\n        strategy_cum_return = strategy_df['Cumulative Return'].iloc[-1]\n\n        if strategy_cum_return &gt; best_cum_return:\n            best_cum_return = strategy_cum_return\n            best_params = (window, std_dev)\n            best_strategy_df = strategy_df.copy()\n\n    print(f\"Best Params (Bollinger Bands): Window={best_params[0]}, Std Dev={best_params[1]}\")\n\n    best_strategy_df['Date'] = df.index\n    best_strategy_df['Indicator'] = 'Bollinger Bands'\n    bb_signals_df = best_strategy_df[['Date', 'Signal', 'Indicator']]\n\n    return bb_signals_df\n\nwindow_range = range(10, 21)\nstd_dev_range = np.arange(1, 3.5, 0.5)\n\nbb_signals_df = optimize_and_calculate_bollinger_bands(df=df, window_range=window_range, std_dev_range=std_dev_range)\nta_rating_df = pd.concat([ta_rating_df, bb_signals_df], ignore_index=True)\n\nFonksiyondan alınan aşağıdaki kod bloğunu inceleyelim.\n\nbb = volatility.BollingerBands(close=strategy_df['Close'], window=window, window_dev=std_dev)\nbb_lband = bb.bollinger_lband()\nbb_hband = bb.bollinger_hband()\n\nfor i in range(len(strategy_df)):\n    lband = bb_lband.iloc[i]\n    hband = bb_hband.iloc[i]\n    close_price = strategy_df['Close'].iloc[i]\n\n    if close_price &lt; lband:\n        signal = 1\n    elif close_price &gt; hband:\n        signal = -1\n    else:\n        signal = 0\n\nBu kod bloğu, indikatörün hesaplanmasını ve buna göre alım-satım sinyallerinin üretilmesini sağlar. İlk olarak, kapanış fiyatları verisiyle birlikte belirli bir pencere boyutu ve standart sapma kullanılarak bir Bollinger Bands nesnesi oluşturulur. Daha sonra, alt bant ve üst bant değerleri hesaplanır. Alt bant, hareketli ortalama fiyatın belirli bir standart sapma kadar altında, üst bant ise hareketli ortalama fiyatın belirli bir standart sapma kadar üstünde yer alır. Son olarak, her bir veri noktası için kapanış fiyatı, alt bant ve üst bant değerleri karşılaştırılarak alım-satım sinyali üretilir. Eğer kapanış fiyatı alt banttan daha düşükse alım sinyali üretilir. Eğer kapanış fiyatı üst banttan daha yüksekse satım sinyali üretilir. Eğer kapanış fiyatı alt bant ve üst bant arasındaysa herhangi bir sinyal üretilmez. Bu şekilde, Bollinger Bands indikatörü kullanılarak belirli bir varlığın fiyat hareketleriyle ilgili alım-satım kararları otomatik olarak belirlenmiş olur.\nBollinger Bands stratejisinin optimize edilmesi için kullanılan parametre aralıkları şöyledir:\n\nwindow_range: Hareketli ortalama pencere boyutu aralığıdır. Değerler 10 ile 20 arasında 1’er birim artacak şekilde belirlenmiştir.\nstd_dev_range: Standart sapma değer aralığıdır. Değerler 1 ile 3.5 arasında 0.5’er birim artacak şekilde belirlenmiştir.\n\nÖrneğe göre en iyi kümülatif getiriyi sağlayan parametreler şunlardır:\nBest Params (Bollinger Bands): Window=10, Std Dev=2.5\n\n\nRate of Change\nRate of Change, belirli bir zaman diliminde fiyatların yüzde değişimini ölçerek fiyatların momentumunu gösteren bir indikatördür.\nRate of Change için seçilen strateji, indikatörü kullanarak alım-satım sinyalleri üretir ve bu sinyalleri optimize ederek en iyi kümülatif getiriyi hedefler. Belirli bir pencere boyutu ve aşırı alım/aşırı satım seviyeleri aralığı ile fiyatın geçmiş performansını ölçer. ROC değeri, belirli bir dönemde fiyatın yüzde değişimini gösterir. Ardından, aşırı alım veya aşırı satım seviyelerini geçtiğinde alım veya satım sinyali üretir. Bu sinyallerle stratejinin getirisini hesaplar ve en iyi parametrelerle belirlenen sinyallerle işlem yapar.\n\ndef optimize_and_calculate_roc(df, window_range, overbought_range, oversold_range):\n    best_cum_return = -np.inf\n    best_params = None\n    best_strategy_df = None\n\n    for window, overbought, oversold in product(window_range, overbought_range, oversold_range):\n        strategy_df = df.copy().reset_index(drop=True)\n\n        roc = momentum.ROCIndicator(strategy_df['Close'], window=window).roc()\n        signal = np.where(roc &gt; overbought, -1, np.where(roc &lt; oversold, 1, 0))\n\n        strategy_df['Signal'] = signal\n        strategy_df['Signal Updated'] = update_signals(strategy_df['Signal'].tolist())\n\n        if not (1 in strategy_df['Signal Updated'].values and -1 in strategy_df['Signal Updated'].values):\n            continue\n\n        strategy_df_ret_calc = strategy_df.copy()\n        strategy_df_ret_calc = strategy_df_ret_calc[strategy_df_ret_calc['Signal Updated'] != 0]\n        strategy_df_ret_calc['Strategy Return'] = strategy_df_ret_calc['Close'].pct_change()\n\n        for index, row in strategy_df_ret_calc.iterrows():\n            signal = row['Signal Updated']\n            if signal == 1:\n                strategy_df_ret_calc.loc[index, 'Strategy Return'] *= -1\n\n        strategy_df_ret_calc = strategy_df_ret_calc[['Strategy Return']]\n        strategy_df = pd.merge(strategy_df, strategy_df_ret_calc, how='left', left_index=True, right_index=True)\n        strategy_df['Strategy Return'] = strategy_df['Strategy Return'].fillna(0)\n\n        strategy_df['Cumulative Return'] = (1 + strategy_df['Strategy Return']).cumprod() - 1\n        strategy_cum_return = strategy_df['Cumulative Return'].iloc[-1]\n\n        if strategy_cum_return &gt; best_cum_return:\n            best_cum_return = strategy_cum_return\n            best_params = (window, overbought, oversold)\n            best_strategy_df = strategy_df.copy()\n\n    print(f\"Best Params (Rate of Change): Window={best_params[0]}, Overbought={best_params[1]}, Oversold={best_params[2]}\")\n\n    best_strategy_df['Date'] = df.index\n    best_strategy_df['Indicator'] = 'Rate of Change'\n    roc_signals_df = best_strategy_df[['Date', 'Signal', 'Indicator']]\n\n    return roc_signals_df\n\nwindow_range_roc = range(5, 21, 5)\noverbought_range_roc = range(5, 31, 5)\noversold_range_roc = range(-5, -31, -5)\n\nroc_signals_df = optimize_and_calculate_roc(df=df, window_range=window_range_roc, overbought_range=overbought_range_roc, oversold_range=oversold_range_roc)\nta_rating_df = pd.concat([ta_rating_df, roc_signals_df], ignore_index=True)\n\nFonksiyondan alınan aşağıdaki kod bloğunu inceleyelim.\n\nroc = momentum.ROCIndicator(strategy_df['Close'], window=window).roc()\nsignal = np.where(roc &gt; overbought, -1, np.where(roc &lt; oversold, 1, 0))\n\nBu kod bloğu, indikatörün hesaplanmasını ve buna göre alım-satım sinyallerinin üretilmesini sağlar. İlk olarak, kapanış fiyatları verisiyle birlikte belirli bir pencere boyutu kullanılarak bir ROC nesnesi oluşturulur. Daha sonra, ROC değerleri belirlenen bir aşırı alım ve aşırı satım seviyeleri ile karşılaştırılarak alım-satım sinyalleri üretilir. Eğer ROC değeri aşırı alım seviyesini aşarsa satım sinyali üretilir. Eğer ROC değeri aşırı satım seviyesinin altına düşerse alım sinyali üretilir. Eğer ROC değeri aşırı alım veya aşırı satım seviyeleri arasındaysa herhangi bir sinyal üretilmez. Bu şekilde, ROC indikatörü kullanılarak belirli bir varlığın fiyat momentumuyla ilişkili alım-satım kararları otomatik olarak belirlenir.\nROC stratejisinin optimize edilmesi için kullanılan parametre aralıkları şöyledir:\n\nwindow_range: Pencere boyutu aralığıdır. Değerler 5 ile 20 arasında 5’er birim artacak şekilde belirlenmiştir.\noverbought_range: Aşırı alım seviyesinin belirlenmesi için kullanılan değerlerin aralığıdır. Değerler 5 ile 30 arasında 5’er birim artacak şekilde belirlenmiştir.\noversold_range: Aşırı satım seviyesinin belirlenmesi için kullanılan değerlerin aralığıdır. Değerler -5 ile -30 arasında 5’er birim azalacak şekilde belirlenmiştir.\n\nÖrneğe göre en iyi kümülatif getiriyi sağlayan parametreler şunlardır:\nBest Params (Rate of Change): Window=5, Overbought=5, Oversold=-15\n\n\nIchimoku\nIchimoku Kinko Hyo, genellikle kısaca Ichimoku olarak bilinen, çok yönlü bir teknik analiz indikatörüdür. Ichimoku, bir varlığın destek ve direnç seviyelerini, trend yönünü, momentumunu ve olası sinyalleri belirlemeye yardımcı olur. Ichimoku, tek bir grafikte beş ana bileşeni birleştirir ve bu da onu oldukça kapsamlı bir analiz aracı yapar.\nIchimoku için seçilen strateji, indikatörü kullanarak alım-satım sinyalleri üretir ve bu sinyalleri optimize ederek en iyi kümülatif getiriyi hedefler. Ichimoku Bulutu, beş çizgiden oluşur: Tenkan-sen, Kijun-sen, Senkou Span A, Senkou Span B ve Chikou Span. Bu stratejide, Tenkan-sen ve Kijun-sen çizgilerinin kesişimlerine dayalı alım-satım sinyalleri üretilir. Ayrıca, fiyatın Senkou Span A ve Senkou Span B arasında veya dışında olması durumuna göre de alım-satım sinyalleri üretilir. Bu sinyallerle stratejinin getirisini hesaplar ve en iyi parametrelerle belirlenen sinyallerle işlem yapar.\n\ndef optimize_and_calculate_ichimoku(df, tenkan_range, kijun_range, senkou_span_b_range):\n    best_cum_return = -np.inf\n    best_params = None\n    best_strategy_df = None\n\n    for tenkan, kijun, senkou_span_b in product(tenkan_range, kijun_range, senkou_span_b_range):\n        strategy_df = df.copy().reset_index(drop=True)\n\n        ichimoku = trend.IchimokuIndicator(strategy_df['High'], strategy_df['Low'], window1=tenkan, window2=kijun, window3=senkou_span_b)\n        strategy_df['Tenkan'] = ichimoku.ichimoku_conversion_line()\n        strategy_df['Kijun'] = ichimoku.ichimoku_base_line()\n        strategy_df['Senkou_A'] = ichimoku.ichimoku_a()\n        strategy_df['Senkou_B'] = ichimoku.ichimoku_b()\n        strategy_df['Lagging_Span'] = strategy_df['Close'].shift(-kijun)\n\n        signals = []\n\n        for i in range(len(strategy_df)):\n            price = strategy_df['Close'].iloc[i]\n            tenkan_sen = strategy_df['Tenkan'].iloc[i]\n            kijun_sen = strategy_df['Kijun'].iloc[i]\n            senkou_a = strategy_df['Senkou_A'].iloc[i]\n            senkou_b = strategy_df['Senkou_B'].iloc[i]\n\n            if price &gt; max(senkou_a, senkou_b):\n                if price &gt; tenkan_sen and tenkan_sen &gt; kijun_sen:\n                    signals.append(1)\n                else:\n                    signals.append(0)\n            elif price &lt; min(senkou_a, senkou_b):\n                if price &lt; tenkan_sen and tenkan_sen &lt; kijun_sen:\n                    signals.append(-1)\n                else:\n                    signals.append(0)\n            else:\n                signals.append(0)\n\n        strategy_df['Signal'] = signals\n        strategy_df['Signal Updated'] = update_signals(strategy_df['Signal'].tolist())\n\n        if not (1 in strategy_df['Signal Updated'].values and -1 in strategy_df['Signal Updated'].values):\n            continue\n\n        strategy_df_ret_calc = strategy_df.copy()\n        strategy_df_ret_calc = strategy_df_ret_calc[strategy_df_ret_calc['Signal Updated'] != 0]\n        strategy_df_ret_calc['Strategy Return'] = strategy_df_ret_calc['Close'].pct_change()\n\n        for index, row in strategy_df_ret_calc.iterrows():\n            signal = row['Signal Updated']\n            if signal == 1:\n                strategy_df_ret_calc.loc[index, 'Strategy Return'] *= -1\n\n        strategy_df_ret_calc = strategy_df_ret_calc[['Strategy Return']]\n        strategy_df = pd.merge(strategy_df, strategy_df_ret_calc, how='left', left_index=True, right_index=True)\n        strategy_df['Strategy Return'] = strategy_df['Strategy Return'].fillna(0)\n\n        strategy_df['Cumulative Return'] = (1 + strategy_df['Strategy Return']).cumprod() - 1\n        strategy_cum_return = strategy_df['Cumulative Return'].iloc[-1]\n\n        if strategy_cum_return &gt; best_cum_return:\n            best_cum_return = strategy_cum_return\n            best_params = (tenkan, kijun, senkou_span_b)\n            best_strategy_df = strategy_df.copy()\n\n    print(f\"Best Params (Ichimoku): Tenkan={best_params[0]}, Kijun={best_params[1]}, Senkou Span B={best_params[2]}\")\n\n    best_strategy_df['Date'] = df.index\n    best_strategy_df['Indicator'] = 'Ichimoku'\n    ichimoku_signals_df = best_strategy_df[['Date', 'Signal', 'Indicator']]\n\n    return ichimoku_signals_df\n\ntenkan_range = range(5, 10)\nkijun_range = range(20, 31, 5)\nsenkou_span_b_range = range(50, 101, 10)\n\nichimoku_signals_df = optimize_and_calculate_ichimoku(df=df, tenkan_range=tenkan_range, kijun_range=kijun_range, senkou_span_b_range=senkou_span_b_range)\nta_rating_df = pd.concat([ta_rating_df, ichimoku_signals_df], ignore_index=True)\n\nFonksiyondan alınan aşağıdaki kod bloğunu inceleyelim.\n\nichimoku = trend.IchimokuIndicator(strategy_df['High'], strategy_df['Low'], window1=tenkan, window2=kijun, window3=senkou_span_b)\nstrategy_df['Tenkan'] = ichimoku.ichimoku_conversion_line()\nstrategy_df['Kijun'] = ichimoku.ichimoku_base_line()\nstrategy_df['Senkou_A'] = ichimoku.ichimoku_a()\nstrategy_df['Senkou_B'] = ichimoku.ichimoku_b()\nstrategy_df['Lagging_Span'] = strategy_df['Close'].shift(-kijun)\n\nsignals = []\n\nfor i in range(len(strategy_df)):\n    price = strategy_df['Close'].iloc[i]\n    tenkan_sen = strategy_df['Tenkan'].iloc[i]\n    kijun_sen = strategy_df['Kijun'].iloc[i]\n    senkou_a = strategy_df['Senkou_A'].iloc[i]\n    senkou_b = strategy_df['Senkou_B'].iloc[i]\n\n    if price &gt; max(senkou_a, senkou_b):\n        if price &gt; tenkan_sen and tenkan_sen &gt; kijun_sen:\n            signals.append(1)\n        else:\n            signals.append(0)\n    elif price &lt; min(senkou_a, senkou_b):\n        if price &lt; tenkan_sen and tenkan_sen &lt; kijun_sen:\n            signals.append(-1)\n        else:\n            signals.append(0)\n    else:\n        signals.append(0)\n\nBu kod bloğu, indikatörün hesaplanmasını ve buna göre alım-satım sinyallerinin üretilmesini sağlar. İlk olarak, yüksek ve düşük fiyatlarla birlikte Tenkan Sen, Kijun Sen, Senkou Span A ve Senkou Span B gibi farklı bileşenlerin hesaplanması için bir Ichimoku nesnesi oluşturulur. Daha sonra, her bir bileşenin değeri, Ichimoku nesnesinin ilgili metotları kullanılarak hesaplanır ve strateji veri çerçevesine eklenir. Ayrıca, geriye kaydırılmış bir Lagging Span bileşeni de oluşturulur. Son olarak, her bir veri noktası için fiyat ve Ichimoku bileşenlerinin değerleri karşılaştırılarak alım-satım sinyalleri üretilir. Eğer fiyat, Senkou Span A ve Senkou Span B’nin maksimum değeriyle karşılaştırıldığında daha yüksekse ve Tenkan Sen, Kijun Sen’in üzerindeyse, alım sinyali üretilir. Eğer fiyat, Senkou Span A ve Senkou Span B’nin minimum değeriyle karşılaştırıldığında daha düşükse ve Tenkan Sen, Kijun Sen’in altındaysa, satım sinyali üretilir. Aksi takdirde, herhangi bir sinyal üretilmez. Bu şekilde, Ichimoku Kinko Hyo indikatörü kullanılarak belirli bir varlığın trendi ve potansiyel alım-satım fırsatları hakkında bilgi sağlanır ve alım-satım sinyalleri otomatik olarak belirlenir.\nIchimoku stratejisinin optimize edilmesi için kullanılan parametre aralıkları şöyledir:\n\ntenkan_range: Tenkan Sen (dönüş hattı) için kullanılacak dönem aralığıdır. Değerler 5 ile 9 arasında belirlenmiştir.\nkijun_range: Kijun Sen (standart hattı) için kullanılacak dönem aralığıdır. Değerler 20 ile 30 arasında 5’er birim artacak şekilde belirlenmiştir.\nsenkou_span_b_range: Senkou Span B (ön gösterge B) için kullanılacak dönem aralığıdır. Değerler 50 ile 100 arasında 10’ar birim artacak şekilde belirlenmiştir.\n\nÖrneğe göre en iyi kümülatif getiriyi sağlayan parametreler şunlardır:\nBest Params (Ichimoku): Tenkan=6, Kijun=20, Senkou Span B=50"
  },
  {
    "objectID": "posts/post_14/index.html#reytinglerin-hesaplanması",
    "href": "posts/post_14/index.html#reytinglerin-hesaplanması",
    "title": "İndikatör Ağırlıklarının Optimize Edilerek Teknik Reytinglerin Oluşturulması ve Al-Tut Stratejisinin Alt Edilmesi",
    "section": "Reytinglerin Hesaplanması",
    "text": "Reytinglerin Hesaplanması\n\nPandas DataFrame’in Güncellenmesi\nta_rating_df’i pivot_table yardımıyla aşağıdaki formata çevirelim ve yeni tabloyu pivot_ta_rating_df’e atayalım.\n\n\npivot_ta_rating_df = ta_rating_df.pivot_table(index='Date', columns='Indicator', values='Signal')\npivot_ta_rating_df.columns.name = None\n\n\n\n\nİndikatör Listesinin Oluşturulması\nDaha önce seçtiğimiz indikatörleri bir liste içerisine alalım.\n\nindicators = [\n    'Relative Strength Index',\n    'Weighted Moving Average',\n    'Moving Average Convergence Divergence',\n    'Average Directional Index',\n    'Bollinger Bands',\n    'Rate of Change',\n    'Ichimoku'\n]\n\n\n\nReyting Aralığının Oluşturulması\nAşağıdaki kriterleri signal_to_category adında bir fonksiyonda tanımlayalım.\n\n-1.0 \\(\\leq\\) reyting \\(\\leq\\) -0.5 ise Sat\n-0.5 \\(&lt;\\) reyting \\(&lt;\\) 0.5 ise Nötr\n0.5 \\(\\leq\\) reyting \\(\\leq\\) 1.0 ise Al\n\n\ndef signal_to_category(signal):\n    if -1.0 &lt;= signal &lt;= -0.5:\n        return -1\n    elif -0.5 &lt; signal &lt; 0.5:\n        return 0\n    elif 0.5 &lt;= signal &lt;= 1.0:\n        return 1\n    else:\n        return None\n\n\n\nİndikatör Ağırlıklarının Optimize Edilmesi\nTüm süreci optimize_weights_and_evaluate adındaki fonksiyonun içerisinde yürüteceğiz.\nFonksiyon, 5 adet parametre alıyor.\n\ndf: İlgili finansal varlığın (örneğimizde BTC-USD) tarihsel verilerinin bulunduğu df’tir.\npivot_df: Son oluşturulan pivot_ta_rating_df’tir.\nindicators: İndikatör listesidir.\ninitial_value: 100 birimin ne olduğuna bakılır. Parametrenin varsayılan değeri 100’dür.\nplot: Süreç tamamlandığında ekrana üç adet görsel basılır. İlk görsel, stratejinin nerelerde alım-satım sinyali ürettiğini gösterir. İkinci görsel, strateji ile al-tut stratejisini karşılaştırır. Üçüncü görsel ise indikatörlerin ağırlıklarını verir. Parametrenin varsayılan değeri False’tur.\n\n\ndef optimize_weights_and_evaluate(df, pivot_df, indicators, initial_value=100, plot=False):\n    results = []\n    best_cum_return = -np.inf\n    best_weights = None\n    best_strategy_df = None\n    buy_and_hold_cum_return = (df['Close'].pct_change().dropna() + 1).cumprod().iloc[-1] - 1\n\n    while best_cum_return &lt;= buy_and_hold_cum_return:\n        strategy_df = pivot_df.copy()\n\n        weights = (np.random.rand(len(indicators)) / np.random.rand(len(indicators)).sum())\n        weights = weights / weights.sum()\n        weight_dict = dict(zip(indicators, weights))\n        weighted_ratings = strategy_df.apply(lambda row: sum(row[indicator] * weight_dict[indicator] for indicator in indicators), axis=1)\n\n        strategy_df['Weighted Average Rating'] = weighted_ratings\n        strategy_df['Signal'] = strategy_df['Weighted Average Rating'].apply(signal_to_category)\n        strategy_df['Signal Updated'] = update_signals(strategy_df['Signal'].tolist())\n\n        if not any(signal in strategy_df['Signal Updated'].values for signal in [1, -1]):\n            continue\n\n        strategy_df['Close'] = df['Close'].reindex(strategy_df.index)\n        strategy_df = strategy_df.reset_index(drop=True)\n\n        strategy_df_ret_calc = strategy_df.copy()\n        strategy_df_ret_calc = strategy_df_ret_calc[strategy_df_ret_calc['Signal Updated'] != 0]\n        strategy_df_ret_calc['Strategy Return'] = strategy_df_ret_calc['Close'].pct_change()\n\n        for index, row in strategy_df_ret_calc.iterrows():\n            signal = row['Signal Updated']\n            if signal == 1:\n                strategy_df_ret_calc.loc[index, 'Strategy Return'] *= -1\n\n        strategy_df_ret_calc = strategy_df_ret_calc[['Strategy Return']]\n        strategy_df = pd.merge(strategy_df, strategy_df_ret_calc, how='left', left_index=True, right_index=True)\n        strategy_df['Strategy Return'] = strategy_df['Strategy Return'].fillna(0)\n        strategy_df.iloc[0, strategy_df.columns.get_loc('Strategy Return')] = np.nan\n\n        strategy_df['Buy and Hold Return'] = strategy_df['Close'].pct_change()\n\n        strategy_df['Strategy Cumulative Return'] = (1 + strategy_df['Strategy Return']).cumprod() - 1\n        strategy_df['Buy and Hold Cumulative Return'] = (1 + strategy_df['Buy and Hold Return']).cumprod() - 1\n\n        strategy_df.iloc[0, strategy_df.columns.get_loc('Strategy Cumulative Return')] = 0\n        strategy_df.iloc[0, strategy_df.columns.get_loc('Buy and Hold Cumulative Return')] = 0\n\n        strategy_df['Strategy Values'] = initial_value * (1 + strategy_df['Strategy Cumulative Return'])\n        strategy_df['Buy and Hold Values'] = initial_value * (1 + strategy_df['Buy and Hold Cumulative Return'])\n\n        strategy_df.index = df.index\n\n        non_neutral_df = strategy_df[strategy_df['Signal Updated'] != 0]\n        non_neutral_list = non_neutral_df['Signal Updated'].tolist()\n        strategy_cum_return = strategy_df['Strategy Cumulative Return'].iloc[-1]\n\n        results.append({\n            **weight_dict,\n            'Last Signal': non_neutral_list[-1],\n            'Strategy Cumulative Return': strategy_cum_return,\n            'Buy and Hold Cumulative Return': buy_and_hold_cum_return\n        })\n\n        if strategy_cum_return &gt; best_cum_return:\n            best_cum_return = strategy_cum_return\n            best_weights = weight_dict\n            best_strategy_df = strategy_df\n\n        if best_cum_return &gt; buy_and_hold_cum_return:\n            break\n\n    results_df = pd.DataFrame(results)\n\n    if plot:\n        plt.figure(figsize=(12, 6))\n        plt.plot(best_strategy_df.index, best_strategy_df['Close'], label='Close Price', color='black')\n\n        for date, row in best_strategy_df.iterrows():\n            signal = row['Signal Updated']\n            if signal == 1 or signal == -1:\n                plt.plot(\n                    date,\n                    row['Close'],\n                    marker='^' if signal == 1 else 'v',\n                    markersize=10,\n                    markerfacecolor='green' if signal == 1 else 'red',\n                    markeredgewidth=1,\n                    markeredgecolor='black'\n                )\n\n        plt.scatter([], [], marker='^', color='g', label='Buy Signal')\n        plt.scatter([], [], marker='v', color='r', label='Sell Signal')\n        plt.legend()\n\n        plt.title('Best Strategy Signals - Cumulative Return: {:.2%}'.format(best_cum_return))\n        plt.ylabel('Price')\n        plt.grid(True)\n        plt.show()\n\n        plt.figure(figsize=(12, 6))\n        plt.plot(\n            best_strategy_df.index,\n            best_strategy_df['Strategy Values'],\n            color='blue',\n            label='Strategy'\n        )\n        plt.plot(\n            best_strategy_df.index,\n            best_strategy_df['Buy and Hold Values'],\n            color='red',\n            label='Buy and Hold'\n        )\n        plt.axhline(y=100, color='black', linestyle='--', linewidth=2)\n        plt.title(f'Strategy vs. Buy and Hold Cumulative Returns\\nStrategy: {best_cum_return:.2%}, Buy and Hold: {buy_and_hold_cum_return:.2%}')\n        plt.legend()\n        plt.grid(True)\n        plt.show()\n\n        sorted_weights = {k: v for k, v in sorted(best_weights.items(), key=lambda item: item[1], reverse=True)}\n        indicators = list(sorted_weights.keys())\n        weights = list(sorted_weights.values())\n\n        plt.figure(figsize=(10, 6))\n        bars = plt.barh(indicators, weights, color='skyblue')\n        for bar, weight in zip(bars, weights):\n            plt.text(bar.get_width(), bar.get_y() + bar.get_height()/2, f'{weight:.3f}', va='center')\n        plt.xlabel('Weights')\n        plt.title('Optimized Weights for Indicators')\n        plt.gca().invert_yaxis()\n        plt.grid(axis='x')\n        plt.show()\n\n    return results_df, best_weights\n\noptimization_results, best_weights = optimize_weights_and_evaluate(df, pivot_ta_rating_df, indicators, plot=True)\n\nYukarıdaki fonksiyon, belirtilen teknik analiz indikatörleri için optimal ağırlıkları bulmak ve bu ağırlıkları kullanarak bir alım-satım stratejisini değerlendirmek amacıyla tasarlanmıştır. Fonksiyon, al-tut stratejisini alt edecek en iyi ağırlıkları bulana kadar çalışır. İşlem adımlarını özetlemek gerekirse:\n\nAğırlıkların Belirlenmesi: Rastgele ağırlıklar oluşturulur ve bunlar normalleştirilerek (toplamı 1 yapılarak) her bir indikatöre atanır.\nStratejinin Uygulanması: Her bir indikatör için ağırlıklı ortalama hesaplanır ve bu değerler belirlenen sinyal kategorilerine (sat, nötr, al) dönüştürülür. Sinyaller güncellenir ve bu sinyallerle strateji değerlendirilir.\nGetirilerin Hesaplanması: Sinyallere göre strateji getirisi ve al-tut getirisi hesaplanır. Her iki strateji için de kümülatif getiriler hesaplanır ve başlangıç değeri (örneğin, 100 birim) ile çarpılarak strateji ve al-tut için değerler belirlenir.\nEn İyi Ağırlıkların Bulunması: Her denemede bulunan strateji getirisi kaydedilir ve en yüksek getiriye sahip olan ağırlıklar en iyi ağırlıklar olarak belirlenir.\nSonuçların Görselleştirilmesi: En iyi strateji için getiriler ve sinyaller grafiklerle gösterilir. Ayrıca, en iyi ağırlıkların hangi indikatörler için olduğunu gösteren bir çubuk grafik oluşturulur.\n\nÖrneğe göre görsel çıktıları aşağıdaki gibidir.\n\n\n\noptimization_results çıktısına göre ilk denemede bulduğunu görebiliriz.\n\nEn uygun ağırlıkları görsel yerine best_weights çıktısında da görebiliriz.\n\nGelecek içeriklerde görüşmek dileğiyle."
  },
  {
    "objectID": "posts/post_12/index.html",
    "href": "posts/post_12/index.html",
    "title": "TCMB’nin Faiz Oranlarına İlişkin Duyuru Metinlerinin İçerik Analizi",
    "section": "",
    "text": "Faiz kararları sadece sayılardan ibaret değildir. Türkiye Cumhuriyet Merkez Bankası’nın (TCMB) faiz oranlarına ilişkin basın duyuruları da büyük bir öneme sahiptir. Bu duyurular, TCMB’nin kararlarını destekleyen veya açıklayan bir metin içerir ve bu metin, ekonomi uzmanları, yatırımcılar ve piyasa analistleri için kritik bir kaynaktır. Basın duyuruları temelde, TCMB’nin ekonomik göstergeleri nasıl yorumladığını ve gelecekteki politika eğilimlerini nasıl değerlendirdiğini içerir.\nVeri seti, Erdem Başçı ve Yaşar Fatih Karahan arası dönemlerde yayınlanan 148 adet İngilizce duyuru metnini kapsamaktadır. Son veri 2024 yılının Nisan ayına aittir. cbrt_press_releases isimli excel dosyasında bulunan veri setine buradan ulaşabilirsiniz."
  },
  {
    "objectID": "posts/post_12/index.html#tarihlere-ve-başkanlara-göre-kelime-sayıları",
    "href": "posts/post_12/index.html#tarihlere-ve-başkanlara-göre-kelime-sayıları",
    "title": "TCMB’nin Faiz Oranlarına İlişkin Duyuru Metinlerinin İçerik Analizi",
    "section": "Tarihlere ve başkanlara göre kelime sayıları",
    "text": "Tarihlere ve başkanlara göre kelime sayıları\nKelimeleri sayacak word_counter isimli ve text parametresi bulunan bir fonksiyon yazalım.\n\ndef word_counter(text):\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    text = text.translate(str.maketrans('', '', '\\n\\xa0'))\n    text = text.lower()\n    tokens = word_tokenize(text)\n    return len(tokens)\n\nYukarıda ilk olarak, metindeki noktalama işaretlerini (string.punctuation ile tanımlı olanlar) kaldırıyoruz. Bu, metindeki noktalama işaretlerinin (virgül, nokta, ünlem işareti vb.) kelime sayısını etkilememesini sağlıyor. Ardından, metindeki boşlukları ve diğer belirli karakterleri (newline karakteri \\n ve non-breaking space karakteri \\xa0) kaldırıyoruz. Bu, metindeki boşlukların kelime sayısını etkilememesini sağlıyor. Metni lower() ile küçük harfe dönüştürüyoruz. Bu, büyük ve küçük harflerin ayrımını kaldırarak aynı kelimenin farklı biçimlerini aynı olarak değerlendirmesini sağlıyor. Metni word_tokenize() ile kelime parçalarına ayırıyoruz (tokenize işlemi). Bu, metindeki her bir kelimeyi ayrı bir öğe olarak ele alarak bu öğelerin sayısını saymamızı sağlıyor. Son olarak, kelime listesinin uzunluğunu len() ile hesaplayarak bu değeri döndürüyoruz.\n\ndf['Word Count'] = df['Text'].apply(word_counter)\n\nnum_governors = len(df['Governor'].unique())\ncolors = cm.Set1(np.linspace(0, 1, num_governors))\nunique_governors = df['Governor'].unique()\n\nplt.figure(figsize=(12, 7))\nfor i, governor in enumerate(unique_governors):\n    data = df[df['Governor'] == governor]\n    plt.plot(data.index, data['Word Count'], label=governor, linestyle='-', color=colors[i])\n\nplt.title('Word Counts by Dates and Governors')\nplt.text(\n    0.99,\n    -0.1,\n    \"Based on CBRT's press releases on interest rates.\",\n    verticalalignment='bottom',\n    horizontalalignment='right',\n    transform=plt.gca().transAxes,\n    color='gray',\n    fontsize=10,\n    fontstyle='italic'\n)\nplt.grid(True)\nplt.legend(fontsize='small')\nplt.tight_layout()\nplt.show()\n\nYukarıda ilk olarak, DataFrame’de bulunan Text sütunundaki her bir metin için word_counter fonksiyonunu çağırıyoruz ve her bir metnin kelime sayısı hesaplıyoruz. Görselde kullanmak için num_governors değişkenine başkan sayısını, colors değişkenine de bu başkan sayısı kadar renk atıyoruz. unique_governors ise daha sonra başkanları sırasıyla kullanacağımız için oluşturduğumuz bir değişken oluyor. Görseli oluştururken lejanttaki dönemlerine göre başkan sıralamasının önemini koruyoruz."
  },
  {
    "objectID": "posts/post_12/index.html#başkanlara-göre-kelime-bulutları",
    "href": "posts/post_12/index.html#başkanlara-göre-kelime-bulutları",
    "title": "TCMB’nin Faiz Oranlarına İlişkin Duyuru Metinlerinin İçerik Analizi",
    "section": "Başkanlara göre kelime bulutları",
    "text": "Başkanlara göre kelime bulutları\n\ngovernors_texts = df.groupby('Governor')['Text'].apply(' '.join)\n\nfig, axs = plt.subplots(7, 1, figsize=(16, 18))\nfor i, governor in enumerate(unique_governors, 1):\n    text = governors_texts[governor]\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    text = text.translate(str.maketrans('', '', '\\n\\xa0'))\n    text = text.lower()\n    wordcloud = WordCloud(\n        background_color='white',\n        colormap='gray',\n        contour_color='black',\n        contour_width=1\n    ).generate(text)\n\n    axs[i-1].imshow(wordcloud, interpolation='bilinear')\n    axs[i-1].set_title(f'Word Cloud for Governor {governor}', fontsize=12)\n    axs[i-1].axis('off')\n\nplt.figtext(\n    0.99,\n    -0.3,\n    \"Based on CBRT's press releases on interest rates.\",\n    verticalalignment='bottom',\n    horizontalalignment='right',\n    transform=plt.gca().transAxes,\n    color='gray',\n    fontsize=9,\n    fontstyle='italic'\n)\nplt.tight_layout()\nplt.show()\n\nYukarıda, governors_texts ile başkanların kendi dönemlerinde yayınlanan duyuru metinlerini bir araya getiriyoruz. Ardından, WordCloud nesnesi oluşturarak bir kelime bulutu oluşturma işlemini gerçekleştiriyoruz. Detaylı bakalım.\nWordCloud nesnesini oluştururken aşağıdaki parametreleri ve değerlerini kullandık.\nbackground_color='white', kelime bulutunun arka plan rengini beyaz yapıyor. Yani, kelime bulutunun içindeki kelimeler beyaz bir arka plan üzerine yerleştiriliyor. colormap='gray', kelime bulutunda kullanılacak renk haritasını gri tonlarda olacak şekilde ayarlıyor. Kelimelerin yoğunluklarına göre farklı gri tonları kullanılıyor. contour_color='black', kelime bulutunun kenar çizgilerinin rengini siyah yapıyor. Bu, kelime bulutunu çevreleyen konturun siyah olacağı anlamına gelir. contour_width=1, kelime bulutunun kenar çizgilerinin kalınlığı 1 piksel olarak ayarlıyor. Bu, konturun ince olacağı anlamına gelir.\ninterpolation parametresine atadığımız bilinear değeri, görüntüyü daha pürüzsüz bir şekilde yeniden örnekleme yaparak görüntülerken, kenarları daha yumuşak hale getiriyor."
  },
  {
    "objectID": "posts/post_12/index.html#tarihlere-göre-metinlerdeki-pozitifnegatif-duygu-değişimleri",
    "href": "posts/post_12/index.html#tarihlere-göre-metinlerdeki-pozitifnegatif-duygu-değişimleri",
    "title": "TCMB’nin Faiz Oranlarına İlişkin Duyuru Metinlerinin İçerik Analizi",
    "section": "Tarihlere göre metinlerdeki pozitif/negatif duygu değişimleri",
    "text": "Tarihlere göre metinlerdeki pozitif/negatif duygu değişimleri\nCentralBankRoBERTa, bir LLM’dir (large language model veya büyük dil modeli).\n\nCentralBankRoBERTa, temelde beş temel makroekonomik aktörü ayıran bir ekonomik aktör sınıflandırıcısı ile merkez bankası iletişimlerindeki cümlelerin duygusal içeriğini belirleyen ikili bir duygu sınıflandırıcısını birleştirir. Bu, merkez bankası iletişimlerindeki cümlelerin duygusal tonunu ve ekonomik aktörleri tanımak için kullanılabilir.\nMimarisi aşağıdaki gibidir.\n\nSentimentClassifier modeli, verilen bir cümlenin hane halkları, işletmeler, finans sektörü veya hükümet için olumlu mu yoksa olumsuz mu olduğunu belirlemek amacıyla tasarlanmıştır. Bu model, RoBERTa mimarisine dayanır ve doğru tahminler sağlamak için çeşitli ve kapsamlı bir veri kümesinde ince ayarlanmıştır (fine-tuned). Bir merkez bankasının iletişimini analiz etmek için duygusal içerikle ilgili uygun bir doğal dil işleme yöntemini test etmek üzere yapılan çalışmada FED, ECB ve BIS’den toplam 13,458 önceden etiketlenmiş cümle örneği kullanılmıştır.\nPerformans metrikleri şöyledir: Accuracy: 88%, F1 Score: 0.88, Precision: 0.88 ve Recall: 0.88\n\nsentiment_classifier = pipeline(\n    'text-classification',\n    model='Moritz-Pfeifer/CentralBankRoBERTa-sentiment-classifier'\n)\n\ndef classify_sentences(text):\n    text = text.replace('\\n', '').replace('\\xa0', '')\n    sentences = text.split('. ')\n    sentences = [sentence + '.' if not sentence.endswith('.') else sentence for sentence in sentences]\n    positive_count = 0\n    negative_count = 0\n    for sentence in sentences:\n        sentiment_result = sentiment_classifier(sentence)\n        positive_count += sum(1 for item in sentiment_result if item['label'] == 'positive')\n        negative_count += sum(1 for item in sentiment_result if item['label'] == 'negative')\n    return positive_count, negative_count\n\nYukarıda ilk olarak, pipeline fonksiyonunu çağırarak bir duygu sınıflandırma pipeline’ı oluşturuyoruz. Burada kullanılan model, Moritz Pfeifer tarafından oluşturulmuş CentralBankRoBERTa modelidir. Ardından, classify_sentences() fonksiyonunu tanımlıyoruz. Bu fonksiyon, bir metin alıyor ve metni cümlelere bölüyor. Metindeki \\n ve \\xa0 gibi boşlukları kaldırmak için replace() kullanıyoruz. Metni bir nokta ve boşluk ile cümlelere bölüyoruz. Cümle nokta ile bitmiyorsa cümlenin sonunda bir nokta olmasını sağlıyoruz. Pozitif ve negatif duyguların sayısını tutmak için positive_count ve negative_count isminde iki değişken tanımlıyor ve sıfır değerini atıyoruz. Her cümle için döngü oluşturuyoruz ve sentiment_classifier() fonksiyonunu çağırarak cümlenin duygusu belirliyoruz. Bu işlem, cümlenin duygusunu tahmin etmek için önceden eğitilmiş modeli kullanıyor. Duygu sonuçlarına bakarak pozitif ve negatif etiket sayılarını hesaplıyoruz. sentiment_result içindeki her bir öğe için etiketinin positive veya negative olup olmadığını kontrol ediyor ve sonuca göre ilgili sayaçları artırıyoruz. Son olarak fonksiyonda toplam pozitif ve negatif etiket sayılarını döndürüyoruz.\nAşağıda ise df’te, Positive Count ve Negative Count sütunları oluşturuyoruz ve classify_sentences() isimli text parametreli fonksiyonu çalıştırıyoruz.\n\ndf['Positive Count'] = 0\ndf['Negative Count'] = 0\n\ndf[['Positive Count', 'Negative Count']] = df['Text'].apply(classify_sentences).apply(pd.Series)\n\nplt.figure(figsize=(12, 8))\nplt.plot(df.index, df['Positive Count'], label='Positive Count', color='blue')\nplt.plot(df.index, df['Negative Count'], label='Negative Count', color='red')\nplt.fill_between(df.index, df['Positive Count'], df['Negative Count'], color='gray', alpha=0.3)\nplt.title('Positive and Negative Sentiment Counts Over Time')\nplt.text(\n    0.99,\n    -0.1,\n    \"Based on CBRT's press releases on interest rates.\",\n    verticalalignment='bottom',\n    horizontalalignment='right',\n    transform=plt.gca().transAxes,\n    color='gray',\n    fontsize=10,\n    fontstyle='italic'\n)\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\nGelecek içeriklerde görüşmek dileğiyle."
  },
  {
    "objectID": "posts/post_10/index.html",
    "href": "posts/post_10/index.html",
    "title": "Rutin Görevlerin Otomatize Edilmesi: Outlook ve Görev Zamanlayıcının Kullanılması",
    "section": "",
    "text": "Giriş\nİş verimliliğini artırmak için rutin görevlerin otomatize edilmesini önemsiyorum. Tekrarlayan rutin görevlerin otomatize edilmesi, çalışanların değerli zamanlarını stratejik ve yaratıcı faaliyetlere odaklamalarını sağlayabilir.\nBu uygulamada, Windows Görev Zamanlayıcı (Task Scheduler) ile belirlediğimiz rutin görevin otomatik olarak çalıştırılmasını sağlayacağız.\nRutin görevimiz şöyle olacak: Belirlediğimiz kripto paraların 52 + 1 haftalık kapanış fiyatlarını alacağız ve her birinin getiri ortalaması ile standart sapmasını hesaplayacağız. Getiri ortalamaları ile standart sapmaları grafik üzerine aktaracağız. Grafiği ve verileri sırasıyla PNG ve XLSX formatlarında kaydedeceğiz. Tüm bu bilgileri Microsoft Outlook uygulaması aracılığıyla göndereceğiz.\n\n\nKullanılacak Kütüphaneler\n\nimport win32com.client as win32\nimport os\nfrom datetime import datetime, timedelta\nimport yfinance as yf\nimport matplotlib.pyplot as plt\n\n\n\nGönderilecek Verilerin Çekilmesi, Hesaplamaların Yapılması ve Grafiğin Oluşturulması\n\ntickers = ['BTC-USD','ETH-USD','SOL-USD','XRP-USD','DOGE-USD','ADA-USD','AVAX-USD','DOT-USD','NEAR-USD','RNDR-USD']\n\ntoday_date = datetime.today().date()\nstart_date = today_date - timedelta(weeks=53)\n\ndf = yf.download(\n    tickers=tickers,\n    start=start_date,\n    end=today_date,\n    progress=False\n)\n\ndaily_returns = df['Adj Close'].pct_change()\n\n# print(daily_returns)\n\ndaily_returns.to_excel('cryptocurrency_returns_data.xlsx')\n\nstd_devs = daily_returns.std()\nmeans = daily_returns.mean()\n\nplt.figure(figsize=(10, 6))\nfor ticker in tickers:\n    plt.scatter(std_devs[ticker], means[ticker], s=100)\n    plt.text(std_devs[ticker], means[ticker], ticker, fontsize=8, ha='right', va='bottom')\n\nplt.title('Average and Standard Deviation of 52-Week Returns of Tracked Cryptocurrencies')\nplt.xlabel('Standard Deviation')\nplt.ylabel('Average')\nplt.grid(True)\nplt.savefig('cryptocurrency_returns_chart.png')\n# plt.show()\n\nGönderilecek grafik:\n\nGönderilecek excel dosyası içeriği:\n\n\n\nOutlook Mail İçeriğinin Hazırlanması\n\noutlook = win32.Dispatch('outlook.application')\nmail = outlook.CreateItem(0)\n\nmail.Subject = 'Cryptocurrency Returns Chart as of ' + datetime.now().strftime('%#d %b %Y %H:%M')\nmail.To = 'urazdev@gmail.com'\n\nattachment = mail.Attachments.Add(os.getcwd() + '\\cryptocurrency_returns_chart.png')\nattachment.PropertyAccessor.SetProperty('http://schemas.microsoft.com/mapi/proptag/0x3712001F', 'cryptocurrency_returns_chart')\n\nmail.HTMLBody = r\"\"\"\nDear Uraz,&lt;br&gt;&lt;br&gt;\nThe chart illustrating the average return and standard deviation for each cryptocurrency is as follows:&lt;br&gt;&lt;br&gt;\n&lt;img src=\"cid:cryptocurrency_returns_chart\"&gt;&lt;br&gt;&lt;br&gt;\nPlease find attached the Excel file containing the 52-week returns data for the tracked cryptocurrencies.&lt;br&gt;&lt;br&gt;\nBest regards,&lt;br&gt;\n@urazdev\n\"\"\"\n\nmail.Attachments.Add(os.getcwd() + '\\cryptocurrency_returns_data.xlsx')\n\nmail.Send()\n\nwin32com.client modülünün Dispatch fonksiyonu Outlook uygulaması oluşturur. Bu, Windows üzerinde Outlook’u otomatik olarak başlatmak ve programı kontrol etmek için kullanılır.\nCreateItem fonksiyonu, Outlook uygulaması üzerinden yeni bir öğe oluşturur. Burada 0 parametresi, bir e-posta öğesi oluşturmak için kullanılan sabit değerdir.\nmail.Subject, e-postanın başlığını belirtir.\nmail.To, e-postanın gönderileceği alıcı adresini belirtir.\nmail.Attachments.Add(), e-postaya bir ek ekler. Bunu hem PNG hem de XLSX için kullanıyoruz.\nattachment.PropertyAccessor.SetProperty(), eklenen dosyanın özelliklerini ayarlar. http://schemas.microsoft.com/mapi/proptag/0x3712001F, ek dosyanın gömülü dosya ismini belirlemek için kullanılır.\nmail.HTMLBody, e-postanın HTML biçimindeki gövdesini tanımlar.\nmail.Send(), oluşturulan e-postayı gönderir.\n\n\nGörev Zamanlayıcı Uygulamasına Görev Girilmesi\nGörev Zamanlayıcı uygulamasını açıyoruz ve Create Task’e tıklıyoruz.\nGeneral:\n\nTrigger:\n\nActions:\n\nProgram/Script için Python .exe dosyasının bulunduğu dosya konumu .exe uzantılı dosya ile girilmelidir. Bunun için CMD’de where python komutu çalıştırılabilir. Add arguments için çalıştırılacak script’in bulunduğu dosya konumu .py uzantılı dosya ile girilmelidir. Start in için çalıştırılacak script’in bulunduğu dosyanın konumu girilmelidir.\n\n\nTest\n\nGelecek içeriklerde görüşmek dileğiyle."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Python Blog",
    "section": "",
    "text": "world.urazdev()"
  },
  {
    "objectID": "posts/post_1/index.html",
    "href": "posts/post_1/index.html",
    "title": "Python Dünyama Hoş Geldin!",
    "section": "",
    "text": "Uygulamalı içerikler oluşturacağım bu blog, bana günlük, sana rehber olsun."
  },
  {
    "objectID": "posts/post_11/index.html",
    "href": "posts/post_11/index.html",
    "title": "TCMB Başkan Yardımcısı Cevdet Akçay’ın ‘Link Kopmuş’ Dediği Konuşmasının Duygu Analizi",
    "section": "",
    "text": "Giriş\nTürkiye Cumhuriyet Merkez Bankası Başkan Yardımcısı Cevdet Akçay, 2024 yılının ilk Enflasyon Raporu bilgilendirme toplantısında şunları aktarmıştı:\nİçinde çalıştığımız setting’de, ağırlıklı ortalama fonlama maliyeti mevduat faizi linki kopmuş, politika faizi enflasyon linki kopmuş, faiz kur linki kopmuş…\nKonuşmasına şöyle devam etmişti:\nBiz yedi aydır bu kopan linkleri tekrar ihdas ediyoruz. Bu linkler tekrar ihdas edilecek. Veriler birikecek. Alan verileri kullanacaksınız, modelleme yapacaksınız. Oradan da 36’dan 38’e çıkma ihtiyacı, çok zor. Modelleme iyi bilen arkadaşlar bu dediğimi çok iyi anlayacaktır. Çok zor değil imkansıza yakın.\nPeki, bu konuşma sırasında Akçay’ın yüzünde hangi duygular oluştu?\nX (Twitter) profil fotoğraflarını veri seti olarak kullandığımız İstanbul İlçe Belediye Başkanlarının X (Twitter) Profillerindeki Duygu Dağılımı başlıklı çalışmada Py-Feat kütüphanesinden faydalanmıştık. Video üzerinden analiz yapacağımız bu çalışmada yine Py-Feat kütüphanesinden faydalanacağız. Kütüphane ile ilgili detaylı bilgi için ilgili çalışma incelenebilir. Çalışmada kullanacağımız video ise Youtube’dan indirilebilir.\n\n\nKullanılacak Kütüphaneler\n\nimport os\nfrom feat import Detector\nimport matplotlib.pyplot as plt\n\n\n\nVideo, Frame ve FPS Kavramlarının Tanımlanması\nBir video temel olarak bir dizi fotoğrafın belirli bir hızda birbirini takip ederek oynatılmasıdır. Her bir fotoğraf karesine frame deniliyor. FPS (Frames Per Second) ise saniyedeki kare ya da frame sayısıdır. Detaylı öğrenmek isteyenler için şuradaki videoyu tavsiye edebilirim.\n\n\nVideonun İçe Aktarılması\ncbrt_cevdet_akcay.mp4 isimli video içinde bulunduğumuz dizindedir.\n\ncurrent_dir = os.getcwd()\nvideo_path = os.path.join(current_dir, 'cbrt_cevdet_akcay.mp4')\n\n\n\nDuyguların Tespit Edilmesi ve Görselleştirilmesi\n\ndetector = Detector(\n    face_model='retinaface',\n    landmark_model='mobilefacenet',\n    au_model='xgb',\n    emotion_model='resmasknet',\n    facepose_model='img2pose',\n    identity_model='facenet'\n)\n\nvideo_prediction = detector.detect_video(video_path, skip_frames=24)\nprint(video_prediction.head())\n\nskip_frames parametresi video üzerindeki karelerin işlenmesinde atlanacak kare sayısını belirtir. Bir video genellikle saniyede çok sayıda kare içerir ve bu kareler arasında önemli ölçüde benzerlik olabilir. Özellikle video çok yüksek çözünürlüklü veya uzunsa her kareyi işlemek oldukça yoğun bir hesaplama gerektirebilir. Ancak bazı uygulamalarda her kareyi işlemek gerekli olmayabilir. Bu durumda, skip_frames kullanılabilir. skip_frames parametresi, belirli sayıda kareyi atlayarak işlem süresini azaltır. Örneğin, skip_frames=24 ile her 24. kareye işlem yapılır.\n\nDuygular aşağıdaki gibi çekilebilir.\n\nakcay_emotions = video_prediction.emotions\nprint(akcay_emotions.head())\n\n\nDuyguları görselleştirelim.\n\nfig, axes = plt.subplots(nrows=7, ncols=1, figsize=(10, 20))\nfor i, (emotion, color) in enumerate(zip(akcay_emotions.columns, ['r', 'g', 'b', 'c', 'm', 'y', 'k'])):\n    ax = akcay_emotions.plot(y=emotion, ax=axes[i], color=color, legend=False)\n    ax.set_ylabel(emotion.capitalize())\n\naxes[0].set_title(\"TCMB Başkan Yardımcısı Cevdet Akçay'ın 'Link Kopmuş' Dediği Konuşmasındaki Duygular\", fontsize=14)\nplt.tight_layout()\nplt.show()\n\n\nHer karede hangi duygunun daha yüksek olduğuna bakalım.\n\nmax_emotion_per_row = akcay_emotions.idxmax(axis=1)\nemotion_distribution = max_emotion_per_row.value_counts(normalize=True) * 100\n\nplt.figure(figsize=(10, 6))\nemotion_distribution.plot(kind='bar', color='skyblue')\nplt.title('Her Karede En Yüksek Duygu Kategorisinin Dağılımı')\nplt.xlabel('Duygu Kategorisi')\nplt.ylabel('Yüzde (%)')\nplt.xticks(rotation=0)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n\nGelecek içeriklerde görüşmek dileğiyle."
  },
  {
    "objectID": "posts/post_13/index.html",
    "href": "posts/post_13/index.html",
    "title": "TCMB’nin Faiz Oranlarına İlişkin Duyuru Metinlerinin Okunabilirliği",
    "section": "",
    "text": "Giriş\nOkunabilirlik, bir metni okumanın ve anlamanın ne kadar kolay olduğunu ifade eder. Metinlerin okunabilirliği, bilgi iletişiminde kritik bir rol oynayabilir ve bir metin ne kadar açık ve anlaşılır olursa, o kadar etkili bir iletişim sağlanabilir. Bu bağlamda, Türkiye Cumhuriyet Merkez Bankası’nın (TCMB) faiz oranlarına ilişkin duyuru metinlerinin ve tabi ki diğer metinlerin okunabilirliği üzerine odaklanmak, hem finansal anlamda hem de genel olarak kamuoyuyla iletişim açısından önem taşıyabilir.\nBank of Canada (Kanada Merkez Bankası), Readability and the Bank of Canada başlıklı bir çalışmalarında, banka yayınlarının 2015-2017 yılları için okunabilirliğini incelemiş ve genel olarak banka yayınlarının normalde bankanın kitlelerinin tükettiği haber makaleleri ve diğer içerikler kadar kolay okunmadığını bulmuş. Ancak aynı zamanda bankanın uluslararasında iyi bir konumda yer aldığı sonucuna da ulaşmış.\nOkunabilirliğini inceleyeceğimiz metinler burada, cbrt_press_releases.xlsx isimli excel dosyasında bulunmaktadır. Veri seti, Erdem Başçı ve Yaşar Fatih Karahan arası dönemlerde yayınlanan 148 adet İngilizce duyuru metnini kapsamaktadır. Son veri 2024 yılının Nisan ayına aittir.\n\n\nKullanılacak Kütüphaneler\n\nimport pandas as pd\nimport re\nimport syllapy\nimport textstat\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\n\n\n\nGunning’in Fog İndeksi\nİndeks, 1952 yılında Gunning Fog tarafından geliştirilmiştir ve aşağıdaki gibi hesaplanmaktadır.\n\\(GFI = 0.4[(\\frac{Total\\ words}{Total\\ Sentences}) + 100(\\frac{Complex\\ words}{Total\\ words})]\\)\nComplex words ile kastedilen üç ve daha fazla heceli kelimelerdir.\nİndeksin seviyeleri aşağıdadır.\n\n\n\nhttps://clickhelp.com/software-documentation-tool/user-manual/gunning-fog-index.html\n\n\n\n\nVeri Setinin İçe Aktarılması ve Bazı Ayarların Yapılması\n\ndf = pd.read_excel('cbrt_press_releases.xlsx')\n\ndf['Date'] = pd.to_datetime(df['Date'], dayfirst=True)\ndf = df.sort_values(by='Date')\ndf = df.set_index('Date')\n\n\n\nİndeksin Hesaplanması\n\ndef gunning_fog_index(text):\n    pattern = r'(\\b\\d+)\\.(\\d+\\b)'\n    text = re.sub(pattern, r'\\1,\\2', text)\n    text = text.translate(str.maketrans('', '', '\\n\\xa0'))\n\n    sentences = re.split(r'(?&lt;=[.!?])\\s+', text)\n    num_sentences = len(sentences)\n\n    words = re.findall(r'\\b(?:[a-zA-Z]+(?:-[a-zA-Z]+)?|-)\\b', text)\n    num_words = len(words)\n\n    avg_words_per_sentence = num_words / num_sentences\n\n    num_complex_words = sum(1 for word in words if syllapy.count(word) &gt;= 3)\n\n    gfi = 0.4 * (avg_words_per_sentence + 100 * (num_complex_words / num_words))\n\n    return round(gfi, 2)\n\ndf['Gunning_Fog_Index'] = df['Text'].apply(gunning_fog_index)\n\n\n\nİndeksin Görselleştirilmesi\n\nplt.figure(figsize=(12, 7))\nplt.scatter(df.index, df['Gunning_Fog_Index'], c=df['Gunning_Fog_Index'], cmap='coolwarm')\nplt.title(\"CBRT's Clarity Rating by Gunning's Fog Index\")\nplt.text(\n    0.99,\n    -0.1,\n    \"Based on CBRT's press releases on interest rates.\",\n    verticalalignment='bottom',\n    horizontalalignment='right',\n    transform=plt.gca().transAxes,\n    color='gray',\n    fontsize=10,\n    fontstyle='italic'\n)\nplt.colorbar()\nplt.show()\n\n\nİndeks değeri arttıkça okunabilirliğin zorlaştığını söyleyebiliriz. Tarihsel bazda baktığımızda okunabilirliğin gittikçe zorlaştığını görüyoruz.\n\ngovernor_stats = df.groupby('Governor')['Gunning_Fog_Index'].agg(['mean', 'std'])\n\nplt.figure(figsize=(10, 6))\nplt.scatter(governor_stats['mean'], governor_stats['std'], color='red', alpha=0.3, s=200)\n\nfor i, governor in enumerate(governor_stats.index):\n    plt.text(governor_stats['mean'][i], governor_stats['std'][i], governor, fontsize=11)\n\nplt.title(\"Governors' Gunning's Fog Index Rating\")\nplt.xlabel('Average')\nplt.ylabel('Standard Deviation')\nplt.grid(True)\nplt.text(\n    0.99,\n    -0.2,\n    \"Based on CBRT's press releases on interest rates.\",\n    verticalalignment='bottom',\n    horizontalalignment='right',\n    transform=plt.gca().transAxes,\n    color='gray',\n    fontsize=10,\n    fontstyle='italic'\n)\nplt.show()\n\n\nErdem Başçı ve Murat Çetinkaya dönemlerindeki metinler diğer dönemlere kıyasla daha kolay okunabilir duruyor. Çetinkaya’nın döneminde standart sapma daha düşük olduğu için kolay okunabilirlik Başçı’ya göre daha homojendir. Diğer taraftan, Naci Ağbal ve Yaşar Fatih Karahan dönemlerindeki metinler diğer dönemlere kıyasla daha zor okunabilir duruyor. Karahan’ın döneminde standart sapma daha düşük olduğu için zor okunabilirlik Ağbal’a göre daha homojendir.\n\n\ntextstat Paketini Neden Kullanmadık?\ntextstat paketi yaygın olarak kullanılsa da aşağıdaki görselde görüleceği üzere indeks değerleri arasında ciddi farklılıklar oluşmaktadır.\n\ndef textstat_gunning_fog_index(text):\n    pattern = r'(\\b\\d+)\\.(\\d+\\b)'\n    text = re.sub(pattern, r'\\1,\\2', text)\n    text = text.translate(str.maketrans('', '', '\\n\\xa0'))\n\n    gfi = textstat.gunning_fog(text)\n\n    return round(gfi, 2)\n\ndf['Gunning_Fog_Index_textstat'] = df['Text'].apply(textstat_gunning_fog_index)\n\nplt.figure(figsize=(10, 6))\nplt.scatter(\n    df['Gunning_Fog_Index'],\n    df['Gunning_Fog_Index_textstat'],\n    c=df['Gunning_Fog_Index'],\n    cmap='coolwarm'\n)\nplt.title(\"Comparison of Gunning's Fog Index Calculation: Manual vs. Using textstat Package\", fontsize=14)\nplt.xlabel('Manual')\nplt.ylabel('textstat Package')\nplt.text(\n    0.99,\n    -0.2,\n    \"Based on CBRT's press releases on interest rates.\",\n    verticalalignment='bottom',\n    horizontalalignment='right',\n    transform=plt.gca().transAxes,\n    color='gray',\n    fontsize=10,\n    fontstyle='italic'\n)\nplt.show()\n\n\nTeyit etmek için son duyuru metnini (25/04/2024) alalım.\n\ntext = df.loc['2024-04-25','Text']\npattern = r'(\\b\\d+)\\.(\\d+\\b)'\ntext = re.sub(pattern, r'\\1,\\2', text)\ntext = text.translate(str.maketrans('', '', '\\n\\xa0'))\n\nBenchmark olarak şuradaki web sitesini alabiliriz. İlgili web sitesinin yukarıdaki örnek metin için çıktısı aşağıdaki gibidir.\n\nManuel hesaplama ile yukarıdaki sonuçları karşılaştıralım.\nCümle sayısı 17.\n\nsentences = re.split(r'(?&lt;=[.!?])\\s+', text)\nnum_sentences = len(sentences)\nprint(num_sentences)\n\nKelime sayısı 379. 2 kelime farkı sayılardan geliyor. Sayıları çıkarmıştık.\n\nwords = re.findall(r'\\b(?:[a-zA-Z]+(?:-[a-zA-Z]+)?|-)\\b', text)\nnum_words = len(words)\nprint(num_words)\n\nHece sayısı 750. 3 hecelik fark web sitesi ile kullandığımız syllapy paketi arasındaki yöntem farklılığından kaynaklanıyor. Örneğin, web sitesinde “ongoing” için hece sayısını 2 verirken, paket ve online ortamdaki kaynaklar 3 veriyor.\n\nsyllable_counter = 0\nfor word in words:\n    word_syllable = syllapy.count(word)\n    syllable_counter += word_syllable\n    print(f\"{word}, {word_syllable}\")\nprint(syllable_counter)\n\nİndeks değerini 22.53 buluyoruz. Benchmark aldığımız web sitesi 21.98 buluyor. Pek farklılık yok.\n\ngunning_fog_index(text)\n\ntextstat paketi ise sonucu 15.15 veriyor ki ciddi anlamda sapıyor.\n\ntextstat.gunning_fog(text)\n\n\n\nTürkçeye Uyarlanan Okunabilirlik Formülleri\nÇalışmayı İngilizce metinler üzerinden yapsak da Türkçe metinler üzerinden de okunabilirlik ölçülebilir.\n\nAteşman Okunabilirlik Formülü (ülkemizde bu alanda yapılan ilk çalışma)\nÇetinkaya-Uzun Okunabilirlik Formülü\nBezirci-Yılmaz Okunabilirlik Formülü\nSönmez Formülü\n\nGelecek içeriklerde görüşmek dileğiyle."
  },
  {
    "objectID": "posts/post_15/index.html",
    "href": "posts/post_15/index.html",
    "title": "Türkçe Finans Haberlerinde Duygu Sınıflandırması",
    "section": "",
    "text": "Finans dünyası, her gün çok sayıda yatırımcı ve profesyonel için hayati öneme sahip bilgilerle doludur. Hisse senetleri, piyasa hareketleri ve ekonomik göstergeler gibi konular, yatırımcıların kararlarını doğrudan etkiler. Ancak bu bilgilerin nasıl yorumlandığı ve hissedilen duygular da yatırım stratejilerinde kritik bir rol üstlenir. Buradan hareketle, Türkçe finans haberlerinde duygu sınıflandırması konusunu ele alacağız.\nBaşlangıç noktamız, Türkçe finans haberlerinden elde edilen başlıkları çekip İngilizceye çeviren bir veri işleme sürecidir. Ardından, bu başlıkları duygu analizi yapabilen bir yapay zeka modeli üzerinden değerlendireceğiz.\nİlk adım olarak, bu hafta içerisinde yayın hayatına başlayan cnbc-e’den finans haberlerini çekiyoruz ve başlıkları analiz için hazırlıyoruz. Ardından, Google Çeviri aracılığıyla Türkçe başlıkları İngilizceye çeviriyoruz. Bu adım, finans haberlerine erişim sağlayarak dil bariyerlerini aşmamıza yardımcı olur. Daha sonra, derin öğrenme tabanlı bir duygu analizi modeli kullanarak çevrilen başlıkların duygu tonlamalarını değerlendiriyoruz. Bu model, her başlığın olumlu, olumsuz veya nötr olarak sınıflandırılmasına olanak tanır. Bu sayede, finans haberlerinin yatırımcılar üzerindeki duygusal etkilerini anlama ve analiz etme yeteneğimizi artırabiliriz.\nSonuçlarımızı görsel olarak sunmak için etiketlenmiş başlıkları renklendiriyoruz. Bu, her bir başlığın duygusal tonunu hızlıca görselleştirmemize ve verileri daha anlamlı hale getirmemize yardımcı olur."
  },
  {
    "objectID": "posts/post_15/index.html#pipelineın-oluşturulması",
    "href": "posts/post_15/index.html#pipelineın-oluşturulması",
    "title": "Türkçe Finans Haberlerinde Duygu Sınıflandırması",
    "section": "Pipeline’ın Oluşturulması",
    "text": "Pipeline’ın Oluşturulması\n\nsentiment_analysis = pipeline(\n    'sentiment-analysis',\n    model='soleimanian/financial-roberta-large-sentiment'\n)\n\nFinancial-RoBERTa adlı önceden eğitilmiş doğal dil işleme modeli, finansal metinlerin duygusunu analiz etmek için özel olarak tasarlanmıştır. Bu metinler arasında finansal beyanlar (financial statements), kazanç duyuruları (earnings announcements), kazanç çağrısı metinleri (earnings call transcripts), kurumsal sosyal sorumluluk (CSR) raporları (corporate social responsibility (CSR) reports), çevresel, sosyal ve yönetişim (ESG) haberleri (environmental, social, and governance (ESG) news), finansal haberler (financial news) gibi çeşitli belgeler bulunmaktadır.\nFinancial-RoBERTa, RoBERTa büyük dil modelini temel alarak daha fazla eğitim ve ayarlama ile oluşturulmuştur. Bu süreçte, 10K ve 10Q raporları, 8K bildirimleri, kazanç çağrısı transkriptleri, CSR raporları, ESG haberleri ve finansal haberler gibi geniş bir metin korpusu kullanılmıştır.\nModel, metin girdilerini alır ve bu metinlerin duygu durumunu belirlemek için softmax çıktılarını üretir. Çıktılar üç temel etiket üzerinden tanımlanır: Pozitif, Negatif veya Nötr.\nFinancial-RoBERTa’nın temel amacı, bu çeşitli finansal belgeleri otomatik olarak analiz ederek, yatırımcıların ve finans profesyonellerinin büyük veri setleri içindeki önemli bilgileri daha hızlı ve etkili bir şekilde keşfetmelerine yardımcı olmaktır."
  },
  {
    "objectID": "posts/post_15/index.html#duyguların-oluşturulması",
    "href": "posts/post_15/index.html#duyguların-oluşturulması",
    "title": "Türkçe Finans Haberlerinde Duygu Sınıflandırması",
    "section": "Duyguların Oluşturulması",
    "text": "Duyguların Oluşturulması\n\nresults = []\nfor english_title in df['English']:\n    result = sentiment_analysis(english_title)\n    results.append(result[0])\n\ndf['Label'] = [res['label'].title() for res in results]\ndf['Score'] = [res['score'] for res in results]"
  },
  {
    "objectID": "posts/post_15/index.html#sonuçların-veri-çerçevesinde-renklendirilerek-gösterilmesi",
    "href": "posts/post_15/index.html#sonuçların-veri-çerçevesinde-renklendirilerek-gösterilmesi",
    "title": "Türkçe Finans Haberlerinde Duygu Sınıflandırması",
    "section": "Sonuçların Veri Çerçevesinde Renklendirilerek Gösterilmesi",
    "text": "Sonuçların Veri Çerçevesinde Renklendirilerek Gösterilmesi\n\ndef color_label(val):\n    color = ''\n    if val == 'Positive':\n        color = 'background-color: skyblue; color: black'\n    elif val == 'Negative':\n        color = 'background-color: lightcoral; color: black'\n    elif val == 'Neutral':\n        color = 'background-color: lightgrey; color: black'\n    return color\n\nstyled_df = df.style.applymap(color_label, subset=['Label'])\n\n\nGelecek içeriklerde görüşmek dileğiyle."
  },
  {
    "objectID": "posts/post_17/index.html",
    "href": "posts/post_17/index.html",
    "title": "Temel Bileşenler Analizi ile 2024 Avrupa Futbol Şampiyonası İlk Grup Maçları Sonrası Takım Konumlarının Belirlenmesi",
    "section": "",
    "text": "Futbol istatistikleri genellikle çok boyutlu verilerden oluşur ve her bir takımın başarısını anlamak için bu verileri etkili bir şekilde işlemek zor olabilir. İşte bu noktada, Temel Bileşenler Analizi devreye girebilir. Bu metodoloji, futbol maçları sırasında elde edilen çeşitli istatistikleri (pas yüzdesi, topa sahip olma süresi, şut sayısı gibi) bir araya getirerek her bir takımın genel performansını özetleyen temel bileşenleri belirler.\nTemel Bileşenler Analizi’nin en büyük avantajlarından biri, büyük miktarda veriyi daha küçük bir sete indirgeyerek veri setinin karmaşıklığını azaltmasıdır. Bu sayede, her bir takımın grup aşamasındaki performansını karşılaştırmak ve görsel olarak analiz etmek daha kolay hale gelir. Grafiksel olarak sunulan bu veriler, takımların güçlü ve zayıf yönlerini daha net bir şekilde ortaya koymamıza olanak tanır."
  },
  {
    "objectID": "posts/post_17/index.html#tanım",
    "href": "posts/post_17/index.html#tanım",
    "title": "Temel Bileşenler Analizi ile 2024 Avrupa Futbol Şampiyonası İlk Grup Maçları Sonrası Takım Konumlarının Belirlenmesi",
    "section": "Tanım",
    "text": "Tanım\nTemel Bileşenler Analizi (Principal Component Analysis, PCA), istatistiksel bir tekniktir ve çok değişkenli veri setlerini daha az değişkenli temel bileşenlere dönüştürmek için kullanılır. Bu analiz yöntemi, veri setindeki değişkenler arasındaki ilişkileri anlamamızı sağlar ve veri setini daha anlaşılır hale getirir.\nTemel Bileşenler Analizi’nin başlıca avantajları şunlardır:\n\nBoyut Azaltma: Çok sayıda değişken içeren bir veri setini daha az sayıda temel bileşene indirger.\nDesenleri Ortaya Koyma: Veri setindeki gizli desenleri ve ilişkileri açığa çıkarır.\nGörselleştirme: Yüksek boyutlu verileri daha düşük boyutlu uzaylarda görsel olarak temsil etme imkanı sağlar.\nVeri Sıkıştırma: Veri setinin karmaşıklığını azaltarak işleme ve yorumlama kolaylığı sağlar."
  },
  {
    "objectID": "posts/post_17/index.html#uygulama",
    "href": "posts/post_17/index.html#uygulama",
    "title": "Temel Bileşenler Analizi ile 2024 Avrupa Futbol Şampiyonası İlk Grup Maçları Sonrası Takım Konumlarının Belirlenmesi",
    "section": "Uygulama",
    "text": "Uygulama\n\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(df)\n\npca = PCA(n_components=2)\nprincipal_components = pca.fit_transform(scaled_data)\n\npca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\npca_df['Team'] = df.index\n\nsorted_teams = pca_df.sort_values(by=['PC1', 'PC2'], ascending=False)\nsorted_teams['Team'] = sorted_teams['Team'].str.split('_').str[0].str.rstrip()"
  },
  {
    "objectID": "posts/post_17/index.html#yüklemeler-ve-takım-konumlarının-belirlenmesi",
    "href": "posts/post_17/index.html#yüklemeler-ve-takım-konumlarının-belirlenmesi",
    "title": "Temel Bileşenler Analizi ile 2024 Avrupa Futbol Şampiyonası İlk Grup Maçları Sonrası Takım Konumlarının Belirlenmesi",
    "section": "Yüklemeler ve Takım Konumlarının Belirlenmesi",
    "text": "Yüklemeler ve Takım Konumlarının Belirlenmesi\nYüklemeler (loadings), her bir orijinal değişkenin temel bileşene katkısını temsil eder. Temel Bileşenler Analizi yüklemeleri, değişkenler arasındaki örüntüleri ve ilişkileri anlamak için kullanılır.\n\nloadings = pd.DataFrame(pca.components_.T, index=df.columns, columns=['PC1', 'PC2']) * np.sqrt(pca.explained_variance_)\n\n\nYukarıdaki tablo ya da yüklemeler temelde iki şekilde yorumlanır.\n\nPozitif ve negatif\n\nBir değişkenin bir ana bileşenle pozitif yükleme değeri varsa, bu değişkenin bu ana bileşenle pozitif bir ilişkisi olduğunu gösterir. Yani, ana bileşen arttığında bu değişkenin değeri de artar. Bir değişkenin bir ana bileşenle negatif yükleme değeri varsa, bu değişkenin bu ana bileşenle negatif bir ilişkisi olduğunu gösterir. Yani, ana bileşen arttığında bu değişkenin değeri azalır.\n\nDaha yüksek mutlak değerler\n\nBir değişkenin yükleme değerinin mutlak değeri ne kadar yüksekse, o değişkenin ilgili ana bileşene katkısı o kadar güçlüdür. Bir değişkenin yükleme değerinin mutlak değeri düşükse, bu değişkenin o ana bileşene katkısı daha zayıftır.\n\nplt.figure(figsize=(14, 10))\nsns.scatterplot(x='PC1', y='PC2', data=sorted_teams, s=100, alpha=0)\nhighlight_teams = ['Türkiye', 'Portugal', 'Czechia', 'Georgia']\nhighlight_color = 'red'\n\nfor i in range(sorted_teams.shape[0]):\n    team_name = sorted_teams['Team'][i]\n    if team_name in highlight_teams:\n        plt.text(\n            x=sorted_teams.PC1[i]+0.1,\n            y=sorted_teams.PC2[i],\n            s=team_name,\n            fontdict=dict(color='black', size=12),\n            bbox=dict(facecolor=highlight_color, alpha=0.5)\n        )\n    else:\n        plt.text(\n            x=sorted_teams.PC1[i]+0.1,\n            y=sorted_teams.PC2[i],\n            s=team_name,\n            fontdict=dict(color='black', size=12)\n        )\n\nfor i in range(loadings.shape[0]):\n    plt.arrow(0, 0, loadings.PC1[i]*3, loadings.PC2[i]*3, color='blue', alpha=0.5)\n    plt.text(loadings.PC1[i]*3.7, loadings.PC2[i]*3.7, loadings.index[i], color='blue', ha='center', va='center')\n\nplt.title('Euro 2024 Team Standings After Matchday 1')\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.grid(True)\nplt.show()\n\n\nTürkiye ile beraber grubumuzdaki diğer takımları da değerlendirelim.\nTürkiye:\n\nPC1 (2.390800): Topa sahip olma, pas doğruluğu, pas sayısı gibi metriklerde yüksek değerlere sahip.\nPC2 (-1.030163): Mesafe kat etme ve belirli savunma istatistiklerinde etkili olabilir.\nTürkiye, hücum ve topa sahip olma konularında güçlü görünüyor. Ancak, savunma zafiyetleri veya düşük mesafe kat etme gibi konularda eksiklikleri olabilir.\n\nPortekiz:\n\nPC1 (4.386746): Topa sahip olma, pas doğruluğu, pas sayısı, hücum girişimleri gibi metriklerde yüksek performansa sahip.\nPC2 (-0.983510): Mesafe kat etme ve savunma istatistiklerinde zayıflık olabilir.\nPortekiz, hücum ve topa sahip olma konularında mükemmel bir performans sergiliyor. Ancak savunma veya mesafe kat etme gibi konularda bazı eksiklikler olabilir.\n\nPortekiz oldukça güçlü bir rakip, özellikle hücumda çok etkili oldukları görülüyor. Ancak, savunma zafiyetleri onların zayıf noktası olabilir. Türkiye’nin bu zayıflıkları kullanması gerekiyor.\nÇekya:\n\nPC1 (-4.636725): Topa sahip olma, pas doğruluğu, pas sayısı ve hücum girişimlerinde düşük performansa sahip.\nPC2 (-0.033145): Savunma veya mesafe kat etme konularında ortalama performans gösteriyor.\nÇekya, genel olarak hücum ve topa sahip olma metriklerinde düşük performans göstermektedir. Savunma veya mesafe kat etme konularında ise ortalama bir performansa sahiptir.\n\nÇekya bizim için endişe kaynağı olmayacak bir rakip gibi görünüyor. Hücumda ve topa sahip olma konusunda oldukça zayıflar.\nGürcistan:\n\nPC1 (-0.290562): Orta düzeyde topa sahip olma, pas doğruluğu, pas sayısı ve hücum girişimlerine sahip.\nPC2 (-0.403487): Mesafe kat etme ve belirli savunma istatistiklerinde düşük performans gösteriyor.\nGürcistan, hem hücum hem de savunma metriklerinde ortalama altı bir performans sergilemektedir.\n\nGürcistan da ortalama altı bir performans sergiliyor. Özellikle savunma ve mesafe kat etme konusunda zayıflar.\nVeri seti, maçların sonucunda oluşan temel istatistikler ile oluşturulmuştur. Türkiye’nin rakibinin zayıf olması onu yenebileceğimiz anlamı taşımamalıdır. Yani, çok güçlü bir takım karşısında zayıf performans göstermiş olabilir. Sonuç olarak, bizim de en az o çok güçlü takım kadar performans göstermemiz gerekiyor.\nGelecek içeriklerde görüşmek dileğiyle."
  },
  {
    "objectID": "posts/post_3/index.html",
    "href": "posts/post_3/index.html",
    "title": "Anlık Veri Akışı ile Oy Öngörüsü",
    "section": "",
    "text": "Giriş\nSeçim öngörüleri için seçim anketlerini kullanabilir veya seçim sonucuna etkisi olan değişkenler ile modelleme yapabiliriz. Modellemeye seçim anketleri de dahil edilebilir pek tabi.\nYukarıdaki iki yönteme ek olarak, seçim akşamı gelen veri akışı da hangi adayın/partinin ne kadarlık bir oy alacağı noktasında öngörüde bulunmamıza yardımcı olabilir. Bu yöntem, diğer yöntemlere göre daha kısa vadelidir. Çünkü veriler seçim akşamı alınabilmektedir. Amacı ise daha akışın başında hem oy oranlarının gidişatını kontrol edebilmek hem de öngörüde bulunabilmektir. Peki, bu yöntemde başarılı bir öngörü nasıl olur? Bunu üç kritere bağlıyorum: Gerçek sonuca yakın, istikrarlı güncellemeye sahip ve öngörüsü olabildiğince erkenden yapılan. Nedenlerini açıklayayım.\n\nÖngörümüzün gerçek sonuca yakın olmasını isteriz. Gerçekten uzak bir öngörü kimseyi tatmin etmez.\nModel güncellenebilir ancak güncellenen modelin güven de vermesi gerekir. Oynaklık seviyesi yüksek bir güncelleme modele olan güveni düşürecektir.\nGerçeğe yakın bir öngörü veri akışının başında da yapılabilir sonunda da. Veri akışının başında yapılan gerçeğe yakın öngörü daha anlamlı olacaktır.\n\nYapacağımız uygulamada İstanbul ilini ve Anadolu Ajansı verilerini baz alacağız. Verileri 31 Mart 2024 akşamı izlediğim NOW kanalından aldım. Veri seti, açılan sandık oranı ve adayların oy oranları ile aralarındaki farkları içermektedir. aa_20240331_istanbul isimli JSON verisine buradan ulaşabilirsiniz.\n\n\nKullanılacak Kütüphaneler\n\nimport json\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nfrom scipy.stats import shapiro\nimport statsmodels.stats.diagnostic as dg\nimport matplotlib.pyplot as plt\n\n\n\nAçılan Sandık Oranlarına Göre Adayların Oy Oranları ve Aralarındaki Farklar\n\nwith open('aa_20240331_istanbul.json', 'r', encoding='utf-8') as file:\n    data = json.load(file)\n\ndf = pd.DataFrame(data['Data'])\n\nprint(df.head())\n\n\n\nplt.figure(figsize=(10, 6))\nplt.plot(\n    df['Açılan Sandık Oranı'],\n    df['Ekrem İmamoğlu'],\n    label='Ekrem İmamoğlu',\n    color='red',\n    linewidth=3\n)\nplt.plot(\n    df['Açılan Sandık Oranı'],\n    df['Murat Kurum'],\n    label='Murat Kurum',\n    color='orange',\n    linewidth=3\n)\nplt.xlabel('Açılan Sandık Oranı')\nplt.ylabel('Oy Oranı')\nplt.title('31 Mart 2024 Yerel Seçim - İstanbul')\nplt.text(\n    0.97,\n    -0.14,\n    'NOW kanalından alınan Anadolu Ajansı verileridir.',\n    color='gray',\n    fontsize=8,\n    fontstyle='italic',\n    ha='right',\n    transform=plt.gca().transAxes\n)\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\nAnadolu Ajansı geçmiş seçimlerde, gözlemlediğimiz üzere, veri akışını AKP ve ittifak ortaklarının adayları lehine başlatmıştır. 31 Mart 2024 yerel seçimlerinde ise CHP’nin adayı Ekrem İmamoğlu lehine başlamıştır. Anadolu Ajansı’nın geçmiş veri akışlarını göz önüne aldığımızda, bu başlangıç hem Ekrem İmamoğlu’nun seçimi kazanacağını hem de rakibi Murat Kurum’a fark atacağını göstermekteydi. Nitekim 11.55 puanlık fark ile öyle de oldu.\n\nplt.figure(figsize=(10, 6))\nplt.plot(\n    df['Açılan Sandık Oranı'],\n    df['Fark'],\n    color='blue',\n    linewidth=3\n)\nplt.xlabel('Açılan Sandık Oranı')\nplt.ylabel('Oy Oranı Farkı')\nplt.title('31 Mart 2024 Yerel Seçim - İstanbul')\nplt.text(\n    0.97,\n    -0.14,\n    'NOW kanalından alınan Anadolu Ajansı verileridir.\\nPozitif fark Ekrem İmamoğlu lehinedir.',\n    color='gray',\n    fontsize=8,\n    fontstyle='italic',\n    ha='right',\n    transform=plt.gca().transAxes\n)\nplt.grid(True)\nplt.show()\n\n\n\n\nModelin Geliştirilmesi\nVeri setimizde bulunan gözlem sayısı 50’dir ancak biz tamamını kullanmayacağız. Öngörülerimizi olabildiğince erken yapmalıyız. Bunun için açılan sandık oranının aşağı yukarı %50 olmasını bekleyebiliriz. Bu da 20 adet gözlem sayısına denk gelecektir. Oldukça küçük fakat kullanılamaz değil.\nÖngörü için regresyon modellerini kullanacağız.\n\ndependent_variable='Ekrem İmamoğlu'\nindependent_variable='Açılan Sandık Oranı'\nballot_box_rate=50\n\nmain_df = df[df[independent_variable] &lt;= ballot_box_rate][[independent_variable, dependent_variable]]\n\nRegresyon modelinde dependent_variable Ekrem İmamoğlu, independent_variable Açılan Sandık Oranı olacak. Burada Ekrem İmamoğlu oylarının açılan sandık oranlarına bağımlı olduğunu varsayıyoruz.\nBaşlamadan önce Ekrem İmamoğlu’nun oy grafiğini görelim.\n\nplt.figure(figsize=(10, 6))\nplt.plot(\n    df[independent_variable],\n    df[dependent_variable],\n    color='red',\n    linewidth=3\n)\nplt.axvline(x=main_df[independent_variable].iloc[-1], color='black', linestyle='--')\nplt.xlabel(f'{independent_variable}')\nplt.ylabel(f'{dependent_variable}')\nplt.title('31 Mart 2024 Yerel Seçim - İstanbul')\nplt.text(\n    0.97,\n    -0.14,\n    'NOW kanalından alınan Anadolu Ajansı verileridir.',\n    color='gray',\n    fontsize=8,\n    fontstyle='italic',\n    ha='right',\n    transform=plt.gca().transAxes\n)\nplt.grid(True)\nplt.show()\n\n\nGrafikte X ekseninde dikey bir çizgi bulunmaktadır. Bu çizginin sol tarafını kullanacak ve sağ tarafını hiç görmediğimizi varsayacağız.\nHer iki değişkenin de doğrusal olduğu regresyon modelini kurarak başlayalım.\n\ny = main_df[dependent_variable]\nX = main_df[independent_variable]\nX = sm.add_constant(X)\n\nmodel_linlin = sm.OLS(y, X).fit()\n\nprint(model_linlin.summary())\n\nTahminleri güven aralığı değerleri ile alalım.\n\nalpha=0.05\n\npredictions = model_linlin.get_prediction(X).summary_frame(alpha)\npredictions_mean = predictions['mean']\npredictions_lower = predictions['mean_ci_lower']\npredictions_upper = predictions['mean_ci_upper']\n\nHata terimlerini alalım.\n\nresiduals = model_linlin.resid\n\n\nSıfıra oldukça yakın p değerlerine sahip const kesme terimi ve Açılan Sandık Oranı değişkeni katsayılarının istatistiksel olarak anlamlı olduğunu söyleyebiliriz. Ayrıca, yaklaşık olarak %82’lik bir \\(R^2\\) yakaladık ki 0-1 ya da 0-100 aralığında değer aldığını düşünürsek bu iyi bir orandır. Regresyon modelindeki Açılan Sandık Oranı değişkeni sıfır olduğunda Ekrem İmamoğlu’nun oy oranı %49.3 olmaktadır. Buna ek olarak, Açılan Sandık Oranı değişkenindeki 1 puanlık artış Ekrem İmamoğlu oyunu ortalamada 0.0164 puan artırmaktadır.\n%95 güven aralığında gerçek ve tahmin edilen değerleri görelim.\n\nplt.figure(figsize=(10,6))\nplt.plot(X.iloc[:, 1], y, color='red', label='Gerçek')\nplt.plot(X.iloc[:, 1], predictions_mean, color='blue', label='Tahmin Edilen')\nplt.fill_between(X.iloc[:, 1], predictions_lower, predictions_upper, color='gray', alpha=0.3, label=f'%{100-alpha*100} Güven Aralığı')\nplt.xlabel(f'{independent_variable}')\nplt.ylabel(f'{dependent_variable}')\nplt.title('Lin-Lin Model')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\nModelimiz fena durmasa da varsayımları sağlayıp sağlamadığına bakalım.\nHata terimleri normal dağılmaktadır.\n\nsw_statistic, sw_p_value = shapiro(residuals)\nif sw_p_value &gt;= alpha:\n    print('Hata terimleri normal dağılıyor.')\nelse:\n    print('Hata terimleri normal dağılmıyor.')\n\nHata terimlerinde otokorelasyon bulunmamaktadır.\n\nbg_test_statistic, bg_p_value, _, _ = dg.acorr_breusch_godfrey(model_linlin, nlags=5)\nif bg_p_value &gt;= alpha:\n    print('Hata terimleri arasında otokorelasyon yoktur.')\nelse:\n    print('Hata terimleri arasında otokorelasyon vardır.')\n\nModelde, alpha değerini 0.01 aldığımızda değişen varyans bulunmamaktadır.\n\nbp_test_statistic, bp_p_value, _, _ = dg.het_breuschpagan(residuals, model_linlin.model.exog)\n\nif bp_p_value &lt;= 0.01: # alpha\n    print('Modelde değişen varyans vardır.')\nelse:\n    print('Modelde değişen varyans yoktur.')\n\nEkrem imamoğlu için oy öngörüsünde bulunalım.\n\nX_value = [1, 100]\n\nforecasts = model_linlin.get_prediction(X_value).summary_frame(alpha)\nforecasted_vote_mean = forecasts['mean']\nforecasted_vote_lower = forecasts['obs_ci_lower']\nforecasted_vote_upper = forecasts['obs_ci_upper']\nprint(f'Oy Öngörüsü: %{forecasted_vote_mean.item():.2f}\\nOy Öngörüsü Aralığı: %{forecasted_vote_lower.item():.2f} - %{forecasted_vote_upper.item():.2f}')\n\n%50.57 - %51.32 öngörü aralığında %50.95’lik bir oy öngörüsünde bulunabiliriz. Gerçek oy oranı %51.14’tür. Öngörü, gerçek oyun yaklaşık 0.19 puan kadar altında kalmıştır ancak fena olmadığını söyleyebiliriz. Bu öngörüyü açılan sandık oranı %49 iken yaptık.\nSonraki seçimlerde pratik bir şekilde kullanmak için buraya kadar yaptığımız modelleme işlemlerini bir fonksiyona dönüştürelim.\n\ndef modeling_and_forecasting(df, dependent_variable, independent_variable, ballot_box_rate=50, alphas=(0.01, 0.05, 0.1)):\n    main_df = df[df[independent_variable] &lt;= ballot_box_rate][[independent_variable, dependent_variable]]\n\n    y = main_df[dependent_variable]\n    X = main_df[independent_variable]\n    X = sm.add_constant(X)\n\n    model_linlin = sm.OLS(y, X).fit()\n\n    print(model_linlin.summary())\n\n    predictions = [model_linlin.get_prediction(X).summary_frame(alpha) for alpha in alphas]\n    predictions_mean = [pred['mean'] for pred in predictions]\n    predictions_lower = [pred['mean_ci_lower'] for pred in predictions]\n    predictions_upper = [pred['mean_ci_upper'] for pred in predictions]\n\n    residuals = model_linlin.resid\n\n    sw_statistic, sw_p_value = shapiro(residuals)\n    h_normal = ['Hata terimleri normal dağılıyor.' if sw_p_value &gt;= alpha else 'Hata terimleri normal dağılmıyor.' for alpha in alphas]\n\n    bg_test_statistic, bg_p_value, _, _ = dg.acorr_breusch_godfrey(model_linlin, nlags=5)\n    h_autocorrelation = ['Hata terimleri arasında otokorelasyon yoktur.' if bg_p_value &gt;= alpha else 'Hata terimleri arasında otokorelasyon vardır.' for alpha in alphas]\n\n    bp_test_statistic, bp_p_value, _, _ = dg.het_breuschpagan(residuals, model_linlin.model.exog)\n    h_heteroscedasticity = ['Modelde değişen varyans vardır.' if bp_p_value &lt;= alpha else 'Modelde değişen varyans yoktur.' for alpha in alphas]\n\n    assumptions_table = {\n        'Alfa': alphas,\n        'Hata Terimleri Normalliği': h_normal,\n        'Otokorelasyon': h_autocorrelation,\n        'Değişen varyans': h_heteroscedasticity\n    }\n\n    assumptions_table_df = pd.DataFrame(assumptions_table)\n\n    X_value = [1, 100]\n\n    forecasts = [model_linlin.get_prediction(X_value).summary_frame(alpha) for alpha in alphas]\n    forecasted_vote_mean = [forecast['mean'].item() for forecast in forecasts]\n    forecasted_vote_lower = [forecast['obs_ci_lower'].item() for forecast in forecasts]\n    forecasted_vote_upper = [forecast['obs_ci_upper'].item() for forecast in forecasts]\n\n    forecast_table = {\n        'Alfa': alphas,\n        'Oy Öngörüsü - Ortalama': forecasted_vote_mean,\n        'Oy Öngörüsü - Alt Sınır': forecasted_vote_lower,\n        'Oy Öngörüsü - Üst Sınır': forecasted_vote_upper\n    }\n\n    forecast_table_df = pd.DataFrame(forecast_table)\n\n    return assumptions_table_df, forecast_table_df\n\nassumptions_df, forecast_df = modeling_and_forecasting(\n    df,\n    dependent_variable='Ekrem İmamoğlu',\n    independent_variable='Açılan Sandık Oranı'\n)\n\n\n\nAçılan sandık oranı %60 olduğunda modeli kontrol etmeye başlayabiliriz. Bundan sonrasında %90’a kadar her yeni veri akışında modeli takip edeceğiz. Peki, nasıl?\nModelimizin bağımsız değişken tarafına kukla değişken ekleyeceğiz. Böylece modelde herhangi bir kırılım olup olmadığını inceleyeceğiz. Eğer kırılım varsa modeli kukla değişkenli kuracağız. Bunun yanında, RMSE (Root Mean Squared Error, Kök Ortalama Kare Hatası) değerlerini de hesaplayacağız.\n\\(\\mathrm{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\\)\n\n# Aşağıda pas geçilen varsayımlar bir önceki fonksiyonda olduğu gibi eklenebilir.\n\ndef update_modeling_and_forecasting(df, ballot_box_rate, alpha=0.1):\n\n    updated_df = df[df['Açılan Sandık Oranı'] &lt;= ballot_box_rate][['Açılan Sandık Oranı', 'Ekrem İmamoğlu']]\n\n    updated_df['Dummy'] = [0 if i &lt; 20 else 1 for i in range(len(updated_df))]\n\n    y = updated_df['Ekrem İmamoğlu']\n    X = updated_df[['Açılan Sandık Oranı','Dummy']]\n    X = sm.add_constant(X)\n\n    model_linlin = sm.OLS(y, X).fit()\n\n    dummy_p_value = model_linlin.pvalues['Dummy']\n    if dummy_p_value &lt;= alpha:\n        X_value = [1, 100, 1]\n        dummy_significance = 'Modele Dahil Edildi'\n    else:\n        X = X.drop(columns=['Dummy'])\n        model_linlin = sm.OLS(y, X).fit()\n        X_value = [1, 100]\n        dummy_significance = 'Modele Dahil Edilmedi'\n\n    forecasts = model_linlin.get_prediction(X_value).summary_frame(alpha)\n    forecasted_vote = forecasts['mean'].iloc[0]\n    forecasted_vote_lower = forecasts['obs_ci_lower'].iloc[0]\n    forecasted_vote_upper = forecasts['obs_ci_upper'].iloc[0]\n    rmse = np.sqrt(np.mean((y - model_linlin.predict(X))**2))\n\n    return forecasted_vote, forecasted_vote_lower, forecasted_vote_upper, dummy_significance, rmse\n\nmin_ballot_box_rate=60\nmax_ballot_box_rate=90\nballot_box_rate = df[(df['Açılan Sandık Oranı'] &gt;= min_ballot_box_rate) & (df['Açılan Sandık Oranı'] &lt;= max_ballot_box_rate)]['Açılan Sandık Oranı'].tolist()\n\nforecast_dfs = []\n\nfor turnout in ballot_box_rate:\n\n    forecasted_vote, forecasted_vote_lower, forecasted_vote_upper,  dummy_significance, rmse = update_modeling_and_forecasting(df, turnout)\n\n    forecast_df = pd.DataFrame({\n        'Açılan Sandık Oranı': [turnout],\n        'Oy Öngörüsü - Ortalama': [forecasted_vote],\n        'Oy Öngörüsü - Alt Sınır': [forecasted_vote_lower],\n        'Oy Öngörüsü - Üst Sınır': [forecasted_vote_upper],\n        'Kukla Değişken': [dummy_significance],\n        'RMSE': [rmse]\n    })\n\n    forecast_dfs.append(forecast_df)\n\nforecast_df = pd.concat(forecast_dfs, ignore_index=True)\n\nprint(forecast_df)\n\nAçılan sandık oranı %83.1 olduğunda istatistiksel olarak anlamlı bir kırılım yaşandığını görüyoruz. Bu noktada öngörülerimizi kukla değişkenli yapıyoruz. Açılan sandık oranı %60.4 iken 0.098 olan RMSE değerini açılan sandık oranı %83.1’e geldiğinde 0.076’ya düşürüyoruz. Bu model ile daha önce yapılan öngörüyü %50.84’e revize edebiliriz. Gerçek oy oranı %51.14’tür. Öngörü, gerçek oyun yaklaşık 0.3 puan kadar altında kalmıştır.\n\nGelecek içeriklerde görüşmek dileğiyle."
  },
  {
    "objectID": "posts/post_5/index.html",
    "href": "posts/post_5/index.html",
    "title": "Partilere Göre Milletvekillerinin Ortalama Yüzü: Türkiye Örneği",
    "section": "",
    "text": "Giriş\n1800’lü yıllarda Francis Galton, belirli bir grup insanda ortak olan yüz özelliklerini görselleştirmek amacıyla birçok farklı bireyin yüz fotoğraflarını tek bir fotoğraf filmi üzerine yansıtmış ve bu yüzlerin kompozit görüntülerini oluşturmuştur.\nBugün benzer bir yaklaşım, belirli bir grup insanın -örneğin, milletvekilleri gibi- ortak yüz özelliklerini incelemek amacıyla modern tekniklerle gerçekleştirilebilir.\nBu uygulamada, 28. dönem AKP, CHP, DEM ve MHP milletvekillerinin fotoğraflarını kullanarak parti ve cinsiyet kategorisinde ortalama bir yüz yaratacağız. Uygulamayı GitHub’ta bulunan şu repo yardımı ile yapacağız. Uygulamanın veri seti olan fotoğraflara ise burada bulunan tbmm_28 isimli klasör ile ulaşabilirsiniz.\n\n\nReponun Klonlanması\nÖncelikle repoyu proje klasörümüze klonlayalım.\n\ngit clone https://github.com/johnwmillr/facer.git Facer\n\nKlonlama işleminden sonra bulunduğumuz dizinde sadece Facer klasöründeki facer klasörünü bırakabiliriz. facer klasörünün içinde ise facer.py ve utils.py dosyaları kalabilir.\nYukarıdaki işlemden sonra aşağıdaki gibi zip’li dosyayı indirip unzip’liyor ve dosyayı bulunduğumuz dizinde açtığımız model isimli klasöre taşıyoruz.\n\ncurl -O http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\nbunzip2 shape_predictor_68_face_landmarks.dat.bz2\n\nmkdir model\nmv shape_predictor_68_face_landmarks.dat model\n\nTBMM’nin web sitesinden alınan görseller .jpe formatında olduğu için facer klasöründe bulunan facer.py dosyasındaki glob_image_files() fonksiyonuna .jpe uzantısını ekliyoruz.\nOrtalama bir yüze ulaşmak için uygulanan adımlar burada detaylı bir şekilde veriliyor.\n\n\nKullanılacak Kütüphaneler\n\nfrom facer.facer import load_images, detect_face_landmarks, create_average_face\nimport matplotlib.pyplot as plt\nimport os\n\n\n\nOrtalama Yüzün Hesaplanması\n\nfolders = [\n    './tbmm_28/akp/kadin',\n    './tbmm_28/chp/kadin',\n    './tbmm_28/dem/kadin',\n    './tbmm_28/mhp/kadin',\n    './tbmm_28/akp/erkek',\n    './tbmm_28/chp/erkek',\n    './tbmm_28/dem/erkek',\n    './tbmm_28/mhp/erkek'\n]\n\nfor folder in folders:\n    images = load_images(folder)\n\n    landmarks, faces = detect_face_landmarks(images)\n    average_face = create_average_face(faces, landmarks, save_image=False)\n\n    gender = 'kadin' if 'kadin' in folder else 'erkek'\n    party = folder.split('/')[2]\n    file_name = f'{party}_{gender}.jpg'\n\n    plt.imshow(average_face)\n    plt.axis('off')\n    plt.savefig(os.path.join('imgs', file_name))\n    plt.show()\n\n\nGelecek içeriklerde görüşmek dileğiyle."
  },
  {
    "objectID": "posts/post_7/index.html",
    "href": "posts/post_7/index.html",
    "title": "Reddit Başlıklarının Duygu Analizi: r/worldnews Örneği",
    "section": "",
    "text": "Giriş\nReddit platformundan verileri çekebilmek için öncelikle buradan bir uygulama oluşturarak OAuth2 anahtarlarını almamız gerekiyor ki API’a ulaşabilelim.\nAdım 1. are you a developer? create an app... butonuna tıklayalım.\nAdım 2. name alanına kullanıcı ismimizi yazalım, script’i seçelim ve redirect uri alanına http://localhost:8080 bilgisini girelim.\nAdım 3. create app butonuna tıklayalım.\nBize verilen personal use script ve secret bilgilerini kullanacağız.\n\n\n\nKullanılacak Kütüphaneler\n\nimport pandas as pd\nimport datetime as dt\nimport praw\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nimport matplotlib.pyplot as plt\n\n\n\nSubreddit Başlıklarının Çekilmesi\npraw kütüphanesini kullanarak Reddit API’ına erişim sağlıyoruz.\n\nreddit = praw.Reddit(\n    client_id='personel_use_script',\n    client_secret='secret',\n    user_agent='username'\n)\n\ntopics_dict isimli bir sözlük oluşturalım. Bu sözlük çekmek istediğimiz bilgileri içerecek.\n\ntopics_dict = {\n    'id':[],\n    'title':[],\n    'score':[],\n    'comms_num':[], # kullanmayacağız ama kalsın\n    'created':[]\n}\n\nVerileri r/worldnews isimli subreddit’ten çekeceğiz ve sözlüğü veri çerçevesine dönüştüreceğiz.\n\nfor submission in reddit.subreddit('worldnews').new(limit=None):\n    topics_dict['id'].append(submission.id)\n    topics_dict['title'].append(submission.title)\n    topics_dict['score'].append(submission.score)\n    topics_dict['comms_num'].append(submission.num_comments)\n    topics_dict['created'].append(submission.created)\n\ntopics_df = pd.DataFrame(topics_dict)\n\n\nTarih ve saat bilgisi timestamp formatındaki created kolonunda yer almaktadır. Bunu datetime formatına dönüştürmemiz gerekiyor.\n\ndef timestamp_to_datetime(created):\n    return dt.datetime.fromtimestamp(created)\n\ntopics_df['datetime'] = topics_df['created'].apply(timestamp_to_datetime)\n\n\n\n\nDuygu Skorlarının Elde Edilmesi ve Verilerin Görselleştirilmesi\n\nsia = SIA()\nresults = []\n\nfor datetime, line, score in zip(topics_df['datetime'], topics_df['title'], topics_df['score']):\n    pol_score = sia.polarity_scores(line)\n    pol_score['datetime'] = datetime\n    pol_score['headline'] = line\n    pol_score['score'] = score\n    results.append(pol_score)\n\nresults_df = pd.DataFrame.from_records(results)\n\n\nYukarıda, öncelikle SIA (SentimentIntensityAnalyzer) isimli duygu analizi aracını kullanabilmek için bir nesne oluşturduk. Sonrasında her bir başlığın duygu analizini yapmak için SIA’in polarity_scores metodunu kullandık. Bu metot, bir metnin duygusal içeriğini analiz eder ve dört farklı duygu ölçüsü verir: pos (olumlu), neg (olumsuz), neu (nötr) ve compound (bileşik, tüm duyguların birleşimi). Biz compound ile ilgileneceğiz. Tüm bilgileri daha önce boş bir liste olarak oluşturduğumuz results değişkenine gönderdik ve döngü bittikten sonra results listesini results_df isimli veri çerçevesine dönüştürdük.\nGörselleştirmeyi iki farklı şekilde yapabiliriz.\nBirincisi, Reddit skorları (beğeni) ile duygu skorlarını gösterebiliriz. Şu an gündemde olduğu için başlıklarda geçen Iran veya Israel için farklı bir renk tercih edebiliriz.\n\nplt.figure(figsize=(10,6))\nplt.scatter(results_df['compound'], results_df['score'], alpha=.1, color='gray', label='Other Headlines')\nfor index, row in results_df.iterrows():\n    if 'Iran' in row['headline'] or 'Israel' in row['headline']:\n        plt.scatter(row['compound'], row['score'], color='red')\nplt.xlabel('Compound Sentiment Score')\nplt.ylabel('Reddit Score')\nplt.title('r/worldnews: Sentiment Score vs. Reddit Score')\nplt.grid(True)\nplt.yscale('log')\nplt.legend(['Other Headlines', 'Headlines containing Iran or Israel'])\nplt.show()\n\n\nİkincisi, duygu skorlarının ortalamada nasıl değiştiğini bir zaman serisi olarak gösterebiliriz.\n\nresults_df['date'] = results_df['datetime'].dt.date\ndaily_avg_compound = results_df.groupby('date')['compound'].mean()\n\nplt.figure(figsize=(10,6))\nplt.plot(daily_avg_compound.index, daily_avg_compound, marker='o', markersize=8, color='r')\nplt.ylabel('Daily Average Compound Score')\nplt.title('r/worldnews: Daily Average Compound Score over Time')\nplt.grid(True)\nplt.show()\n\n\nGelecek içeriklerde görüşmek dileğiyle."
  },
  {
    "objectID": "posts/post_9/index.html",
    "href": "posts/post_9/index.html",
    "title": "TCMB/EVDS Verilerine Erişim (05/04/2024 Değişikliği Sonrası)",
    "section": "",
    "text": "Türkiye Cumhuriyet Merkez Bankası, yenievds@tcmb.gov.tr adresinden bir duyuru göndermişti. Mail almayanlar için bir kısmını aşağıda paylaşıyorum.\nÖnceki duyurumuzda EVDS nin web servis parametrelerinde güvenlik sebebiyle bir düzenleme yapılacağını belirtmiştik. URL adresinden giden “key” parametresinin artık http request header içinde gelmesi ve web servis kullanan uygulamalarınızda “key=xxxxxx” parametresini http request header olarak dönüştürmeniz gerektiği belirtilmişti. Bu düzenleme 05 Nisan 2024 Cuma saat 21:00 TSİ tarihinden itibaren geçerli olacaktır.\nURL adresinden gönderdiğimiz key parametresini artık headers içinden göndereceğiz."
  },
  {
    "objectID": "posts/post_9/index.html#tek-serili-örnek",
    "href": "posts/post_9/index.html#tek-serili-örnek",
    "title": "TCMB/EVDS Verilerine Erişim (05/04/2024 Değişikliği Sonrası)",
    "section": "Tek Serili Örnek",
    "text": "Tek Serili Örnek\n\napi_key = 'api_key'\n\nseries_code='TP.APIFON4' # TCMB Ağırlıklı Ortalama Fonlama Maliyeti\nstart_date='01-01-2011' # Başlangıç\nend_date='30-04-2024' # Bitiş\nfrequency='5' # Aylık\naggregationType='avg' # Ortalama\n\nparams = {\n    'series': series_code,\n    'startDate': start_date,\n    'endDate': end_date,\n    'frequency': frequency,\n    'aggregationTypes': aggregationType,\n    'type': 'json'\n}\n\nurl = f'https://evds2.tcmb.gov.tr/service/evds/{urlencode(params)}'\n\nresponse = requests.get(url=url, headers={'key': api_key})\n\n# print(response.request.headers)\n\nformatted_response = json.loads(response.content)\n\ndata = formatted_response['items']\ndf = pd.DataFrame(data)\n\ndf['Tarih'] = pd.to_datetime(df['Tarih']) + pd.offsets.Day(0)\ndf = df.drop(columns=['UNIXTIME'])\ndf.columns = ['Tarih', 'AOFM']\ndf['AOFM'] = pd.to_numeric(df['AOFM'])"
  },
  {
    "objectID": "posts/post_9/index.html#çok-serili-örnek",
    "href": "posts/post_9/index.html#çok-serili-örnek",
    "title": "TCMB/EVDS Verilerine Erişim (05/04/2024 Değişikliği Sonrası)",
    "section": "Çok Serili Örnek",
    "text": "Çok Serili Örnek\nEğer birden fazla seri ile çalışmak istiyorsak, seriler arasında - olacak şekilde giriş yapacağız.\n\nseries_code='TP.BISTTLREF.DUSUK-TP.BISTTLREF.YUKSEK-TP.BISTTLREF.KAPANIS' # Türk Lirası Gecelik Referans Faiz Oranı\nstart_date='14-06-2019' # Başlangıç\nend_date='30-04-2024' # Bitiş\nfrequency='5' # Aylık\n\nparams = {\n    'series': series_code,\n    'startDate': start_date,\n    'endDate': end_date,\n    'frequency': frequency,\n    'type': 'json'\n}\n\nurl = f'https://evds2.tcmb.gov.tr/service/evds/{urlencode(params)}'\n\nresponse = requests.get(url=url, headers={'key': api_key})\n\n# print(response.request.headers)\n\nformatted_response = json.loads(response.content)\n\ndata = formatted_response['items']\ndf = pd.DataFrame(data)\n\ndf['Tarih'] = pd.to_datetime(df['Tarih']) + pd.offsets.Day(0)\ndf = df.drop(columns=['UNIXTIME'])\nnumeric_columns = df.columns[df.columns != 'Tarih']\ndf[numeric_columns] = df[numeric_columns].apply(pd.to_numeric)\n\nGelecek içeriklerde görüşmek dileğiyle."
  }
]