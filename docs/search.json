[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Python Blog",
    "section": "",
    "text": "Sıralama Öğesi\n       Varsayılan\n         \n          Tarih - En eski\n        \n         \n          Tarih - En yeni\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nİndikatör Ağırlıklarının Optimize Edilerek Teknik Reytinglerin Oluşturulması ve Al-Tut Stratejisinin Alt Edilmesi\n\n\n\n\n\n\n\n\nMayıs 25, 2024\n\n\n44 dakika\n\n\n\n\n\n\n\n\n\n\n\n\nTCMB’nin Faiz Oranlarına İlişkin Duyuru Metinlerinin Okunabilirliği\n\n\n\n\n\n\n\n\nMayıs 14, 2024\n\n\n4 dakika\n\n\n\n\n\n\n\n\n\n\n\n\nTCMB’nin Faiz Oranlarına İlişkin Duyuru Metinlerinin İçerik Analizi\n\n\n\n\n\n\n\n\nMayıs 4, 2024\n\n\n7 dakika\n\n\n\n\n\n\n\n\n\n\n\n\nTCMB Başkan Yardımcısı Cevdet Akçay’ın ‘Link Kopmuş’ Dediği Konuşmasının Duygu Analizi\n\n\n\n\n\n\n\n\nNisan 25, 2024\n\n\n2 dakika\n\n\n\n\n\n\n\n\n\n\n\n\nRutin Görevlerin Otomatize Edilmesi: Outlook ve Görev Zamanlayıcının Kullanılması\n\n\n\n\n\n\n\n\nNisan 23, 2024\n\n\n2 dakika\n\n\n\n\n\n\n\n\n\n\n\n\nTCMB/EVDS Verilerine Erişim (05/04/2024 Değişikliği Sonrası)\n\n\n\n\n\n\n\n\nNisan 21, 2024\n\n\n2 dakika\n\n\n\n\n\n\n\n\n\n\n\n\nTürkiye ve Çevresinde Gerçekleşen Depremlerin Animasyonlu Harita ile Gösterilmesi\n\n\n\n\n\n\n\n\nNisan 20, 2024\n\n\n4 dakika\n\n\n\n\n\n\n\n\n\n\n\n\nReddit Başlıklarının Duygu Analizi: r/worldnews Örneği\n\n\n\n\n\n\n\n\nNisan 14, 2024\n\n\n3 dakika\n\n\n\n\n\n\n\n\n\n\n\n\nİstanbul İlçe Belediye Başkanlarının X (Twitter) Profillerindeki Duygu Dağılımı\n\n\n\n\n\n\n\n\nNisan 13, 2024\n\n\n3 dakika\n\n\n\n\n\n\n\n\n\n\n\n\nPartilere Göre Milletvekillerinin Ortalama Yüzü: Türkiye Örneği\n\n\n\n\n\n\n\n\nNisan 10, 2024\n\n\n2 dakika\n\n\n\n\n\n\n\n\n\n\n\n\nSeçim Verilerinin Türkiye Haritasında Görselleştirilmesi\n\n\n\n\n\n\n\n\nNisan 8, 2024\n\n\n3 dakika\n\n\n\n\n\n\n\n\n\n\n\n\nAnlık Veri Akışı ile Oy Öngörüsü\n\n\n\n\n\n\n\n\nNisan 7, 2024\n\n\n9 dakika\n\n\n\n\n\n\n\n\n\n\n\n\nKorelasyon Tahmin Oyunu\n\n\n\n\n\n\n\n\nMart 24, 2024\n\n\n4 dakika\n\n\n\n\n\n\n\n\n\n\n\n\nPython Dünyama Hoş Geldin!\n\n\n\n\n\n\n\n\nMart 23, 2024\n\n\n1 dakika\n\n\n\n\n\n\nEşleşen öğe yok"
  },
  {
    "objectID": "posts/post_8/index.html",
    "href": "posts/post_8/index.html",
    "title": "Türkiye ve Çevresinde Gerçekleşen Depremlerin Animasyonlu Harita ile Gösterilmesi",
    "section": "",
    "text": "Giriş\nAnimasyonlu haritalar, zaman içinde değişen verileri görselleştiren etkili bir yöntemdir. Örneğin, Türkiye ve çevresinde gerçekleşen depremleri animasyonlu bir harita üzerinde göstermek, deprem aktivitesinin zamanla nasıl değiştiğini ve hangi bölgelerin daha fazla risk altında olduğunu görsel olarak görmemize yardımcı olabilir.\nAnimasyonlu haritayı oluşturmak için kullanacağımız verilere USGS (United States Geological Survey) API ile ulaşacağız.\n\n\nKullanılacak Kütüphaneler\n\nimport requests\nimport pandas as pd\nimport os\nimport time\nimport folium\nfrom folium.plugins import HeatMap\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\nfrom PIL import Image, ImageDraw, ImageFont\nfrom pathlib import Path\nimport imageio\nimport shutil\n\n\n\nAPI ile Verilerin Çekilmesi ve Kullanılabilir Formata Dönüştürülmesi\n\nstart = '2014-01-01'\nmin_mag = 3\nlat = 39.1458\nlon = 34.1614\nmax_rad_km = 1000\n\nurl = f'https://earthquake.usgs.gov/fdsnws/event/1/query?format=geojson&starttime={start}&minmagnitude={min_mag}&latitude={lat}&longitude={lon}&maxradiuskm={max_rad_km}'\nresponse = requests.get(url)\ndata = response.json()\n\ndf = pd.DataFrame({\n    'Place': [feature['properties']['place'] for feature in data['features']],\n    'Magnitude': [feature['properties']['mag'] for feature in data['features']],\n    'Time': [pd.to_datetime(feature['properties']['time'], unit='ms').strftime('%Y-%m-%d') for feature in data['features']],\n    'Latitude': [feature['geometry']['coordinates'][1] for feature in data['features']],\n    'Longitude': [feature['geometry']['coordinates'][0] for feature in data['features']]\n})\n\nYukarıdaki kodda, verilen parametrelerle USGS API’ını kullanarak deprem verilerini çekiyoruz. Başlangıç tarihini (start) '2014-01-01' ve minimum deprem büyüklüğünü (min_mag) 3 olarak belirledik. Ayrıca, enlem (lat) 39.1458 ve boylam (lon) 34.1614 olacak şekilde Türkiye’nin koordinatlarına yakın ve maksimum 1000 kilometrelik yarıçapa (max_rad_km) sahip bir bölgeyi sorguluyoruz. Son olarak, API’dan gelen verileri işleyerek bir DataFrame oluşturuyoruz. Bu DataFrame, depremlerin yerini (Place), büyüklüğünü (Magnitude), zamanını (Time) ve enlem-boylam koordinatlarını (Latitude-Longitude) içeriyor.\nAylık bir seri ile çalışacağımız için tarihlerdeki (Time sütunu) günleri 01 ile değiştiriyoruz.\n\ndf['Time'] = df['Time'].str.replace(r'-\\d{2}$', '-01', regex=True)\n\n\n\nHaritaların Yapılması ve HTML-PNG Formatlarında Kaydedilmesi\n\nchrome_options = Options()\nchrome_options.add_argument('--start-fullscreen')\n\nunique_dates = df['Time'].unique()\nunique_dates.sort()\n\nturkey_latlon = [39, 35]\n\ndelay = 5\n\nfont = ImageFont.load_default()\nfont_size = 36\n\nfor unique_date in unique_dates:\n\n    filtered_df = df[df['Time'] == unique_date]\n    filtered_df = filtered_df[['Latitude', 'Longitude', 'Magnitude']]\n    turkey_map = folium.Map(location=turkey_latlon, zoom_start=6, tiles='cartodbdark_matter')\n    HeatMap(data=filtered_df, radius=15).add_to(turkey_map)\n\n    html_filename = f'turkey_heatmap_{unique_date}.html'\n    turkey_map.save(html_filename)\n\n    browser = webdriver.Chrome(options=chrome_options)\n    browser.get(os.path.abspath(html_filename))\n    time.sleep(delay)\n\n    screenshot_filename = f'turkey_heatmap_{unique_date}.png'\n    browser.save_screenshot(screenshot_filename)\n\n    browser.quit()\n\n    img = Image.open(screenshot_filename)\n    draw = ImageDraw.Draw(img)\n    font = ImageFont.truetype('arial.ttf', font_size)\n    draw.text((10, img.height - 50), pd.to_datetime(unique_date).strftime('%B %Y'), font=font, fill=(255, 255, 255))\n    img.save(screenshot_filename)\n\n    os.remove(html_filename)\n    print(f'{html_filename} loaded in the browser and screenshot {screenshot_filename} captured.')\n\nYukarıdaki kodda, her bir tarih döngüde kullanılacağı için tekil tarihleri unique_dates değişkenine gönderip bu tarihleri eskiden yeniye doğru olacak şekilde sıralıyoruz. Türkiye’nin koordinatlarını tanımladığımız ve haritaların merkezi olacak değerler turkey_latlon değişkeninde bulunuyor. Her işlemde 5 saniyelik bir bekleme süresi olacak. Bunu delay değişkeninde tutuyoruz. Resimlerin üzerinde görünecek yazılara ait varsayılan font ve font büyüklüğü değerleri olan arial.ttf ve 36 sırasıyla font ve font_size değişkenlerinde bulunuyor. Her döngüde açılacak tarayıcıların ekranı kapsayacak şekilde olmasını istediğimiz için '--start-fullscreen' olacak şekilde ayarlama da yaptık.\nDöngüde 7 kod grubu bulunmaktadır. İlk grup, tarih filtresi yapıp haritayı oluşturuyor. İkinci grup, haritayı içinde bulunduğu dizine HTML formatında kaydediyor. Üçüncü grup, HTML formatında kaydedilen dosyayı tarayıcıda açıyor ve açtıktan sonra belirlenen süre kadar bekletiyor. Dördüncü grup, ekran görüntüsü alıyor ve içinde bulunduğu dizine PNG formatında kaydediyor. Beşinci grup, açılan tarayıcıyı kapatıyor. Altıncı grup, resim üzerinde yazı işlemlerini yapıyor. Yedinci ve son grup, kaydedilen HTML dosyalarını siliyor ve ekrana bilgi veriyor.\n\n\nAnimasyonlu Harita Yapımı ve GIF Formatında Kaydedilmesi\nÇalışmanın odak noktasını bu başlık altında göreceğiz.\n\nimage_path = Path()\nimages = list(image_path.glob('*.png'))\nimage_list = [imageio.v3.imread(file_name) for file_name in images]\nimageio.mimwrite('Turkey_Earthquake.gif', image_list, fps=2)\n_ = [file.unlink() for file in images]\n\nshutil.move('Turkey_Earthquake.gif', 'imgs/Turkey_Earthquake.gif')\n\nYukarıdaki kodda ilk olarak, Path() fonksiyonunu çağırarak bir dosya yolu nesnesi oluşturuyor ve bunu image_path değişkenine atıyoruz. Ardından, bu dosya yolu nesnesi üzerinde .glob() yöntemini kullanarak tüm PNG dosyalarını alıyor ve images listesine atıyoruz. imageio modülünü kullanarak her bir PNG dosyasını imageio.v3.imread() fonksiyonuyla okuyor ve bu okunan görüntüleri image_list listesine ekliyoruz. imageio.mimwrite() fonksiyonuyla image_list içindeki görüntüleri kullanarak bir GIF dosyası oluşturuyoruz. Oluşturduğumuz GIF dosyasının ismini 'Turkey_Earthquake.gif' olarak belirliyor ve saniyede 2 kare (fps=2) hızında olacak şekilde ayarlıyoruz. Daha sonra, artık gereksiz hale gelmiş olan PNG dosyalarını tek tek sildiriyoruz. Bu işlem için bir liste dönülüyor ve her bir dosya unlink() yöntemi kullanılarak siliniyor. Son olarak, shutil.move() fonksiyonunu kullanarak oluşturduğumuz GIF dosyasını 'imgs/' dizini altına taşıyoruz.\n\nGelecek içeriklerde görüşmek dileğiyle."
  },
  {
    "objectID": "posts/post_6/index.html",
    "href": "posts/post_6/index.html",
    "title": "İstanbul İlçe Belediye Başkanlarının X (Twitter) Profillerindeki Duygu Dağılımı",
    "section": "",
    "text": "Giriş\nYapılan çalışmaların yanında içinde profil fotoğrafının da olduğu sosyal medya kullanımı da önemli olabiliyor.\nBu uygulamada, İstanbul ilçe belediye başkanlarının X (Twitter) profillerindeki duygularını inceleyeceğiz. Duyguların tespitinde Py-Feat kütüphanesinden faydalanacağız.\n\n\nKullanılacak Kütüphaneler\n\nimport os\nimport pandas as pd\nfrom feat import Detector\nfrom feat.plotting import imshow\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n\nFotoğrafların İçe Aktarılması\nBuradan ulaşabileceğiniz fotoğraflar ilçe belediye başkanlarının X (Twitter) hesaplarından indirilmiştir.\n\nimage_folder = 'ilce_bb_profil/'\nimage_files = [f for f in os.listdir(image_folder) if f.endswith('.jpg')]\n\n\n\nDuyguların Tespit Edilmesi ve Görselleştirilmesi\nPy-Feat, görüntülerden ve videolardan yüz ifadelerini kolayca tespit etmek, yüz ifadesi verilerini önceden işlemek ve analiz etmek ve yüz ifadesi verilerini görselleştirmek için kapsamlı bir araç ve model seti sağlar. Py-Feat önceden eğitilmiş çeşitli modelleri içeriyor.\n\ndetector = Detector(\n    face_model='retinaface', # face detection\n    landmark_model='mobilefacenet', # facial landmark detection\n    facepose_model='img2pose', # facial pose estimation\n    au_model='xgb', # action unit detection\n    emotion_model='resmasknet', # emotion detection\n)\n\n\nYüz tespiti (Face detection, face_model): Bir görüntüdeki veya bir videodaki insan yüzlerini algılama sürecidir. Yüz tespiti, bir görüntü içindeki yüz bölgelerini belirlemek için kullanılır. Parametre seçenekleri şunlardır: retinaface, mtcnn, faceboxes, img2pose ve img2pose-c.\nYüz belirleme (Facial landmark detection, landmark_model): Yüzün belirli noktalarını (örneğin, gözler, burun, ağız) tespit etme sürecidir. Bu noktalar genellikle gözbebekleri, burun ucundaki nokta, dudakların kenarları gibi önemli anatomik yerlerdir. Parametre seçenekleri şunlardır: mobilefacenet, mobilenet ve pfld.\nYüz poz tahmini (Facial Pose estimation, facepose_model): Bir yüzün konumunu ve/veya dönüşünü belirleme sürecidir. Yüzün kaç derece eğik olduğunu, hangi yönde baktığını tahmin etmeyi içerir. Parametre seçenekleri şunlardır: img2pose ve img2pose-c.\nHareket Birimi tespiti (Action Unit detection, au_model): Yüz ifadelerindeki belirli kas gruplarını temsil eden hareket birimlerini (action units) tanımlama sürecidir. Örneğin, kaşların kaldırılması, dudakların büzülmesi gibi. Parametre seçenekleri şunlardır: xgb ve svm.\nDuygu tespiti (Emotion detection, emotion_model): Bir kişinin yüz ifadesinden duygusal durumunu belirleme sürecidir. Örneğin, mutlu, üzgün, kızgın gibi duyguları tanımlama. Parametre seçenekleri şunlardır: resmasknet ve svm.\n\nBir döngü ile tüm fotoğrafları dahil edeceğiz ama öncesinde örnek bir fotoğraf ile sürece bakalım.\n\nexample_img = 'uskudar_chp.jpg'\nimshow(image_folder + example_img)\n\n\ndetect_image() ile yüklenen modelleri kullanalım.\n\nsingle_face_prediction = detector.detect_image(image_folder + example_img)\n\n\nBuradan duyguları çekelim.\n\nemotions = single_face_prediction.emotions\n\n\nPy-Feat görselleştirme imkanı da sunuyor. Bunu iki farklı şekilde yapabiliriz.\n\nfigs = single_face_prediction.plot_detections(poses=True)\n\n\n\nfigs = single_face_prediction.plot_detections(faces='aus', muscles=True)\n\n\nTüm fotoğraflar için duyguları tespit edelim. Görseli ısı haritası ile yapacağız.\nÖncesinde image_files değişkenine tüm fotoğrafları aktarmıştık. Bunu kullanabiliriz.\n\nemotions_df = pd.DataFrame()\n\nfor image_file in image_files:\n    face_prediction = detector.detect_image(image_folder + image_file)\n    face_prediction_final = face_prediction[face_prediction['FaceScore'] == face_prediction['FaceScore'].max()]\n    emotions = face_prediction_final.emotions\n    ilce, parti = image_file.split('_')\n    parti = parti.split('.')[0]\n    emotions['image_file'] = ilce.upper() + '-' + parti.upper()\n    emotions_df = pd.concat([emotions_df, emotions], ignore_index=True)\n\nemotions_df = emotions_df.set_index('image_file')\nemotions_df.columns = emotions_df.columns.str.upper()\n\nDöngüde bulunan face_prediction_final değişkenini birden fazla yüz tespiti olduğu için oluşturdum. Büyükçekmece belediye başkanının profilinde kedi de bulunmaktadır.\n\nexample_img = 'buyukcekmece_chp.jpg'\nimshow(image_folder + example_img)\n\n\nIsı haritası ile duyguları gösterebiliriz.\n\nplt.figure(figsize=(10, 12))\nsns.heatmap(emotions_df, cmap='Reds', annot=False, linewidths=.5)\nplt.title('İstanbul İlçe Belediye Başkanları X (Twitter) Profilleri Duygu Dağılımı')\nplt.tick_params(left=False, bottom=False)\nplt.ylabel('')\nplt.show()\n\n\nProfil fotoğraflarında doğal olarak en güçlü duygunun happiness olduğunu görüyoruz. neutral pozlar da kendini göstermektedir.\nGelecek içeriklerde görüşmek dileğiyle."
  },
  {
    "objectID": "posts/post_4/index.html",
    "href": "posts/post_4/index.html",
    "title": "Seçim Verilerinin Türkiye Haritasında Görselleştirilmesi",
    "section": "",
    "text": "Giriş\n31 Mart 2024 yerel seçimlerini geride bıraktık ancak biz Türkiye’de yaşayanlar için iki seçim arası kısa bir reklam arası gibi. Önümüzdeki seçimlere hazırlıklı olmak için bu arayı değerlendirmeye devam ediyoruz.\nBu uygulamada, CHP’nin iller bazında aldığı oyları harita üzerinde göstereceğiz. local_elections_province_20240331 isimli JSON dosyasında bulunan verilere buradan ulaşabilirsiniz.\nHarita üzerinde görselleştirme yapmak için geopandas kütüphanesini kullanacağız. geopandas, isminden de anlaşılacağı üzere, popüler veri bilimi kütüphanesi pandas’ı jeo-uzamsal veriler ile destekleyip genişletiyor.\nHaritada görselleştirmek için öncelikle Türkiye’nin .shp uzantılı dosyasını bulmamız gerekiyor. .shp, coğrafi verileri depolamak için kullanılan bir vektör veri formatıdır. Buradan turkey_administrativelevels0_1_2.zip isimli dosyayı indirebilirsiniz. Eğer bir problem ile karşılaşırsanız burada bulunan dosyayı da indirebilirsiniz.\n\n\nKullanılacak Kütüphaneler\n\nimport json\nimport pandas as pd\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\n\n\nVerilerin Hazırlanması\nread_file() fonksiyonu ile .shp uzantılı dosyayı içeri aktaralım.\n\nshapefile_tr = gpd.read_file('turkey_administrativelevels0_1_2/tur_polbnda_adm1.shp')\n\nprint(shapefile_tr)\n\n\nshapefile_tr ile birleştireceğimiz seçim verilerinin yer aldığı JSON dosyasını içe aktaralım.\n\nwith open('local_elections_province_20240331.json', 'r', encoding='utf-8') as file:\n    data = json.load(file)\n\ndf = pd.DataFrame(data['Data'])\n\nprint(df)\n\n\nshapefile_tr tablosundaki adm1_tr sütunu ile df tablosundaki PROVINCE sütununu kullanarak iki tabloyu birleştireceğiz.\n\nmerged_df = pd.merge(\n  shapefile_tr,\n  df,\n  left_on='adm1_tr',\n  right_on='PROVINCE',\n  how='left'\n)\n\n\nBirleştirdikten sonra harita aşamasına geçebiliriz.\n\n\nHaritanın Oluşturulması\nHaritayı en temiz haliyle görelim.\n\nfig, ax = plt.subplots(figsize = (10,10))\nmerged_df.plot(ax=ax)\nplt.show()\n\n\nŞimdi verileri haritaya gönderelim ve görselin daha profesyonel görünmesini sağlayalım.\n\nfig, ax = plt.subplots(figsize = (10,10))\nmerged_df.plot(ax=ax, column='CHP', cmap='Reds')\nax.axis('off')\nax.set_title(\n    '31 Mart 2024 Yerel Seçimleri - Cumhuriyet Halk Partisi Oy Dağılımı',\n    fontdict = {'fontsize': 8}\n)\nax.text(\n    0.95,\n    0.01,\n    \"Veriler Yeni Şafak'ın web sitesinden alınmıştır.\",\n    color='gray',\n    fontsize=6,\n    fontstyle='italic',\n    ha='right',\n    va='bottom',\n    transform=ax.transAxes\n)\nplt.show()\n\n\nNeler yaptık? İnceleyelim.\nÖncelikle, plt.subplots() ile bir Figure ve Axes nesnesi oluşturuyoruz. Sonrasında, merged_df isimli DataFrame’den gelen verileri kullanarak harita oluşturuyoruz ve bu işlemi plot() ile gerçekleştiriyoruz. column parametresi haritada renk kodlaması yapılacak sütunu belirtirken, cmap parametresi renk haritasını belirler. Sadece haritanın görüntülenmesini sağlamak için ax.axis() ile eksenleri kapalı hale getiriyoruz. ax.set_title() ile haritanın başlığını ayarlıyoruz. Son olarak, ax.text() ile sağ alt köşeye bir metin ekliyoruz ve plt.show() ile grafiği görüntülüyoruz.\nYukarıda sürekli verileri kullandık. Peki, kategorik verileri harita üzerinde nasıl gösterebiliriz?\nCHP’nin %50’den az ve çok aldığı illeri görselleştirmek istediğimizi varsayalım.\n\nmerged_df['CHP_50'] = merged_df['CHP'].apply(\n  lambda x: '%50\\'den az' if x &lt; 50 else '%50\\'den çok'\n)\n\n\n\ncolors = {\n    \"%50'den az\": \"gray\",\n    \"%50'den çok\": \"black\"\n}\n\ncmap = ListedColormap(list(colors.values()))\n\nfig, ax = plt.subplots(figsize = (10,10))\nmerged_df.plot(\n  ax=ax,\n  column='CHP_50',\n  cmap=cmap,\n  legend=True,\n  legend_kwds={'loc': 'lower left', 'fontsize': 8}\n)\nax.axis('off')\nax.set_title(\n    \"31 Mart 2024 Yerel Seçimleri - Cumhuriyet Halk Partisi'nin %50'den Az ve Çok Aldığı İller\",\n    fontdict = {'fontsize': 8}\n)\nax.text(\n    0.95,\n    0.01,\n    \"Veriler Yeni Şafak'ın web sitesinden alınmıştır.\",\n    color='gray',\n    fontsize=6,\n    fontstyle='italic',\n    ha='right',\n    va='bottom',\n    transform=ax.transAxes\n)\nplt.show()\n\n\nFarklı neler yaptık? İnceleyelim.\ncolors isminde bir sözlük tanımladık. Bu sözlük, görselleştirmede kullanılacak renkleri ve bunlara karşılık gelen kategorileri içeriyor. Ayrıca, cmap isminde bir ListedColormap nesnesi oluşturduk. Bu, renk paletini belirtiyor ve colors sözlüğünden alınan renklerle oluşturuluyor. Önceki haritada olmayan, bu haritada sol alt köşede bulunan lejant ise kategorilerin tanımlarını içeriyor.\nGelecek içeriklerde görüşmek dileğiyle."
  },
  {
    "objectID": "posts/post_2/index.html",
    "href": "posts/post_2/index.html",
    "title": "Korelasyon Tahmin Oyunu",
    "section": "",
    "text": "Giriş\nKorelasyon tahmin oyunu yapımında nelere ihtiyacımız olabilir?\nBirincisi, iki adet rassal seri üretmeliyiz ve bu serileri üretirken korelasyon katsayısını dikkate almalıyız. Buradaki korelasyon katsayısı Pearson’ı ifade etmektedir. Rassal serileri üretmek için numpy kütüphanesinin np.random.multivariate_normal fonksiyonundan faydalanabiliriz.\nİkincisi, kullanıcı için bir skorlama yapmalıyız. Bunun için RMSE (Root Mean Squared Error, Kök Ortalama Kare Hatası) metriğini kullanabiliriz. RMSE skoru düştükçe başarı artacaktır.\nÜçüncüsü, kullanıcı ile etkileşimde olmalıyız. Etkileşim için Streamlit ile bir web uygulaması yapabiliriz. Uygulamayı lokalde çalıştıracağız.\n\n\nKullanılacak Kütüphaneler\n\nimport streamlit as st\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\nKorelasyonlu Rassal Serilerin Üretilmesi\n\ndef generate_correlated_data():\n    num_points = int(np.random.uniform(low=100, high=1000))\n    rho = round(np.random.uniform(low=-1, high=1), 2)\n    cov_matrix = np.array([[1, rho], [rho, 1]])\n    mu = [0, 0]\n    correlated_data = np.random.multivariate_normal(mean=mu, cov=cov_matrix, size=num_points)\n    return correlated_data, rho\n\nFonksiyonumuzun ismini generate_correlated_data olarak belirledik ve fonksiyonumuzun herhangi bir parametresi bulunmamaktadır.\nnum_points değişkeni, 100 ile 1000 arasında olmak üzere rassal olarak üretilecek serilere ait verilerin uzunluğunu temsil etmektedir. Bu değeri üretebilmek için numpy kütüphanesinin np.random.uniform fonksiyonunu kullandık ve int() ile değerin tam sayı veri tipinde olmasını sağladık.\nrho değişkeni, -1 ile 1 arasında rassal olarak olarak üretilmiş bir korelasyon katsayısını temsil etmektedir. Bu değeri üretebilmek için np.random.uniform fonksiyonunu kullandık. rho değerini noktadan sonra iki rakam gelecek şekilde ayarladık.\ncov_matrix değişkeni, 2x2’lik bir kovaryans matrisini temsil etmektedir.\nmu değişkeni, veri kümesinin her bir boyutu için belirlenen ortalama değeri temsil etmektedir. Bu değer 0 olacağı için X ve Y [0, 0]’dır.\ncorrelated_data değişkeni, np.random.multivariate_normal fonksiyonu yardımıyla üretilen verileri temsil etmektedir. Bu fonksiyon, çok değişkenli bir normal dağılımdan rassal örnekler üretir. Fonksiyonun içerisine parametre olarak ortalama, kovaryans matrisi ve örnek büyüklüğü girilir.\ngenerate_correlated_data fonksiyonu bize correlated_data ve rho değerlerini dönüyor. correlated_data değişkenindeki ilk seriye correlated_data[:,0]; ikinci seriye correlated_data[:,1] ile ulaşılabilir.\nFonksiyonun döndüğü değerleri kullanarak bir görselleştirme yapalım.\n\ncorrelated_data, rho = generate_correlated_data()\n\n# print(f'Randomly selected correlation: {rho}')\n\nplt.figure(figsize=(8, 6))\nplt.scatter(correlated_data[:,0], correlated_data[:,1], alpha=0.7)\nplt.title('Scatter Plot of Correlated Data')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.grid(True)\nplt.show()\n\n\nBir diğer fonksiyonumuz olan ve RMSE değerini hesaplayan calculate_rmse fonksiyonuna bakalım.\n\\(\\mathrm{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\\)\n\ndef calculate_rmse(predictions, actuals):\n    mse = np.mean((predictions - actuals) ** 2)\n    rmse = round(np.sqrt(mse), 4)\n    return rmse\n\ncalculate_rmse fonksiyonu, predictions ve actuals olmak üzere 2 adet parametre alıyor. Önce mse değişkenine tahminler ile gerçek değerler arasındaki farkların karelerinin ortalamasını gönderiyoruz. Ardından da bu ortalamanın karekökünü rmse değişkenine atıyoruz ve bu değeri noktadan sonra 4 rakam olacak şekilde döndürüyoruz.\nKodların ana yapısını oluşturduk.\n\n\nStreamlit Web Uygulamasının Yapımı\nWeb uygulaması tarafında kullanılan kodlar aşağıdadır.\n\n# Oturum durumu değişkenlerinin tanımlanması ve varsayılan değerlerin ayarlanması\nst.session_state.setdefault('rhos', []) # Korelasyon katsayıları\nst.session_state.setdefault('guesses', []) # Kullanıcının tahminleri\nst.session_state.setdefault('rmse_values', []) # RMSE değeri\nst.session_state.setdefault('plots', []) # Görseller\n\n# Korelasyonlu veri oluşturan fonksiyon\ndef generate_correlated_data():\n    num_points = int(np.random.uniform(low=100, high=1000))\n    rho = round(np.random.uniform(low=-1, high=1), 2)\n    cov_matrix = np.array([[1, rho], [rho, 1]])\n    mu = [0, 0]\n    correlated_data = np.random.multivariate_normal(mean=mu, cov=cov_matrix, size=num_points)\n    return correlated_data, rho\n\n# RMSE hesaplayan fonksiyon\ndef calculate_rmse(predictions, actuals):\n    mse = np.mean((predictions - actuals) ** 2)\n    rmse = round(np.sqrt(mse), 4)\n    return rmse\n\n# 'rhos' listesi boş ise yeni bir korelasyonlu veri oluşturulması ve korelasyon katsayısının kaydedilmesi\nif len(st.session_state['rhos']) == 0:\n    correlated_data, rho = generate_correlated_data()\n    st.session_state['rhos'].append(rho)\n\n# 'plots' listesi boşsa ilk görselin oluşturulması ve kaydedilmesi\nif len(st.session_state['plots']) == 0:\n    plt.figure(figsize=(8, 6))\n    plt.scatter(correlated_data[:,0], correlated_data[:,1], alpha=0.7)\n    plt.title('Scatter Plot of Correlated Data')\n    plt.xlabel('X')\n    plt.ylabel('Y')\n    plt.grid(True)\n    st.session_state['plots'].append(plt)\n    st.pyplot(st.session_state['plots'][0])\n\n# Kullanıcının tahmini\nuser_guess = st.sidebar.text_input(\n    label='Your Guess:',\n    value='0.0'\n)\n\n# Kullanıcı tahmininin ondalık sayıya dönüştürülmesi\nuser_guess = float(user_guess)\n\n# Tahmin butonunun oluşturulması\nguess_button = st.sidebar.button(label='Guess')\n\n# Tahmin butonuna basıldığında yapılacaklar\nif guess_button:\n    correlated_data, rho = generate_correlated_data()\n    st.session_state['rhos'].append(rho)\n    st.session_state['guesses'].append(user_guess)\n    rmse = calculate_rmse(np.array(st.session_state['rhos'][:-1]), np.array(st.session_state['guesses']))\n    st.session_state['rmse_values'].append(rmse)\n\n    st.subheader(f'Guess: {st.session_state[\"guesses\"][-1]}, Actual: {st.session_state[\"rhos\"][-2]}, RMSE: {st.session_state[\"rmse_values\"][-1]}')\n\n    plt.figure(figsize=(8, 6))\n    plt.scatter(correlated_data[:,0], correlated_data[:,1], alpha=0.7)\n    plt.title('Scatter Plot of Correlated Data')\n    plt.xlabel('X')\n    plt.ylabel('Y')\n    plt.grid(True)\n    st.session_state['plots'].append(plt)\n    st.pyplot(st.session_state['plots'][-1])\n\nWindows/Visual Studio Code için bir not: Streamlit uygulamasının yapımında kullanılacak olan kodları .py uzantılı app.py isminde bir script’e kaydedip terminalden streamlit run app.py komutu ile çalıştırabilirsiniz.\nGelecek içeriklerde görüşmek dileğiyle."
  },
  {
    "objectID": "posts/post_13/index.html",
    "href": "posts/post_13/index.html",
    "title": "TCMB’nin Faiz Oranlarına İlişkin Duyuru Metinlerinin Okunabilirliği",
    "section": "",
    "text": "Giriş\nOkunabilirlik, bir metni okumanın ve anlamanın ne kadar kolay olduğunu ifade eder. Metinlerin okunabilirliği, bilgi iletişiminde kritik bir rol oynayabilir ve bir metin ne kadar açık ve anlaşılır olursa, o kadar etkili bir iletişim sağlanabilir. Bu bağlamda, Türkiye Cumhuriyet Merkez Bankası’nın (TCMB) faiz oranlarına ilişkin duyuru metinlerinin ve tabi ki diğer metinlerin okunabilirliği üzerine odaklanmak, hem finansal anlamda hem de genel olarak kamuoyuyla iletişim açısından önem taşıyabilir.\nBank of Canada (Kanada Merkez Bankası), Readability and the Bank of Canada başlıklı bir çalışmalarında, banka yayınlarının 2015-2017 yılları için okunabilirliğini incelemiş ve genel olarak banka yayınlarının normalde bankanın kitlelerinin tükettiği haber makaleleri ve diğer içerikler kadar kolay okunmadığını bulmuş. Ancak aynı zamanda bankanın uluslararasında iyi bir konumda yer aldığı sonucuna da ulaşmış.\nOkunabilirliğini inceleyeceğimiz metinler burada, cbrt_press_releases.xlsx isimli excel dosyasında bulunmaktadır. Veri seti, Erdem Başçı ve Yaşar Fatih Karahan arası dönemlerde yayınlanan 148 adet İngilizce duyuru metnini kapsamaktadır. Son veri 2024 yılının Nisan ayına aittir.\n\n\nGunning’in Fog İndeksi\nİndeks, 1952 yılında Gunning Fog tarafından geliştirilmiştir ve aşağıdaki gibi hesaplanmaktadır.\n\\(GFI = 0.4[(\\frac{Total\\ words}{Total\\ Sentences}) + 100(\\frac{Complex\\ words}{Total\\ words})]\\)\nComplex words ile kastedilen üç ve daha fazla heceli kelimelerdir.\nİndeksin seviyeleri aşağıdadır.\n\n\n\nhttps://clickhelp.com/software-documentation-tool/user-manual/gunning-fog-index.html\n\n\n\n\nKullanılacak Kütüphaneler\n\nimport pandas as pd\nimport re\nimport syllapy\nimport textstat\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\n\n\n\nVeri Setinin İçe Aktarılması ve Bazı Ayarların Yapılması\n\ndf = pd.read_excel('cbrt_press_releases.xlsx')\n\ndf['Date'] = pd.to_datetime(df['Date'], dayfirst=True)\ndf = df.sort_values(by='Date')\ndf = df.set_index('Date')\n\n\n\nİndeksin Hesaplanması\n\ndef gunning_fog_index(text):\n    pattern = r'(\\b\\d+)\\.(\\d+\\b)'\n    text = re.sub(pattern, r'\\1,\\2', text)\n    text = text.translate(str.maketrans('', '', '\\n\\xa0'))\n\n    sentences = re.split(r'(?&lt;=[.!?])\\s+', text)\n    num_sentences = len(sentences)\n\n    words = re.findall(r'\\b(?:[a-zA-Z]+(?:-[a-zA-Z]+)?|-)\\b', text)\n    num_words = len(words)\n\n    avg_words_per_sentence = num_words / num_sentences\n\n    num_complex_words = sum(1 for word in words if syllapy.count(word) &gt;= 3)\n\n    gfi = 0.4 * (avg_words_per_sentence + 100 * (num_complex_words / num_words))\n\n    return round(gfi, 2)\n\ndf['Gunning_Fog_Index'] = df['Text'].apply(gunning_fog_index)\n\n\n\nİndeksin Görselleştirilmesi\n\nplt.figure(figsize=(12, 7))\nplt.scatter(df.index, df['Gunning_Fog_Index'], c=df['Gunning_Fog_Index'], cmap='coolwarm')\nplt.title(\"CBRT's Clarity Rating by Gunning's Fog Index\")\nplt.text(\n    0.99,\n    -0.1,\n    \"Based on CBRT's press releases on interest rates.\",\n    verticalalignment='bottom',\n    horizontalalignment='right',\n    transform=plt.gca().transAxes,\n    color='gray',\n    fontsize=10,\n    fontstyle='italic'\n)\nplt.colorbar()\nplt.show()\n\n\nİndeks değeri arttıkça okunabilirliğin zorlaştığını söyleyebiliriz. Tarihsel bazda baktığımızda okunabilirliğin gittikçe zorlaştığını görüyoruz.\n\ngovernor_stats = df.groupby('Governor')['Gunning_Fog_Index'].agg(['mean', 'std'])\n\nplt.figure(figsize=(10, 6))\nplt.scatter(governor_stats['mean'], governor_stats['std'], color='red', alpha=0.3, s=200)\n\nfor i, governor in enumerate(governor_stats.index):\n    plt.text(governor_stats['mean'][i], governor_stats['std'][i], governor, fontsize=11)\n\nplt.title(\"Governors' Gunning's Fog Index Rating\")\nplt.xlabel('Average')\nplt.ylabel('Standard Deviation')\nplt.grid(True)\nplt.text(\n    0.99,\n    -0.2,\n    \"Based on CBRT's press releases on interest rates.\",\n    verticalalignment='bottom',\n    horizontalalignment='right',\n    transform=plt.gca().transAxes,\n    color='gray',\n    fontsize=10,\n    fontstyle='italic'\n)\nplt.show()\n\n\nErdem Başçı ve Murat Çetinkaya dönemlerindeki metinler diğer dönemlere kıyasla daha kolay okunabilir duruyor. Çetinkaya’nın döneminde standart sapma daha düşük olduğu için kolay okunabilirlik Başçı’ya göre daha homojendir. Diğer taraftan, Naci Ağbal ve Yaşar Fatih Karahan dönemlerindeki metinler diğer dönemlere kıyasla daha zor okunabilir duruyor. Karahan’ın döneminde standart sapma daha düşük olduğu için zor okunabilirlik Ağbal’a göre daha homojendir.\n\n\ntextstat Paketini Neden Kullanmadık?\ntextstat paketi yaygın olarak kullanılsa da aşağıdaki görselde görüleceği üzere indeks değerleri arasında ciddi farklılıklar oluşmaktadır.\n\ndef textstat_gunning_fog_index(text):\n    pattern = r'(\\b\\d+)\\.(\\d+\\b)'\n    text = re.sub(pattern, r'\\1,\\2', text)\n    text = text.translate(str.maketrans('', '', '\\n\\xa0'))\n\n    gfi = textstat.gunning_fog(text)\n\n    return round(gfi, 2)\n\ndf['Gunning_Fog_Index_textstat'] = df['Text'].apply(textstat_gunning_fog_index)\n\nplt.figure(figsize=(10, 6))\nplt.scatter(\n    df['Gunning_Fog_Index'],\n    df['Gunning_Fog_Index_textstat'],\n    c=df['Gunning_Fog_Index'],\n    cmap='coolwarm'\n)\nplt.title(\"Comparison of Gunning's Fog Index Calculation: Manual vs. Using textstat Package\", fontsize=14)\nplt.xlabel('Manual')\nplt.ylabel('textstat Package')\nplt.text(\n    0.99,\n    -0.2,\n    \"Based on CBRT's press releases on interest rates.\",\n    verticalalignment='bottom',\n    horizontalalignment='right',\n    transform=plt.gca().transAxes,\n    color='gray',\n    fontsize=10,\n    fontstyle='italic'\n)\nplt.show()\n\n\nTeyit etmek için son duyuru metnini (25/04/2024) alalım.\n\ntext = df.loc['2024-04-25','Text']\npattern = r'(\\b\\d+)\\.(\\d+\\b)'\ntext = re.sub(pattern, r'\\1,\\2', text)\ntext = text.translate(str.maketrans('', '', '\\n\\xa0'))\n\nBenchmark olarak şuradaki web sitesini alabiliriz. İlgili web sitesinin yukarıdaki örnek metin için çıktısı aşağıdaki gibidir.\n\nManuel hesaplama ile yukarıdaki sonuçları karşılaştıralım.\nCümle sayısı 17.\n\nsentences = re.split(r'(?&lt;=[.!?])\\s+', text)\nnum_sentences = len(sentences)\nprint(num_sentences)\n\nKelime sayısı 379. 2 kelime farkı sayılardan geliyor. Sayıları çıkarmıştık.\n\nwords = re.findall(r'\\b(?:[a-zA-Z]+(?:-[a-zA-Z]+)?|-)\\b', text)\nnum_words = len(words)\nprint(num_words)\n\nHece sayısı 750. 3 hecelik fark web sitesi ile kullandığımız syllapy paketi arasındaki yöntem farklılığından kaynaklanıyor. Örneğin, web sitesinde “ongoing” için hece sayısını 2 verirken, paket ve online ortamdaki kaynaklar 3 veriyor.\n\nsyllable_counter = 0\nfor word in words:\n    word_syllable = syllapy.count(word)\n    syllable_counter += word_syllable\n    print(f\"{word}, {word_syllable}\")\nprint(syllable_counter)\n\nİndeks değerini 22.53 buluyoruz. Benchmark aldığımız web sitesi 21.98 buluyor. Pek farklılık yok.\n\ngunning_fog_index(text)\n\ntextstat paketi ise sonucu 15.15 veriyor ki ciddi anlamda sapıyor.\n\ntextstat.gunning_fog(text)\n\n\n\nTürkçeye Uyarlanan Okunabilirlik Formülleri\nÇalışmayı İngilizce metinler üzerinden yapsak da Türkçe metinler üzerinden de okunabilirlik ölçülebilir.\n\nAteşman Okunabilirlik Formülü (ülkemizde bu alanda yapılan ilk çalışma)\nÇetinkaya-Uzun Okunabilirlik Formülü\nBezirci-Yılmaz Okunabilirlik Formülü\nSönmez Formülü\n\nGelecek içeriklerde görüşmek dileğiyle."
  },
  {
    "objectID": "posts/post_11/index.html",
    "href": "posts/post_11/index.html",
    "title": "TCMB Başkan Yardımcısı Cevdet Akçay’ın ‘Link Kopmuş’ Dediği Konuşmasının Duygu Analizi",
    "section": "",
    "text": "Giriş\nTürkiye Cumhuriyet Merkez Bankası Başkan Yardımcısı Cevdet Akçay, 2024 yılının ilk Enflasyon Raporu bilgilendirme toplantısında şunları aktarmıştı:\nİçinde çalıştığımız setting’de, ağırlıklı ortalama fonlama maliyeti mevduat faizi linki kopmuş, politika faizi enflasyon linki kopmuş, faiz kur linki kopmuş…\nKonuşmasına şöyle devam etmişti:\nBiz yedi aydır bu kopan linkleri tekrar ihdas ediyoruz. Bu linkler tekrar ihdas edilecek. Veriler birikecek. Alan verileri kullanacaksınız, modelleme yapacaksınız. Oradan da 36’dan 38’e çıkma ihtiyacı, çok zor. Modelleme iyi bilen arkadaşlar bu dediğimi çok iyi anlayacaktır. Çok zor değil imkansıza yakın.\nPeki, bu konuşma sırasında Akçay’ın yüzünde hangi duygular oluştu?\nX (Twitter) profil fotoğraflarını veri seti olarak kullandığımız İstanbul İlçe Belediye Başkanlarının X (Twitter) Profillerindeki Duygu Dağılımı başlıklı çalışmada Py-Feat kütüphanesinden faydalanmıştık. Video üzerinden analiz yapacağımız bu çalışmada yine Py-Feat kütüphanesinden faydalanacağız. Kütüphane ile ilgili detaylı bilgi için ilgili çalışma incelenebilir. Çalışmada kullanacağımız video ise Youtube’dan indirilebilir.\n\n\nKullanılacak Kütüphaneler\n\nimport os\nfrom feat import Detector\nimport matplotlib.pyplot as plt\n\n\n\nVideo, Frame ve FPS Kavramlarının Tanımlanması\nBir video temel olarak bir dizi fotoğrafın belirli bir hızda birbirini takip ederek oynatılmasıdır. Her bir fotoğraf karesine frame deniliyor. FPS (Frames Per Second) ise saniyedeki kare ya da frame sayısıdır. Detaylı öğrenmek isteyenler için şuradaki videoyu tavsiye edebilirim.\n\n\nVideonun İçe Aktarılması\ncbrt_cevdet_akcay.mp4 isimli video içinde bulunduğumuz dizindedir.\n\ncurrent_dir = os.getcwd()\nvideo_path = os.path.join(current_dir, 'cbrt_cevdet_akcay.mp4')\n\n\n\nDuyguların Tespit Edilmesi ve Görselleştirilmesi\n\ndetector = Detector(\n    face_model='retinaface',\n    landmark_model='mobilefacenet',\n    au_model='xgb',\n    emotion_model='resmasknet',\n    facepose_model='img2pose',\n    identity_model='facenet'\n)\n\nvideo_prediction = detector.detect_video(video_path, skip_frames=24)\nprint(video_prediction.head())\n\nskip_frames parametresi video üzerindeki karelerin işlenmesinde atlanacak kare sayısını belirtir. Bir video genellikle saniyede çok sayıda kare içerir ve bu kareler arasında önemli ölçüde benzerlik olabilir. Özellikle video çok yüksek çözünürlüklü veya uzunsa her kareyi işlemek oldukça yoğun bir hesaplama gerektirebilir. Ancak bazı uygulamalarda her kareyi işlemek gerekli olmayabilir. Bu durumda, skip_frames kullanılabilir. skip_frames parametresi, belirli sayıda kareyi atlayarak işlem süresini azaltır. Örneğin, skip_frames=24 ile her 24. kareye işlem yapılır.\n\nDuygular aşağıdaki gibi çekilebilir.\n\nakcay_emotions = video_prediction.emotions\nprint(akcay_emotions.head())\n\n\nDuyguları görselleştirelim.\n\nfig, axes = plt.subplots(nrows=7, ncols=1, figsize=(10, 20))\nfor i, (emotion, color) in enumerate(zip(akcay_emotions.columns, ['r', 'g', 'b', 'c', 'm', 'y', 'k'])):\n    ax = akcay_emotions.plot(y=emotion, ax=axes[i], color=color, legend=False)\n    ax.set_ylabel(emotion.capitalize())\n\naxes[0].set_title(\"TCMB Başkan Yardımcısı Cevdet Akçay'ın 'Link Kopmuş' Dediği Konuşmasındaki Duygular\", fontsize=14)\nplt.tight_layout()\nplt.show()\n\n\nHer karede hangi duygunun daha yüksek olduğuna bakalım.\n\nmax_emotion_per_row = akcay_emotions.idxmax(axis=1)\nemotion_distribution = max_emotion_per_row.value_counts(normalize=True) * 100\n\nplt.figure(figsize=(10, 6))\nemotion_distribution.plot(kind='bar', color='skyblue')\nplt.title('Her Karede En Yüksek Duygu Kategorisinin Dağılımı')\nplt.xlabel('Duygu Kategorisi')\nplt.ylabel('Yüzde (%)')\nplt.xticks(rotation=0)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n\nGelecek içeriklerde görüşmek dileğiyle."
  },
  {
    "objectID": "posts/post_1/index.html",
    "href": "posts/post_1/index.html",
    "title": "Python Dünyama Hoş Geldin!",
    "section": "",
    "text": "Uygulamalı içerikler oluşturacağım bu blog, bana günlük, sana rehber olsun."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Python Blog",
    "section": "",
    "text": "world.urazdev()"
  },
  {
    "objectID": "posts/post_10/index.html",
    "href": "posts/post_10/index.html",
    "title": "Rutin Görevlerin Otomatize Edilmesi: Outlook ve Görev Zamanlayıcının Kullanılması",
    "section": "",
    "text": "Giriş\nİş verimliliğini artırmak için rutin görevlerin otomatize edilmesini önemsiyorum. Tekrarlayan rutin görevlerin otomatize edilmesi, çalışanların değerli zamanlarını stratejik ve yaratıcı faaliyetlere odaklamalarını sağlayabilir.\nBu uygulamada, Windows Görev Zamanlayıcı (Task Scheduler) ile belirlediğimiz rutin görevin otomatik olarak çalıştırılmasını sağlayacağız.\nRutin görevimiz şöyle olacak: Belirlediğimiz kripto paraların 52 + 1 haftalık kapanış fiyatlarını alacağız ve her birinin getiri ortalaması ile standart sapmasını hesaplayacağız. Getiri ortalamaları ile standart sapmaları grafik üzerine aktaracağız. Grafiği ve verileri sırasıyla PNG ve XLSX formatlarında kaydedeceğiz. Tüm bu bilgileri Microsoft Outlook uygulaması aracılığıyla göndereceğiz.\n\n\nKullanılacak Kütüphaneler\n\nimport win32com.client as win32\nimport os\nfrom datetime import datetime, timedelta\nimport yfinance as yf\nimport matplotlib.pyplot as plt\n\n\n\nGönderilecek Verilerin Çekilmesi, Hesaplamaların Yapılması ve Grafiğin Oluşturulması\n\ntickers = ['BTC-USD','ETH-USD','SOL-USD','XRP-USD','DOGE-USD','ADA-USD','AVAX-USD','DOT-USD','NEAR-USD','RNDR-USD']\n\ntoday_date = datetime.today().date()\nstart_date = today_date - timedelta(weeks=53)\n\ndf = yf.download(\n    tickers=tickers,\n    start=start_date,\n    end=today_date,\n    progress=False\n)\n\ndaily_returns = df['Adj Close'].pct_change()\n\n# print(daily_returns)\n\ndaily_returns.to_excel('cryptocurrency_returns_data.xlsx')\n\nstd_devs = daily_returns.std()\nmeans = daily_returns.mean()\n\nplt.figure(figsize=(10, 6))\nfor ticker in tickers:\n    plt.scatter(std_devs[ticker], means[ticker], s=100)\n    plt.text(std_devs[ticker], means[ticker], ticker, fontsize=8, ha='right', va='bottom')\n\nplt.title('Average and Standard Deviation of 52-Week Returns of Tracked Cryptocurrencies')\nplt.xlabel('Standard Deviation')\nplt.ylabel('Average')\nplt.grid(True)\nplt.savefig('cryptocurrency_returns_chart.png')\n# plt.show()\n\nGönderilecek grafik:\n\nGönderilecek excel dosyası içeriği:\n\n\n\nOutlook Mail İçeriğinin Hazırlanması\n\noutlook = win32.Dispatch('outlook.application')\nmail = outlook.CreateItem(0)\n\nmail.Subject = 'Cryptocurrency Returns Chart as of ' + datetime.now().strftime('%#d %b %Y %H:%M')\nmail.To = 'urazdev@gmail.com'\n\nattachment = mail.Attachments.Add(os.getcwd() + '\\cryptocurrency_returns_chart.png')\nattachment.PropertyAccessor.SetProperty('http://schemas.microsoft.com/mapi/proptag/0x3712001F', 'cryptocurrency_returns_chart')\n\nmail.HTMLBody = r\"\"\"\nDear Uraz,&lt;br&gt;&lt;br&gt;\nThe chart illustrating the average return and standard deviation for each cryptocurrency is as follows:&lt;br&gt;&lt;br&gt;\n&lt;img src=\"cid:cryptocurrency_returns_chart\"&gt;&lt;br&gt;&lt;br&gt;\nPlease find attached the Excel file containing the 52-week returns data for the tracked cryptocurrencies.&lt;br&gt;&lt;br&gt;\nBest regards,&lt;br&gt;\n@urazdev\n\"\"\"\n\nmail.Attachments.Add(os.getcwd() + '\\cryptocurrency_returns_data.xlsx')\n\nmail.Send()\n\nwin32com.client modülünün Dispatch fonksiyonu Outlook uygulaması oluşturur. Bu, Windows üzerinde Outlook’u otomatik olarak başlatmak ve programı kontrol etmek için kullanılır.\nCreateItem fonksiyonu, Outlook uygulaması üzerinden yeni bir öğe oluşturur. Burada 0 parametresi, bir e-posta öğesi oluşturmak için kullanılan sabit değerdir.\nmail.Subject, e-postanın başlığını belirtir.\nmail.To, e-postanın gönderileceği alıcı adresini belirtir.\nmail.Attachments.Add(), e-postaya bir ek ekler. Bunu hem PNG hem de XLSX için kullanıyoruz.\nattachment.PropertyAccessor.SetProperty(), eklenen dosyanın özelliklerini ayarlar. http://schemas.microsoft.com/mapi/proptag/0x3712001F, ek dosyanın gömülü dosya ismini belirlemek için kullanılır.\nmail.HTMLBody, e-postanın HTML biçimindeki gövdesini tanımlar.\nmail.Send(), oluşturulan e-postayı gönderir.\n\n\nGörev Zamanlayıcı Uygulamasına Görev Girilmesi\nGörev Zamanlayıcı uygulamasını açıyoruz ve Create Task’e tıklıyoruz.\nGeneral:\n\nTrigger:\n\nActions:\n\nProgram/Script için Python .exe dosyasının bulunduğu dosya konumu .exe uzantılı dosya ile girilmelidir. Bunun için CMD’de where python komutu çalıştırılabilir. Add arguments için çalıştırılacak script’in bulunduğu dosya konumu .py uzantılı dosya ile girilmelidir. Start in için çalıştırılacak script’in bulunduğu dosyanın konumu girilmelidir.\n\n\nTest\n\nGelecek içeriklerde görüşmek dileğiyle."
  },
  {
    "objectID": "posts/post_12/index.html",
    "href": "posts/post_12/index.html",
    "title": "TCMB’nin Faiz Oranlarına İlişkin Duyuru Metinlerinin İçerik Analizi",
    "section": "",
    "text": "Faiz kararları sadece sayılardan ibaret değildir. Türkiye Cumhuriyet Merkez Bankası’nın (TCMB) faiz oranlarına ilişkin basın duyuruları da büyük bir öneme sahiptir. Bu duyurular, TCMB’nin kararlarını destekleyen veya açıklayan bir metin içerir ve bu metin, ekonomi uzmanları, yatırımcılar ve piyasa analistleri için kritik bir kaynaktır. Basın duyuruları temelde, TCMB’nin ekonomik göstergeleri nasıl yorumladığını ve gelecekteki politika eğilimlerini nasıl değerlendirdiğini içerir.\nVeri seti, Erdem Başçı ve Yaşar Fatih Karahan arası dönemlerde yayınlanan 148 adet İngilizce duyuru metnini kapsamaktadır. Son veri 2024 yılının Nisan ayına aittir. cbrt_press_releases isimli excel dosyasında bulunan veri setine buradan ulaşabilirsiniz."
  },
  {
    "objectID": "posts/post_12/index.html#tarihlere-ve-başkanlara-göre-kelime-sayıları",
    "href": "posts/post_12/index.html#tarihlere-ve-başkanlara-göre-kelime-sayıları",
    "title": "TCMB’nin Faiz Oranlarına İlişkin Duyuru Metinlerinin İçerik Analizi",
    "section": "Tarihlere ve başkanlara göre kelime sayıları",
    "text": "Tarihlere ve başkanlara göre kelime sayıları\nKelimeleri sayacak word_counter isimli ve text parametresi bulunan bir fonksiyon yazalım.\n\ndef word_counter(text):\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    text = text.translate(str.maketrans('', '', '\\n\\xa0'))\n    text = text.lower()\n    tokens = word_tokenize(text)\n    return len(tokens)\n\nYukarıda ilk olarak, metindeki noktalama işaretlerini (string.punctuation ile tanımlı olanlar) kaldırıyoruz. Bu, metindeki noktalama işaretlerinin (virgül, nokta, ünlem işareti vb.) kelime sayısını etkilememesini sağlıyor. Ardından, metindeki boşlukları ve diğer belirli karakterleri (newline karakteri \\n ve non-breaking space karakteri \\xa0) kaldırıyoruz. Bu, metindeki boşlukların kelime sayısını etkilememesini sağlıyor. Metni lower() ile küçük harfe dönüştürüyoruz. Bu, büyük ve küçük harflerin ayrımını kaldırarak aynı kelimenin farklı biçimlerini aynı olarak değerlendirmesini sağlıyor. Metni word_tokenize() ile kelime parçalarına ayırıyoruz (tokenize işlemi). Bu, metindeki her bir kelimeyi ayrı bir öğe olarak ele alarak bu öğelerin sayısını saymamızı sağlıyor. Son olarak, kelime listesinin uzunluğunu len() ile hesaplayarak bu değeri döndürüyoruz.\n\ndf['Word Count'] = df['Text'].apply(word_counter)\n\nnum_governors = len(df['Governor'].unique())\ncolors = cm.Set1(np.linspace(0, 1, num_governors))\nunique_governors = df['Governor'].unique()\n\nplt.figure(figsize=(12, 7))\nfor i, governor in enumerate(unique_governors):\n    data = df[df['Governor'] == governor]\n    plt.plot(data.index, data['Word Count'], label=governor, linestyle='-', color=colors[i])\n\nplt.title('Word Counts by Dates and Governors')\nplt.text(\n    0.99,\n    -0.1,\n    \"Based on CBRT's press releases on interest rates.\",\n    verticalalignment='bottom',\n    horizontalalignment='right',\n    transform=plt.gca().transAxes,\n    color='gray',\n    fontsize=10,\n    fontstyle='italic'\n)\nplt.grid(True)\nplt.legend(fontsize='small')\nplt.tight_layout()\nplt.show()\n\nYukarıda ilk olarak, DataFrame’de bulunan Text sütunundaki her bir metin için word_counter fonksiyonunu çağırıyoruz ve her bir metnin kelime sayısı hesaplıyoruz. Görselde kullanmak için num_governors değişkenine başkan sayısını, colors değişkenine de bu başkan sayısı kadar renk atıyoruz. unique_governors ise daha sonra başkanları sırasıyla kullanacağımız için oluşturduğumuz bir değişken oluyor. Görseli oluştururken lejanttaki dönemlerine göre başkan sıralamasının önemini koruyoruz."
  },
  {
    "objectID": "posts/post_12/index.html#başkanlara-göre-kelime-bulutları",
    "href": "posts/post_12/index.html#başkanlara-göre-kelime-bulutları",
    "title": "TCMB’nin Faiz Oranlarına İlişkin Duyuru Metinlerinin İçerik Analizi",
    "section": "Başkanlara göre kelime bulutları",
    "text": "Başkanlara göre kelime bulutları\n\ngovernors_texts = df.groupby('Governor')['Text'].apply(' '.join)\n\nfig, axs = plt.subplots(7, 1, figsize=(16, 18))\nfor i, governor in enumerate(unique_governors, 1):\n    text = governors_texts[governor]\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    text = text.translate(str.maketrans('', '', '\\n\\xa0'))\n    text = text.lower()\n    wordcloud = WordCloud(\n        background_color='white',\n        colormap='gray',\n        contour_color='black',\n        contour_width=1\n    ).generate(text)\n\n    axs[i-1].imshow(wordcloud, interpolation='bilinear')\n    axs[i-1].set_title(f'Word Cloud for Governor {governor}', fontsize=12)\n    axs[i-1].axis('off')\n\nplt.figtext(\n    0.99,\n    -0.3,\n    \"Based on CBRT's press releases on interest rates.\",\n    verticalalignment='bottom',\n    horizontalalignment='right',\n    transform=plt.gca().transAxes,\n    color='gray',\n    fontsize=9,\n    fontstyle='italic'\n)\nplt.tight_layout()\nplt.show()\n\nYukarıda, governors_texts ile başkanların kendi dönemlerinde yayınlanan duyuru metinlerini bir araya getiriyoruz. Ardından, WordCloud nesnesi oluşturarak bir kelime bulutu oluşturma işlemini gerçekleştiriyoruz. Detaylı bakalım.\nWordCloud nesnesini oluştururken aşağıdaki parametreleri ve değerlerini kullandık.\nbackground_color='white', kelime bulutunun arka plan rengini beyaz yapıyor. Yani, kelime bulutunun içindeki kelimeler beyaz bir arka plan üzerine yerleştiriliyor. colormap='gray', kelime bulutunda kullanılacak renk haritasını gri tonlarda olacak şekilde ayarlıyor. Kelimelerin yoğunluklarına göre farklı gri tonları kullanılıyor. contour_color='black', kelime bulutunun kenar çizgilerinin rengini siyah yapıyor. Bu, kelime bulutunu çevreleyen konturun siyah olacağı anlamına gelir. contour_width=1, kelime bulutunun kenar çizgilerinin kalınlığı 1 piksel olarak ayarlıyor. Bu, konturun ince olacağı anlamına gelir.\ninterpolation parametresine atadığımız bilinear değeri, görüntüyü daha pürüzsüz bir şekilde yeniden örnekleme yaparak görüntülerken, kenarları daha yumuşak hale getiriyor."
  },
  {
    "objectID": "posts/post_12/index.html#tarihlere-göre-metinlerdeki-pozitifnegatif-duygu-değişimleri",
    "href": "posts/post_12/index.html#tarihlere-göre-metinlerdeki-pozitifnegatif-duygu-değişimleri",
    "title": "TCMB’nin Faiz Oranlarına İlişkin Duyuru Metinlerinin İçerik Analizi",
    "section": "Tarihlere göre metinlerdeki pozitif/negatif duygu değişimleri",
    "text": "Tarihlere göre metinlerdeki pozitif/negatif duygu değişimleri\nCentralBankRoBERTa, bir LLM’dir (large language model veya büyük dil modeli).\n\nCentralBankRoBERTa, temelde beş temel makroekonomik aktörü ayıran bir ekonomik aktör sınıflandırıcısı ile merkez bankası iletişimlerindeki cümlelerin duygusal içeriğini belirleyen ikili bir duygu sınıflandırıcısını birleştirir. Bu, merkez bankası iletişimlerindeki cümlelerin duygusal tonunu ve ekonomik aktörleri tanımak için kullanılabilir.\nMimarisi aşağıdaki gibidir.\n\nSentimentClassifier modeli, verilen bir cümlenin hane halkları, işletmeler, finans sektörü veya hükümet için olumlu mu yoksa olumsuz mu olduğunu belirlemek amacıyla tasarlanmıştır. Bu model, RoBERTa mimarisine dayanır ve doğru tahminler sağlamak için çeşitli ve kapsamlı bir veri kümesinde ince ayarlanmıştır (fine-tuned). Bir merkez bankasının iletişimini analiz etmek için duygusal içerikle ilgili uygun bir doğal dil işleme yöntemini test etmek üzere yapılan çalışmada FED, ECB ve BIS’den toplam 13,458 önceden etiketlenmiş cümle örneği kullanılmıştır.\nPerformans metrikleri şöyledir: Accuracy: 88%, F1 Score: 0.88, Precision: 0.88 ve Recall: 0.88\n\nsentiment_classifier = pipeline(\n    'text-classification',\n    model='Moritz-Pfeifer/CentralBankRoBERTa-sentiment-classifier'\n)\n\ndef classify_sentences(text):\n    text = text.replace('\\n', '').replace('\\xa0', '')\n    sentences = text.split('. ')\n    sentences = [sentence + '.' if not sentence.endswith('.') else sentence for sentence in sentences]\n    positive_count = 0\n    negative_count = 0\n    for sentence in sentences:\n        sentiment_result = sentiment_classifier(sentence)\n        positive_count += sum(1 for item in sentiment_result if item['label'] == 'positive')\n        negative_count += sum(1 for item in sentiment_result if item['label'] == 'negative')\n    return positive_count, negative_count\n\nYukarıda ilk olarak, pipeline fonksiyonunu çağırarak bir duygu sınıflandırma pipeline’ı oluşturuyoruz. Burada kullanılan model, Moritz Pfeifer tarafından oluşturulmuş CentralBankRoBERTa modelidir. Ardından, classify_sentences() fonksiyonunu tanımlıyoruz. Bu fonksiyon, bir metin alıyor ve metni cümlelere bölüyor. Metindeki \\n ve \\xa0 gibi boşlukları kaldırmak için replace() kullanıyoruz. Metni bir nokta ve boşluk ile cümlelere bölüyoruz. Cümle nokta ile bitmiyorsa cümlenin sonunda bir nokta olmasını sağlıyoruz. Pozitif ve negatif duyguların sayısını tutmak için positive_count ve negative_count isminde iki değişken tanımlıyor ve sıfır değerini atıyoruz. Her cümle için döngü oluşturuyoruz ve sentiment_classifier() fonksiyonunu çağırarak cümlenin duygusu belirliyoruz. Bu işlem, cümlenin duygusunu tahmin etmek için önceden eğitilmiş modeli kullanıyor. Duygu sonuçlarına bakarak pozitif ve negatif etiket sayılarını hesaplıyoruz. sentiment_result içindeki her bir öğe için etiketinin positive veya negative olup olmadığını kontrol ediyor ve sonuca göre ilgili sayaçları artırıyoruz. Son olarak fonksiyonda toplam pozitif ve negatif etiket sayılarını döndürüyoruz.\nAşağıda ise df’te, Positive Count ve Negative Count sütunları oluşturuyoruz ve classify_sentences() isimli text parametreli fonksiyonu çalıştırıyoruz.\n\ndf['Positive Count'] = 0\ndf['Negative Count'] = 0\n\ndf[['Positive Count', 'Negative Count']] = df['Text'].apply(classify_sentences).apply(pd.Series)\n\nplt.figure(figsize=(12, 8))\nplt.plot(df.index, df['Positive Count'], label='Positive Count', color='blue')\nplt.plot(df.index, df['Negative Count'], label='Negative Count', color='red')\nplt.fill_between(df.index, df['Positive Count'], df['Negative Count'], color='gray', alpha=0.3)\nplt.title('Positive and Negative Sentiment Counts Over Time')\nplt.text(\n    0.99,\n    -0.1,\n    \"Based on CBRT's press releases on interest rates.\",\n    verticalalignment='bottom',\n    horizontalalignment='right',\n    transform=plt.gca().transAxes,\n    color='gray',\n    fontsize=10,\n    fontstyle='italic'\n)\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\nGelecek içeriklerde görüşmek dileğiyle."
  },
  {
    "objectID": "posts/post_14/index.html",
    "href": "posts/post_14/index.html",
    "title": "İndikatör Ağırlıklarının Optimize Edilerek Teknik Reytinglerin Oluşturulması ve Al-Tut Stratejisinin Alt Edilmesi",
    "section": "",
    "text": "Tradingview platformundaki tanıma göre teknik reytingler, işlemcinin ve yatırımcıların karlı işlemler bulmasını kolaylaştırmak için çeşitli teknik göstergelerin derecelendirmelerini birleştiren teknik bir analiz aracıdır şeklindedir. Örneğin, BTC-USD’nin teknik reytingi aşağıdaki gibidir.\n\nOluşturacağımız teknik reytinglerin mantığı Tradingview platformundaki gibi olsa da bazı değişiklikler yapacağız. Bu değişiklikleri aşağıdaki gibi özetleyebiliriz.\n\nİndikatörlerin seçilmesi: Tradingview platformunda kullanılan indikatörlerden farklılaşacağız. Bu noktada, yatırımcı kendi indikatörlerini seçebilir.\nStratejilerin oluşturulması: Her bir indikatör için bir alım-satım stratejisi belirleyeceğiz. Bu noktada, yatırımcı kendi alım-satım stratejilerini belirleyebilir.\nStratejide kullanılacak değerlerin optimize edilmesi: Tarihsel verileri kullanarak ilgili indikatöre ait alım-satım stratejisinde kullanılacak değerlerden en iyi kümülatif getiri sağlayanları kullanacağız.\nReytinglerin oluşturulmasında ağırlıkların optimize edilmesi: Alım-satım kararında her indikatöre eşit ağırlık vermek yerine en iyi kümülatif getiri performansı gösteren ağırlık dağılımını kullanacağız.\n\nÇalışmada, tüm etkilerin sıfır varsayıldığının ve sadece getiriye odaklanıldığının altını çizelim. Ayrıca bu çalışma, aktif bir şekilde alım-satım stratejisi takip etmenin al-tut stratejisini her zaman yenebileceğini iddia etmemektedir."
  },
  {
    "objectID": "posts/post_14/index.html#kümülatif-getiri",
    "href": "posts/post_14/index.html#kümülatif-getiri",
    "title": "İndikatör Ağırlıklarının Optimize Edilerek Teknik Reytinglerin Oluşturulması ve Al-Tut Stratejisinin Alt Edilmesi",
    "section": "Kümülatif Getiri",
    "text": "Kümülatif Getiri\nÇalışmada kullanacağımız kümülatif getiri, tüm değerleme dönemi boyunca bileşik günlük getiriyi verir ve aşağıdaki gibi formülize edilebilir.\n\\(R_c = \\prod_{i=1}^{n} (1 + R_i) - 1 = (1 + R_1)(1 + R_2)...(1 + R_n) - 1\\)\nBir örnek ile nasıl hesaplandığına bakalım.\n\n# Veri\n\ndata = {\n    'Date': pd.date_range(start='2024-05-20', periods=5, freq='D'),\n    'Prices': [100, 101, 100.5, 102.5, 101.3]\n}\ndata_df = pd.DataFrame(data)\n\n# Getiri\n\ndata_df['Return'] = data_df['Prices'].pct_change()\n\n# Kümülatif Getiri\n\ndata_df['Cumulative Return'] = (1 + data_df['Return']).cumprod() - 1\ncumulative_return = data_df['Cumulative Return'].iloc[-1]\nprint(f\"Cumulative Return: {cumulative_return:.2%}\")\n\n# Başlangıç Değer Değişimi\n\ninitial_value = 100\nfinal_value = initial_value * (1 + cumulative_return)\nprint(f\"Final Value: {final_value:.2f} TL\")"
  },
  {
    "objectID": "posts/post_14/index.html#hesaplamanın-tasarımı",
    "href": "posts/post_14/index.html#hesaplamanın-tasarımı",
    "title": "İndikatör Ağırlıklarının Optimize Edilerek Teknik Reytinglerin Oluşturulması ve Al-Tut Stratejisinin Alt Edilmesi",
    "section": "Hesaplamanın Tasarımı",
    "text": "Hesaplamanın Tasarımı\nPopüler bir teknik analiz kütüphanesi olan ta kütüphanesini ve aşağıdaki trend, momentum ve volatilite indikatörlerini kullanacağız.\n\nTrend\n\nWeighted Moving Average: WMAIndicator\nMoving Average Convergence Divergence: MACD\nAverage Directional Movement Index: ADXIndicator\nIchimoku Kinkō Hyō: IchimokuIndicator\n\nMomentum\n\nRelative Strength Index: RSIIndicator\nRate of Change: ROCIndicator\n\nVolatilite\n\nBollinger Bands: BollingerBands\n\n\nHer bir indikatörde al sinyallerine 1, sat sinyallerine -1 ve nötr sinyallerine 0 atayacağız. Tüm optimizasyon işlemleri bittikten sonra aşağıdaki kriterleri baz alarak alım-satım kararını uygulayacağız.\n\n-1.0 \\(\\leq\\) reyting \\(\\leq\\) -0.5 ise Sat\n-0.5 \\(&lt;\\) reyting \\(&lt;\\) 0.5 ise Nötr\n0.5 \\(\\leq\\) reyting \\(\\leq\\) 1.0 ise Al"
  },
  {
    "objectID": "posts/post_14/index.html#indikatörlerin-hesaplanması",
    "href": "posts/post_14/index.html#indikatörlerin-hesaplanması",
    "title": "İndikatör Ağırlıklarının Optimize Edilerek Teknik Reytinglerin Oluşturulması ve Al-Tut Stratejisinin Alt Edilmesi",
    "section": "İndikatörlerin Hesaplanması",
    "text": "İndikatörlerin Hesaplanması\nta_rating_df adında bir pandas DataFrame oluşturalım. Bu DataFrame, tarih (Date), indikatör (Indicator) ve sinyal (Signal) bilgilerini içerecek. İndikatörlerden gelen bilgiler ta_rating_df’te saklanacak.\n\nta_rating_df = pd.DataFrame(columns=['Date', 'Indicator', 'Signal'])\n\n\nSinyallerin Güncellenmesi\nİndikatörlerin alım-satım stratejilerinde kullanılacak değerlerini optimize ederken alım-satım sinyallerini güncelleyeceğiz. Bu güncelleme ile beraber indikatörün art arda aynı sinyali üretmesi yerine sırayla sinyal üretmesini sağlayacağız.\n\ndef update_signals(signals):\n    updated_signals = []\n    previous_signal = None\n    for signal in signals:\n        if signal == previous_signal:\n            updated_signals.append(0)\n        else:\n            updated_signals.append(signal)\n            previous_signal = signal if signal in (1, -1) else previous_signal\n\n    return updated_signals\n\nupdate_signals fonksiyonu signals adında bir parametre alıyor. Güncellenmiş sinyalleri saklamak için updated_signals adında boş bir liste kullanılıyor. Bir önceki sinyali saklamak için de başlangıçta None olarak ayarlanmış previous_signal adında bir değişken kullanılıyor. Sonra süreç her bir sinyal için for döngüsü ile devam ediyor.\nif bloğu: Eğer mevcut sinyal önceki sinyal ile aynıysa updated_signals listesine 0 (nötr sinyali) ekleniyor.\nelse bloğu: Eğer mevcut sinyal önceki sinyal ile aynı değilse updated_signals listesine mevcut sinyal ekleniyor. Devamında ise mevcut sinyal 1 (al sinyali) veya -1 (sat sinyali) ise previous_signal değişkenine signal değişkeninin değeri atanıyor, aksi halde previous_signal değişkeninin değeri korunuyor.\nYukarıdaki işlemler bittikten sonra fonksiyon, updated_signals içerisinde bulunan güncellenmiş sinyaller listesini döndürür.\n\n\nİndikatörlerin Optimize Edilmesinde Genel Yaklaşım\nİndikatörlerin alım-satım stratejilerinde kullanılacak değerler optimize edilirken hepsinde ortak olan bir süreç bulunmaktadır. Alım-satım sinyali verecek değerler belli bir aralık içine alınır ve tarihsel verilerle kümülatif getirileri hesaplanır. En iyi kümülatif getiriyi sağlayan değerler kullanılarak sinyaller üretilir ve veriler ta_rating_df’te saklanır.\nKodsal olarak süreç şöyledir:\nbest_cum_return, best_params ve best_strategy_df değişkenleri tanımlanır. best_cum_return değişkeninin -np.inf değeri negatif sonsuz demektir ve henüz en iyi kümülatif getirinin hesaplanmadığı; best_params değişkeninin None değeri henüz en iyi getiriyi sağlayan stratejiye ait parametrelerin olmadığı; best_strategy_df değişkeninin None değeri ise henüz en iyi getiriyi sağlayan stratejinin işlem verilerinin olmadığı anlamına gelir.\nproduct fonksiyonu ile stratejide kullanılacak değerler oluşturulur ve bu değerler döngüye girer.\nSinyaller üretilir ve bu sinyaller güncellenir. Güncellenen sinyaller üzerinden kümülatif getiriler hesaplanır. Kümülatif getirilerden önce hesaplanan getiri hesaplamasında bir al-sat sinyalinden diğer al-sat sinyaline olan geçişe dikkat edilir.\nstrategy_cum_return’ün best_cum_return’den büyük olup olmadığına bakılır. Eğer büyükse başta oluşturulan değişkenler güncellenir.\nStratejinin en iyi değerleri ekrana basılır ve DataFrame’e son hali verilerek döndürülür.\n\n\nİndikatörlerin Optimize Edilmesi\n\nRelative Strength Index\nRelative Strength Index, bir varlığın geçmiş fiyat değişimlerine dayalı olarak aşırı alım veya aşırı satım durumlarını belirlemeye yardımcı olan bir momentum indikatörüdür.\nRelative Strength Index için seçilen strateji, indikatörü kullanarak alım-satım sinyalleri üretir ve bu sinyalleri optimize ederek en iyi kümülatif getiriyi elde etmeyi amaçlar. Belirli bir pencere boyutu, aşırı alım ve aşırı satım seviyeleri aralığını kullanarak fiyatların aşırı alım veya aşırı satım durumunda olup olmadığını belirler. Ardından, bu sinyalleri alım ve satım pozisyonlarına dönüştürür ve stratejinin getirisini hesaplar. En iyi parametrelerle belirlenen bu sinyaller daha sonra alım-satım stratejisi için kullanılır.\n\ndef optimize_and_calculate_rsi(df, window_range, overbought_range, oversold_range):\n    best_cum_return = -np.inf\n    best_params = None\n    best_strategy_df = None\n\n    for window, overbought, oversold in product(window_range, overbought_range, oversold_range):\n        strategy_df = df.copy().reset_index(drop=True)\n\n        rsi = momentum.RSIIndicator(strategy_df['Close'], window=window).rsi()\n        signal = np.where(rsi &lt; oversold, 1, np.where(rsi &gt; overbought, -1, 0))\n\n        strategy_df['Signal'] = signal\n        strategy_df['Signal Updated'] = update_signals(strategy_df['Signal'].tolist())\n\n        if not (1 in strategy_df['Signal Updated'].values and -1 in strategy_df['Signal Updated'].values):\n            continue\n\n        strategy_df_ret_calc = strategy_df.copy()\n        strategy_df_ret_calc = strategy_df_ret_calc[strategy_df_ret_calc['Signal Updated'] != 0]\n        strategy_df_ret_calc['Strategy Return'] = strategy_df_ret_calc['Close'].pct_change()\n\n        for index, row in strategy_df_ret_calc.iterrows():\n            signal = row['Signal Updated']\n            if signal == 1:\n                strategy_df_ret_calc.loc[index, 'Strategy Return'] *= -1\n\n        strategy_df_ret_calc = strategy_df_ret_calc[['Strategy Return']]\n        strategy_df = pd.merge(strategy_df, strategy_df_ret_calc, how='left', left_index=True, right_index=True)\n        strategy_df['Strategy Return'] = strategy_df['Strategy Return'].fillna(0)\n\n        strategy_df['Cumulative Return'] = (1 + strategy_df['Strategy Return']).cumprod() - 1\n        strategy_cum_return = strategy_df['Cumulative Return'].iloc[-1]\n\n        if strategy_cum_return &gt; best_cum_return:\n            best_cum_return = strategy_cum_return\n            best_params = (window, overbought, oversold)\n            best_strategy_df = strategy_df.copy()\n\n    print(f\"Best Params (Relative Strength Index): Window={best_params[0]}, Overbought={best_params[1]}, Oversold={best_params[2]}\")\n\n    best_strategy_df['Date'] = df.index\n    best_strategy_df['Indicator'] = 'Relative Strength Index'\n    rsi_signals_df = best_strategy_df[['Date', 'Signal', 'Indicator']]\n\n    return rsi_signals_df\n\nwindow_range = range(10, 21, 1)\noverbought_range = range(65, 86, 5)\noversold_range = range(15, 36, 5)\n\nrsi_signals_df = optimize_and_calculate_rsi(df=df, window_range=window_range, overbought_range=overbought_range, oversold_range=oversold_range)\nta_rating_df = pd.concat([ta_rating_df, rsi_signals_df], ignore_index=True)\n\nFonksiyondan alınan aşağıdaki kod bloğunu inceleyelim.\n\nrsi = momentum.RSIIndicator(strategy_df['Close'], window=window).rsi()\nsignal = np.where(rsi &lt; oversold, 1, np.where(rsi &gt; overbought, -1, 0))\n\nBu kod bloğu, indikatörün hesaplanmasını ve buna göre alım-satım sinyallerinin üretilmesini sağlar. İlk olarak, belirli bir pencere boyutu kullanılarak RSI hesaplanır. Ardından, RSI değerleri belirlenen aşırı alım ve aşırı satım seviyeleri ile karşılaştırılır. Eğer RSI değeri belirlenen aşırı satım seviyesinin altındaysa, bu durum aşırı satım olarak kabul edilir ve alım sinyali üretilir. Benzer şekilde, RSI değeri belirlenen aşırı alım seviyesinin üzerindeyse, bu durum aşırı alım olarak kabul edilir ve satım sinyali üretilir. RSI değeri bu seviyelerin arasındaysa, herhangi bir alım-satım sinyali üretilmez. Bu şekilde, RSI indikatörü kullanılarak belirli bir varlığın alım-satım kararları otomatik olarak belirlenmiş olur.\nRSI stratejisinin optimize edilmesi için kullanılan parametre aralıkları şöyledir:\n\nwindow_range: Pencere boyutunun aralığıdır. Değerler 10 ile 20 arasında her biri 1’er birim artacak şekilde belirlenmiştir.\noverbought_range: Aşırı alım seviyesinin belirlenmesi için kullanılan değerlerin aralığıdır. Değerler 65 ile 85 arasında her biri 5’er birim artacak şekilde belirlenmiştir.\noversold_range: Aşırı satım seviyesinin belirlenmesi için kullanılan değerlerin aralığıdır. Değerler 15 ile 35 arasında her biri 5’er birim artacak şekilde belirlenmiştir.\n\nÖrneğe göre en iyi kümülatif getiriyi sağlayan parametreler şunlardır:\nBest Params (Relative Strength Index): Window=20, Overbought=85, Oversold=25\n\n\nWeighted Moving Average\nWeighted Moving Average, belirli bir zaman diliminde fiyatların ağırlıklı ortalamasını hesaplayarak trendi belirlemeye yardımcı olan bir indikatördür.\nWeighted Moving Average için seçilen strateji, indikatörü kullanarak alım-satım sinyalleri üretir ve bu sinyalleri optimize ederek en iyi kümülatif getiriyi elde etmeyi amaçlar. Kısa ve uzun vadeli hareketli ortalamalar arasındaki ilişkiyi değerlendirir. Kısa vadeli WMA’in uzun vadeli WMA’yi aştığı durumlarda alım, kısa vadeli WMA’in uzun vadeli WMA’den düşük olduğu durumlarda satış sinyali üretir. Bu sinyalleri kullanarak stratejinin getirisini hesaplar ve en iyi parametrelerle belirlenen sinyallerle işlem yapar.\n\ndef optimize_and_calculate_wma(df, short_range, long_range):\n    best_cum_return = -np.inf\n    best_params = None\n    best_strategy_df = None\n\n    for short, long in product(short_range, long_range):\n        if short &gt;= long:\n            continue\n\n        strategy_df = df.copy().reset_index(drop=True)\n\n        wma_short = trend.WMAIndicator(strategy_df['Close'], window=short).wma()\n        wma_long = trend.WMAIndicator(strategy_df['Close'], window=long).wma()\n\n        signal = np.where(wma_short &gt; wma_long, 1, np.where(wma_short &lt; wma_long, -1, 0))\n\n        strategy_df['Signal'] = signal\n        strategy_df['Signal Updated'] = update_signals(strategy_df['Signal'].tolist())\n\n        if not (1 in strategy_df['Signal Updated'].values and -1 in strategy_df['Signal Updated'].values):\n            continue\n\n        strategy_df_ret_calc = strategy_df.copy()\n        strategy_df_ret_calc = strategy_df_ret_calc[strategy_df_ret_calc['Signal Updated'] != 0]\n        strategy_df_ret_calc['Strategy Return'] = strategy_df_ret_calc['Close'].pct_change()\n\n        for index, row in strategy_df_ret_calc.iterrows():\n            signal = row['Signal Updated']\n            if signal == 1:\n                strategy_df_ret_calc.loc[index, 'Strategy Return'] *= -1\n\n        strategy_df_ret_calc = strategy_df_ret_calc[['Strategy Return']]\n        strategy_df = pd.merge(strategy_df, strategy_df_ret_calc, how='left', left_index=True, right_index=True)\n        strategy_df['Strategy Return'] = strategy_df['Strategy Return'].fillna(0)\n\n        strategy_df['Cumulative Return'] = (1 + strategy_df['Strategy Return']).cumprod() - 1\n        strategy_cum_return = strategy_df['Cumulative Return'].iloc[-1]\n\n        if strategy_cum_return &gt; best_cum_return:\n            best_cum_return = strategy_cum_return\n            best_params = (short, long)\n            best_strategy_df = strategy_df.copy()\n\n    print(f\"Best Params (Weighted Moving Average): Short={best_params[0]}, Long={best_params[1]}\")\n\n    best_strategy_df['Date'] = df.index\n    best_strategy_df['Indicator'] = 'Weighted Moving Average'\n    wma_signals_df = best_strategy_df[['Date', 'Signal', 'Indicator']]\n\n    return wma_signals_df\n\nshort_range = range(20, 101, 5)\nlong_range = range(50, 201, 5)\n\nwma_signals_df = optimize_and_calculate_wma(df=df, short_range=short_range, long_range=long_range)\nta_rating_df = pd.concat([ta_rating_df, wma_signals_df], ignore_index=True)\n\nFonksiyondan alınan aşağıdaki kod bloğunu inceleyelim.\n\nwma_short = trend.wma_indicator(strategy_df['Close'], window=short)\nwma_long = trend.wma_indicator(strategy_df['Close'], window=long)\nsignal = np.where(wma_short &gt; wma_long, 1, np.where(wma_short &lt; wma_long, -1, 0))\n\nBu kod bloğu, indikatörün hesaplanmasını ve buna göre alım-satım sinyallerinin üretilmesini sağlar. İlk olarak, kısa vadeli WMA ve uzun vadeli WMA değerleri, belirli pencere boyutları (short ve long) kullanılarak hesaplanır. Daha sonra, kısa vadeli WMA’in uzun vadeli WMA’yi aştığı durumlarda alım sinyali üretilir, kısa vadeli WMA’in uzun vadeli WMA’den düşük olduğu durumlarda ise satım sinyali üretilir. Eğer kısa vadeli WMA ve uzun vadeli WMA birbirine eşitse veya kesişirse, herhangi bir sinyal üretilmez. Bu şekilde, WMA indikatörü kullanılarak belirli bir varlığın alım-satım kararları otomatik olarak belirlenmiş olur.\nKısa vadeli değerin uzun vadeli değere eşit veya uzun vadeli değerden büyük olmasının önüne geçilmiştir.\nWMA stratejisinin optimize edilmesi için kullanılan parametre aralıkları şöyledir:\n\nshort_range: Kısa vadeli ağırlıklı hareketli ortalama için kullanılacak pencere boyutu aralığıdır. Değerler 20 ile 100 arasında 5’er birim artacak şekilde belirlenmiştir.\nlong_range: Uzun vadeli ağırlıklı hareketli ortalama için kullanılacak pencere boyutu aralığıdır. Değerler 50 ile 200 arasında 5’er birim artacak şekilde belirlenmiştir.\n\nÖrneğe göre en iyi kümülatif getiriyi sağlayan parametreler şunlardır:\nBest Params (Weighted Moving Average): Short=45, Long=65\n\n\nMoving Average Convergence Divergence\nMoving Average Convergence Divergence, iki farklı üssel hareketli ortalamanın (EMA) farkını hesaplayarak trendin yönünü ve gücünü belirlemeye yardımcı olur\nMoving Average Convergence Divergence için seçilen strateji, indikatörü kullanarak alım-satım sinyalleri üretir ve bu sinyalleri optimize ederek en iyi kümülatif getiriyi elde etmeyi hedefler. Kısa vadeli ve uzun vadeli hareketli ortalamalar arasındaki farkı ve hareketli ortalamaların ortalama yakınsama ve diverjansını hesaplar. Ardından, sinyal hattıyla karşılaştırarak alım ve satım sinyallerini üretir. Bu sinyallerle stratejinin getirisini hesaplar ve en iyi parametrelerle belirlenen sinyallerle işlem yapar.\n\ndef optimize_and_calculate_macd(df, window_short_range, window_long_range, window_signal_range):\n    best_cum_return = -np.inf\n    best_params = None\n    best_strategy_df = None\n\n    for window_short, window_long, window_signal in product(window_short_range, window_long_range, window_signal_range):\n        strategy_df = df.copy().reset_index(drop=True)\n\n        macd_obj = trend.MACD(close=strategy_df['Close'], window_fast=window_short, window_slow=window_long, window_sign=window_signal)\n        macd = macd_obj.macd()\n        signal_line = macd_obj.macd_signal()\n\n        signal = np.where(macd &gt; signal_line, 1, np.where(macd &lt; signal_line, -1, 0))\n\n        strategy_df['Signal'] = signal\n        strategy_df['Signal Updated'] = update_signals(strategy_df['Signal'].tolist())\n\n        if not (1 in strategy_df['Signal Updated'].values and -1 in strategy_df['Signal Updated'].values):\n            continue\n\n        strategy_df_ret_calc = strategy_df.copy()\n        strategy_df_ret_calc = strategy_df_ret_calc[strategy_df_ret_calc['Signal Updated'] != 0]\n        strategy_df_ret_calc['Strategy Return'] = strategy_df_ret_calc['Close'].pct_change()\n\n        for index, row in strategy_df_ret_calc.iterrows():\n            signal = row['Signal Updated']\n            if signal == 1:\n                strategy_df_ret_calc.loc[index, 'Strategy Return'] *= -1\n\n        strategy_df_ret_calc = strategy_df_ret_calc[['Strategy Return']]\n        strategy_df = pd.merge(strategy_df, strategy_df_ret_calc, how='left', left_index=True, right_index=True)\n        strategy_df['Strategy Return'] = strategy_df['Strategy Return'].fillna(0)\n\n        strategy_df['Cumulative Return'] = (1 + strategy_df['Strategy Return']).cumprod() - 1\n        strategy_cum_return = strategy_df['Cumulative Return'].iloc[-1]\n\n        if strategy_cum_return &gt; best_cum_return:\n            best_cum_return = strategy_cum_return\n            best_params = (window_short, window_long, window_signal)\n            best_strategy_df = strategy_df.copy()\n\n    print(f\"Best Params (Moving Average Convergence Divergence): Short Window={best_params[0]}, Long Window={best_params[1]}, Signal Window={best_params[2]}\")\n\n    best_strategy_df['Date'] = df.index\n    best_strategy_df['Indicator'] = 'Moving Average Convergence Divergence'\n    macd_signals_df = best_strategy_df[['Date', 'Signal', 'Indicator']]\n\n    return macd_signals_df\n\nwindow_short_range = range(6, 13)\nwindow_long_range = range(20, 41)\nwindow_signal_range = range(4, 11)\n\nmacd_signals_df = optimize_and_calculate_macd(df=df, window_short_range=window_short_range, window_long_range=window_long_range, window_signal_range=window_signal_range)\nta_rating_df = pd.concat([ta_rating_df, macd_signals_df], ignore_index=True)\n\nFonksiyondan alınan aşağıdaki kod bloğunu inceleyelim.\n\nmacd_obj = trend.MACD(close=strategy_df['Close'], window_fast=window_short, window_slow=window_long, window_sign=window_signal)\nmacd = macd_obj.macd()\nsignal_line = macd_obj.macd_signal()\nsignal = np.where(macd &gt; signal_line, 1, np.where(macd &lt; signal_line, -1, 0))\n\nBu kod bloğu, indikatörün hesaplanmasını ve buna göre alım-satım sinyallerinin üretilmesini sağlar. İlk olarak, hızlı, yavaş ve sinyal pencere boyutları kullanılarak bir MACD nesnesi oluşturulur. MACD hesaplaması, hızlı hareketli ortalamanın yavaş hareketli ortalamadan çıkarılmasıyla elde edilir. Ardından, MACD hattı ve sinyal hattı hesaplanır. MACD hattı, hızlı EMA ile yavaş EMA arasındaki farkı gösterirken, sinyal hattı ise MACD hattının belirli bir zaman aralığındaki hareketli ortalamasıdır. Son olarak, MACD hattının sinyal hattını aştığı durumlarda alım sinyali üretilirken, MACD hattının sinyal hattının aşağısına geçtiği durumlarda satım sinyali üretilir. Eğer MACD hattı ve sinyal hattı birbirine eşitse veya kesişirse, herhangi bir sinyal üretilmez. Bu şekilde, MACD indikatörü kullanılarak belirli bir varlığın alım-satım kararları otomatik olarak belirlenir.\nMACD stratejisinin optimize edilmesi için kullanılan parametre aralıkları şöyledir:\n\nwindow_short_range: Kısa vadeli hareketli ortalama için kullanılacak pencere boyutu aralığıdır. Değerler 6 ile 12 arasında 1’er birim artacak şekilde belirlenmiştir.\nwindow_long_range: Uzun vadeli hareketli ortalama için kullanılacak pencere boyutu aralığıdır. Değerler 20 ile 40 arasında 1’er birim artacak şekilde belirlenmiştir.\nwindow_signal_range: MACD hattının sinyal hattını oluşturmak için kullanılacak pencere boyutu aralığıdır. Değerler 4 ile 10 arasında 1’er birim artacak şekilde belirlenmiştir.\n\nÖrneğe göre en iyi kümülatif getiriyi sağlayan parametreler şunlardır:\nBest Params (Moving Average Convergence Divergence): Short Window=6, Long Window=20, Signal Window=4\n\n\nAverage Directional Index\nAverage Directional Index, bir varlığın trendinin gücünü ölçen bir indikatördür.\nAverage Directional Index için seçilen strateji, indikatörü kullanarak alım-satım sinyalleri üretir ve bu sinyalleri optimize ederek en iyi kümülatif getiriyi hedefler. ADX, trendin gücünü ölçerken, +DI ve -DI göstergeleri trendin yönünü belirler. Belirli bir pencere boyutu ve eşik değeri ile bu göstergeleri değerlendirir ve belirli bir eşik değerinin üzerindeyse ve +DI, -DI’dan büyükse alım, tersi durumda satım sinyali üretir. Bu sinyallerle stratejinin getirisini hesaplar ve en iyi parametrelerle belirlenen sinyallerle işlem yapar.\n\ndef optimize_and_calculate_adx(df, window_range, threshold_range):\n    best_cum_return = -np.inf\n    best_params = None\n    best_strategy_df = None\n\n    for window in window_range:\n        for threshold in threshold_range:\n            strategy_df = df.copy().reset_index(drop=True)\n\n            adx_obj = trend.ADXIndicator(high=strategy_df['High'], low=strategy_df['Low'], close=strategy_df['Close'], window=window)\n            adx = adx_obj.adx()\n            plus_di = adx_obj.adx_pos()\n            minus_di = adx_obj.adx_neg()\n\n            for i in range(len(strategy_df)):\n                last_adx = adx.iloc[i]\n                last_plus_di = plus_di.iloc[i]\n                last_minus_di = minus_di.iloc[i]\n\n                if last_adx &gt; threshold:\n                    if last_plus_di &gt; last_minus_di:\n                        signal = 1\n                    elif last_plus_di &lt; last_minus_di:\n                        signal = -1\n                    else:\n                        signal = 0\n                else:\n                    signal = 0\n\n                strategy_df.loc[i, 'Signal'] = signal\n\n            strategy_df['Signal Updated'] = update_signals(strategy_df['Signal'].tolist())\n\n            if not (1 in strategy_df['Signal Updated'].values and -1 in strategy_df['Signal Updated'].values):\n                continue\n\n            strategy_df_ret_calc = strategy_df.copy()\n            strategy_df_ret_calc = strategy_df_ret_calc[strategy_df_ret_calc['Signal Updated'] != 0]\n            strategy_df_ret_calc['Strategy Return'] = strategy_df_ret_calc['Close'].pct_change()\n\n            for index, row in strategy_df_ret_calc.iterrows():\n                signal = row['Signal Updated']\n                if signal == 1:\n                    strategy_df_ret_calc.loc[index, 'Strategy Return'] *= -1\n\n            strategy_df_ret_calc = strategy_df_ret_calc[['Strategy Return']]\n            strategy_df = pd.merge(strategy_df, strategy_df_ret_calc, how='left', left_index=True, right_index=True)\n            strategy_df['Strategy Return'] = strategy_df['Strategy Return'].fillna(0)\n\n            strategy_df['Cumulative Return'] = (1 + strategy_df['Strategy Return']).cumprod() - 1\n            strategy_cum_return = strategy_df['Cumulative Return'].iloc[-1]\n\n            if strategy_cum_return &gt; best_cum_return:\n                best_cum_return = strategy_cum_return\n                best_params = (window, threshold)\n                best_strategy_df = strategy_df.copy()\n\n    print(f\"Best Params (Average Directional Index): Window={best_params[0]}, Threshold={best_params[1]}\")\n\n    best_strategy_df['Date'] = df.index\n    best_strategy_df['Indicator'] = 'Average Directional Index'\n    adx_signals_df = best_strategy_df[['Date', 'Signal', 'Indicator']]\n\n    return adx_signals_df\n\nwindow_range = range(10, 21)\nthreshold_range = range(10, 31)\nadx_signals_df = optimize_and_calculate_adx(df=df, window_range=window_range, threshold_range=threshold_range)\nta_rating_df = pd.concat([ta_rating_df, adx_signals_df], ignore_index=True)\n\nFonksiyondan alınan aşağıdaki kod bloğunu inceleyelim.\n\nadx_obj = trend.ADXIndicator(high=strategy_df['High'], low=strategy_df['Low'], close=strategy_df['Close'], window=window)\nadx = adx_obj.adx()\nplus_di = adx_obj.adx_pos()\nminus_di = adx_obj.adx_neg()\n\nfor i in range(len(strategy_df)):\n    last_adx = adx.iloc[i]\n    last_plus_di = plus_di.iloc[i]\n    last_minus_di = minus_di.iloc[i]\n\n    if last_adx &gt; threshold:\n        if last_plus_di &gt; last_minus_di:\n            signal = 1\n        elif last_plus_di &lt; last_minus_di:\n            signal = -1\n        else:\n            signal = 0\n    else:\n        signal = 0\n\nBu kod bloğu, indikatörün hesaplanmasını ve buna göre alım-satım sinyallerinin üretilmesini sağlar. İlk olarak, yüksek, düşük ve kapanış fiyatlarından oluşan veri seti ile birlikte belirli bir pencere boyutu kullanılarak bir ADX nesnesi oluşturulur. Daha sonra, ADX’in yanı sıra +DI ve -DI endeksler de hesaplanır. +DI, alım hareketinin gücünü, -DI ise satım hareketinin gücünü ölçer. Son olarak, belirlenen bir eşik değeri (threshold) kullanılarak alım-satım sinyalleri üretilir. Eğer ADX değeri belirli eşik değerini aşarsa, +DI ve -DI yönlü endeksler karşılaştırılır. Eğer +DI, -DI’dan büyükse, alım sinyali üretilir. Eğer -DI, +DI’dan büyükse, satım sinyali üretilir. Eğer artı ve eksi endeksler birbirine eşitse veya eşik değerini aşan bir ADX değeri yoksa, herhangi bir sinyal üretilmez. Bu şekilde, ADX indikatörü kullanılarak belirli bir varlığın trendinin gücüne bağlı olarak alım-satım kararları otomatik olarak belirlenir.\nADX stratejisinin optimize edilmesi için kullanılan parametre aralıkları şöyledir:\n\nwindow_range: Pencere boyutu aralığıdır. Değerler 10 ile 20 arasında 1’er birim artacak şekilde belirlenmiştir.\nthreshold_range: ADX indikatörünün değerini değerlendirirken kullanılacak olan eşik değerdir. Değerler 10 ile 30 arasında 1’er birim artacak şekilde belirlenmiştir.\n\nÖrneğe göre en iyi kümülatif getiriyi sağlayan parametreler şunlardır:\nBest Params (Average Directional Index): Window=11, Threshold=12\n\n\nBollinger Bands\nBollinger Bands, fiyatların ortalamadan sapmasını ölçerek volatiliteyi gösteren bir indikatördür.\nBollinger Bands için seçilen strateji, indikatörü kullanarak alım-satım sinyalleri üretir ve bu sinyalleri optimize ederek en iyi kümülatif getiriyi hedefler. Bollinger Bands, fiyatın ortalamadan ne kadar uzakta olduğunu ölçer ve standart sapma kullanarak üst ve alt bantları hesaplar. Belirli bir pencere boyutu ve standart sapma aralığı ile bu bantları hesaplar ve fiyatın bu bantların içinde veya dışında olduğu durumları değerlendirir. Fiyatın alt bant altında olduğu durumlarda alım, üst bant üstünde olduğu durumlarda satım sinyali üretir. Bu sinyallerle stratejinin getirisini hesaplar ve en iyi parametrelerle belirlenen sinyallerle işlem yapar.\n\ndef optimize_and_calculate_bollinger_bands(df, window_range, std_dev_range):\n    best_cum_return = -np.inf\n    best_params = None\n    best_strategy_df = None\n\n    for window, std_dev in product(window_range, std_dev_range):\n        strategy_df = df.copy().reset_index(drop=True)\n\n        bb = volatility.BollingerBands(close=strategy_df['Close'], window=window, window_dev=std_dev)\n        bb_lband = bb.bollinger_lband()\n        bb_hband = bb.bollinger_hband()\n\n        for i in range(len(strategy_df)):\n            lband = bb_lband.iloc[i]\n            hband = bb_hband.iloc[i]\n            close_price = strategy_df['Close'].iloc[i]\n\n            if close_price &lt; lband:\n                signal = 1\n            elif close_price &gt; hband:\n                signal = -1\n            else:\n                signal = 0\n\n            strategy_df.loc[i, 'Signal'] = signal\n\n        strategy_df['Signal Updated'] = update_signals(strategy_df['Signal'].tolist())\n\n        if not (1 in strategy_df['Signal Updated'].values and -1 in strategy_df['Signal Updated'].values):\n            continue\n\n        strategy_df_ret_calc = strategy_df.copy()\n        strategy_df_ret_calc = strategy_df_ret_calc[strategy_df_ret_calc['Signal Updated'] != 0]\n        strategy_df_ret_calc['Strategy Return'] = strategy_df_ret_calc['Close'].pct_change()\n\n        for index, row in strategy_df_ret_calc.iterrows():\n            signal = row['Signal Updated']\n            if signal == 1:\n                strategy_df_ret_calc.loc[index, 'Strategy Return'] *= -1\n\n        strategy_df_ret_calc = strategy_df_ret_calc[['Strategy Return']]\n        strategy_df = pd.merge(strategy_df, strategy_df_ret_calc, how='left', left_index=True, right_index=True)\n        strategy_df['Strategy Return'] = strategy_df['Strategy Return'].fillna(0)\n\n        strategy_df['Cumulative Return'] = (1 + strategy_df['Strategy Return']).cumprod() - 1\n        strategy_cum_return = strategy_df['Cumulative Return'].iloc[-1]\n\n        if strategy_cum_return &gt; best_cum_return:\n            best_cum_return = strategy_cum_return\n            best_params = (window, std_dev)\n            best_strategy_df = strategy_df.copy()\n\n    print(f\"Best Params (Bollinger Bands): Window={best_params[0]}, Std Dev={best_params[1]}\")\n\n    best_strategy_df['Date'] = df.index\n    best_strategy_df['Indicator'] = 'Bollinger Bands'\n    bb_signals_df = best_strategy_df[['Date', 'Signal', 'Indicator']]\n\n    return bb_signals_df\n\nwindow_range = range(10, 21)\nstd_dev_range = np.arange(1, 3.5, 0.5)\n\nbb_signals_df = optimize_and_calculate_bollinger_bands(df=df, window_range=window_range, std_dev_range=std_dev_range)\nta_rating_df = pd.concat([ta_rating_df, bb_signals_df], ignore_index=True)\n\nFonksiyondan alınan aşağıdaki kod bloğunu inceleyelim.\n\nbb = volatility.BollingerBands(close=strategy_df['Close'], window=window, window_dev=std_dev)\nbb_lband = bb.bollinger_lband()\nbb_hband = bb.bollinger_hband()\n\nfor i in range(len(strategy_df)):\n    lband = bb_lband.iloc[i]\n    hband = bb_hband.iloc[i]\n    close_price = strategy_df['Close'].iloc[i]\n\n    if close_price &lt; lband:\n        signal = 1\n    elif close_price &gt; hband:\n        signal = -1\n    else:\n        signal = 0\n\nBu kod bloğu, indikatörün hesaplanmasını ve buna göre alım-satım sinyallerinin üretilmesini sağlar. İlk olarak, kapanış fiyatları verisiyle birlikte belirli bir pencere boyutu ve standart sapma kullanılarak bir Bollinger Bands nesnesi oluşturulur. Daha sonra, alt bant ve üst bant değerleri hesaplanır. Alt bant, hareketli ortalama fiyatın belirli bir standart sapma kadar altında, üst bant ise hareketli ortalama fiyatın belirli bir standart sapma kadar üstünde yer alır. Son olarak, her bir veri noktası için kapanış fiyatı, alt bant ve üst bant değerleri karşılaştırılarak alım-satım sinyali üretilir. Eğer kapanış fiyatı alt banttan daha düşükse alım sinyali üretilir. Eğer kapanış fiyatı üst banttan daha yüksekse satım sinyali üretilir. Eğer kapanış fiyatı alt bant ve üst bant arasındaysa herhangi bir sinyal üretilmez. Bu şekilde, Bollinger Bands indikatörü kullanılarak belirli bir varlığın fiyat hareketleriyle ilgili alım-satım kararları otomatik olarak belirlenmiş olur.\nBollinger Bands stratejisinin optimize edilmesi için kullanılan parametre aralıkları şöyledir:\n\nwindow_range: Hareketli ortalama pencere boyutu aralığıdır. Değerler 10 ile 20 arasında 1’er birim artacak şekilde belirlenmiştir.\nstd_dev_range: Standart sapma değer aralığıdır. Değerler 1 ile 3.5 arasında 0.5’er birim artacak şekilde belirlenmiştir.\n\nÖrneğe göre en iyi kümülatif getiriyi sağlayan parametreler şunlardır:\nBest Params (Bollinger Bands): Window=10, Std Dev=2.5\n\n\nRate of Change\nRate of Change, belirli bir zaman diliminde fiyatların yüzde değişimini ölçerek fiyatların momentumunu gösteren bir indikatördür.\nRate of Change için seçilen strateji, indikatörü kullanarak alım-satım sinyalleri üretir ve bu sinyalleri optimize ederek en iyi kümülatif getiriyi hedefler. Belirli bir pencere boyutu ve aşırı alım/aşırı satım seviyeleri aralığı ile fiyatın geçmiş performansını ölçer. ROC değeri, belirli bir dönemde fiyatın yüzde değişimini gösterir. Ardından, aşırı alım veya aşırı satım seviyelerini geçtiğinde alım veya satım sinyali üretir. Bu sinyallerle stratejinin getirisini hesaplar ve en iyi parametrelerle belirlenen sinyallerle işlem yapar.\n\ndef optimize_and_calculate_roc(df, window_range, overbought_range, oversold_range):\n    best_cum_return = -np.inf\n    best_params = None\n    best_strategy_df = None\n\n    for window, overbought, oversold in product(window_range, overbought_range, oversold_range):\n        strategy_df = df.copy().reset_index(drop=True)\n\n        roc = momentum.ROCIndicator(strategy_df['Close'], window=window).roc()\n        signal = np.where(roc &gt; overbought, -1, np.where(roc &lt; oversold, 1, 0))\n\n        strategy_df['Signal'] = signal\n        strategy_df['Signal Updated'] = update_signals(strategy_df['Signal'].tolist())\n\n        if not (1 in strategy_df['Signal Updated'].values and -1 in strategy_df['Signal Updated'].values):\n            continue\n\n        strategy_df_ret_calc = strategy_df.copy()\n        strategy_df_ret_calc = strategy_df_ret_calc[strategy_df_ret_calc['Signal Updated'] != 0]\n        strategy_df_ret_calc['Strategy Return'] = strategy_df_ret_calc['Close'].pct_change()\n\n        for index, row in strategy_df_ret_calc.iterrows():\n            signal = row['Signal Updated']\n            if signal == 1:\n                strategy_df_ret_calc.loc[index, 'Strategy Return'] *= -1\n\n        strategy_df_ret_calc = strategy_df_ret_calc[['Strategy Return']]\n        strategy_df = pd.merge(strategy_df, strategy_df_ret_calc, how='left', left_index=True, right_index=True)\n        strategy_df['Strategy Return'] = strategy_df['Strategy Return'].fillna(0)\n\n        strategy_df['Cumulative Return'] = (1 + strategy_df['Strategy Return']).cumprod() - 1\n        strategy_cum_return = strategy_df['Cumulative Return'].iloc[-1]\n\n        if strategy_cum_return &gt; best_cum_return:\n            best_cum_return = strategy_cum_return\n            best_params = (window, overbought, oversold)\n            best_strategy_df = strategy_df.copy()\n\n    print(f\"Best Params (Rate of Change): Window={best_params[0]}, Overbought={best_params[1]}, Oversold={best_params[2]}\")\n\n    best_strategy_df['Date'] = df.index\n    best_strategy_df['Indicator'] = 'Rate of Change'\n    roc_signals_df = best_strategy_df[['Date', 'Signal', 'Indicator']]\n\n    return roc_signals_df\n\nwindow_range_roc = range(5, 21, 5)\noverbought_range_roc = range(5, 31, 5)\noversold_range_roc = range(-5, -31, -5)\n\nroc_signals_df = optimize_and_calculate_roc(df=df, window_range=window_range_roc, overbought_range=overbought_range_roc, oversold_range=oversold_range_roc)\nta_rating_df = pd.concat([ta_rating_df, roc_signals_df], ignore_index=True)\n\nFonksiyondan alınan aşağıdaki kod bloğunu inceleyelim.\n\nroc = momentum.ROCIndicator(strategy_df['Close'], window=window).roc()\nsignal = np.where(roc &gt; overbought, -1, np.where(roc &lt; oversold, 1, 0))\n\nBu kod bloğu, indikatörün hesaplanmasını ve buna göre alım-satım sinyallerinin üretilmesini sağlar. İlk olarak, kapanış fiyatları verisiyle birlikte belirli bir pencere boyutu kullanılarak bir ROC nesnesi oluşturulur. Daha sonra, ROC değerleri belirlenen bir aşırı alım ve aşırı satım seviyeleri ile karşılaştırılarak alım-satım sinyalleri üretilir. Eğer ROC değeri aşırı alım seviyesini aşarsa satım sinyali üretilir. Eğer ROC değeri aşırı satım seviyesinin altına düşerse alım sinyali üretilir. Eğer ROC değeri aşırı alım veya aşırı satım seviyeleri arasındaysa herhangi bir sinyal üretilmez. Bu şekilde, ROC indikatörü kullanılarak belirli bir varlığın fiyat momentumuyla ilişkili alım-satım kararları otomatik olarak belirlenir.\nROC stratejisinin optimize edilmesi için kullanılan parametre aralıkları şöyledir:\n\nwindow_range: Pencere boyutu aralığıdır. Değerler 5 ile 20 arasında 5’er birim artacak şekilde belirlenmiştir.\noverbought_range: Aşırı alım seviyesinin belirlenmesi için kullanılan değerlerin aralığıdır. Değerler 5 ile 30 arasında 5’er birim artacak şekilde belirlenmiştir.\noversold_range: Aşırı satım seviyesinin belirlenmesi için kullanılan değerlerin aralığıdır. Değerler -5 ile -30 arasında 5’er birim azalacak şekilde belirlenmiştir.\n\nÖrneğe göre en iyi kümülatif getiriyi sağlayan parametreler şunlardır:\nBest Params (Rate of Change): Window=5, Overbought=5, Oversold=-15\n\n\nIchimoku\nIchimoku Kinko Hyo, genellikle kısaca Ichimoku olarak bilinen, çok yönlü bir teknik analiz indikatörüdür. Ichimoku, bir varlığın destek ve direnç seviyelerini, trend yönünü, momentumunu ve olası sinyalleri belirlemeye yardımcı olur. Ichimoku, tek bir grafikte beş ana bileşeni birleştirir ve bu da onu oldukça kapsamlı bir analiz aracı yapar.\nIchimoku için seçilen strateji, indikatörü kullanarak alım-satım sinyalleri üretir ve bu sinyalleri optimize ederek en iyi kümülatif getiriyi hedefler. Ichimoku Bulutu, beş çizgiden oluşur: Tenkan-sen, Kijun-sen, Senkou Span A, Senkou Span B ve Chikou Span. Bu stratejide, Tenkan-sen ve Kijun-sen çizgilerinin kesişimlerine dayalı alım-satım sinyalleri üretilir. Ayrıca, fiyatın Senkou Span A ve Senkou Span B arasında veya dışında olması durumuna göre de alım-satım sinyalleri üretilir. Bu sinyallerle stratejinin getirisini hesaplar ve en iyi parametrelerle belirlenen sinyallerle işlem yapar.\n\ndef optimize_and_calculate_ichimoku(df, tenkan_range, kijun_range, senkou_span_b_range):\n    best_cum_return = -np.inf\n    best_params = None\n    best_strategy_df = None\n\n    for tenkan, kijun, senkou_span_b in product(tenkan_range, kijun_range, senkou_span_b_range):\n        strategy_df = df.copy().reset_index(drop=True)\n\n        ichimoku = trend.IchimokuIndicator(strategy_df['High'], strategy_df['Low'], window1=tenkan, window2=kijun, window3=senkou_span_b)\n        strategy_df['Tenkan'] = ichimoku.ichimoku_conversion_line()\n        strategy_df['Kijun'] = ichimoku.ichimoku_base_line()\n        strategy_df['Senkou_A'] = ichimoku.ichimoku_a()\n        strategy_df['Senkou_B'] = ichimoku.ichimoku_b()\n        strategy_df['Lagging_Span'] = strategy_df['Close'].shift(-kijun)\n\n        signals = []\n\n        for i in range(len(strategy_df)):\n            price = strategy_df['Close'].iloc[i]\n            tenkan_sen = strategy_df['Tenkan'].iloc[i]\n            kijun_sen = strategy_df['Kijun'].iloc[i]\n            senkou_a = strategy_df['Senkou_A'].iloc[i]\n            senkou_b = strategy_df['Senkou_B'].iloc[i]\n\n            if price &gt; max(senkou_a, senkou_b):\n                if price &gt; tenkan_sen and tenkan_sen &gt; kijun_sen:\n                    signals.append(1)\n                else:\n                    signals.append(0)\n            elif price &lt; min(senkou_a, senkou_b):\n                if price &lt; tenkan_sen and tenkan_sen &lt; kijun_sen:\n                    signals.append(-1)\n                else:\n                    signals.append(0)\n            else:\n                signals.append(0)\n\n        strategy_df['Signal'] = signals\n        strategy_df['Signal Updated'] = update_signals(strategy_df['Signal'].tolist())\n\n        if not (1 in strategy_df['Signal Updated'].values and -1 in strategy_df['Signal Updated'].values):\n            continue\n\n        strategy_df_ret_calc = strategy_df.copy()\n        strategy_df_ret_calc = strategy_df_ret_calc[strategy_df_ret_calc['Signal Updated'] != 0]\n        strategy_df_ret_calc['Strategy Return'] = strategy_df_ret_calc['Close'].pct_change()\n\n        for index, row in strategy_df_ret_calc.iterrows():\n            signal = row['Signal Updated']\n            if signal == 1:\n                strategy_df_ret_calc.loc[index, 'Strategy Return'] *= -1\n\n        strategy_df_ret_calc = strategy_df_ret_calc[['Strategy Return']]\n        strategy_df = pd.merge(strategy_df, strategy_df_ret_calc, how='left', left_index=True, right_index=True)\n        strategy_df['Strategy Return'] = strategy_df['Strategy Return'].fillna(0)\n\n        strategy_df['Cumulative Return'] = (1 + strategy_df['Strategy Return']).cumprod() - 1\n        strategy_cum_return = strategy_df['Cumulative Return'].iloc[-1]\n\n        if strategy_cum_return &gt; best_cum_return:\n            best_cum_return = strategy_cum_return\n            best_params = (tenkan, kijun, senkou_span_b)\n            best_strategy_df = strategy_df.copy()\n\n    print(f\"Best Params (Ichimoku): Tenkan={best_params[0]}, Kijun={best_params[1]}, Senkou Span B={best_params[2]}\")\n\n    best_strategy_df['Date'] = df.index\n    best_strategy_df['Indicator'] = 'Ichimoku'\n    ichimoku_signals_df = best_strategy_df[['Date', 'Signal', 'Indicator']]\n\n    return ichimoku_signals_df\n\ntenkan_range = range(5, 10)\nkijun_range = range(20, 31, 5)\nsenkou_span_b_range = range(50, 101, 10)\n\nichimoku_signals_df = optimize_and_calculate_ichimoku(df=df, tenkan_range=tenkan_range, kijun_range=kijun_range, senkou_span_b_range=senkou_span_b_range)\nta_rating_df = pd.concat([ta_rating_df, ichimoku_signals_df], ignore_index=True)\n\nFonksiyondan alınan aşağıdaki kod bloğunu inceleyelim.\n\nichimoku = trend.IchimokuIndicator(strategy_df['High'], strategy_df['Low'], window1=tenkan, window2=kijun, window3=senkou_span_b)\nstrategy_df['Tenkan'] = ichimoku.ichimoku_conversion_line()\nstrategy_df['Kijun'] = ichimoku.ichimoku_base_line()\nstrategy_df['Senkou_A'] = ichimoku.ichimoku_a()\nstrategy_df['Senkou_B'] = ichimoku.ichimoku_b()\nstrategy_df['Lagging_Span'] = strategy_df['Close'].shift(-kijun)\n\nsignals = []\n\nfor i in range(len(strategy_df)):\n    price = strategy_df['Close'].iloc[i]\n    tenkan_sen = strategy_df['Tenkan'].iloc[i]\n    kijun_sen = strategy_df['Kijun'].iloc[i]\n    senkou_a = strategy_df['Senkou_A'].iloc[i]\n    senkou_b = strategy_df['Senkou_B'].iloc[i]\n\n    if price &gt; max(senkou_a, senkou_b):\n        if price &gt; tenkan_sen and tenkan_sen &gt; kijun_sen:\n            signals.append(1)\n        else:\n            signals.append(0)\n    elif price &lt; min(senkou_a, senkou_b):\n        if price &lt; tenkan_sen and tenkan_sen &lt; kijun_sen:\n            signals.append(-1)\n        else:\n            signals.append(0)\n    else:\n        signals.append(0)\n\nBu kod bloğu, indikatörün hesaplanmasını ve buna göre alım-satım sinyallerinin üretilmesini sağlar. İlk olarak, yüksek ve düşük fiyatlarla birlikte Tenkan Sen, Kijun Sen, Senkou Span A ve Senkou Span B gibi farklı bileşenlerin hesaplanması için bir Ichimoku nesnesi oluşturulur. Daha sonra, her bir bileşenin değeri, Ichimoku nesnesinin ilgili metotları kullanılarak hesaplanır ve strateji veri çerçevesine eklenir. Ayrıca, geriye kaydırılmış bir Lagging Span bileşeni de oluşturulur. Son olarak, her bir veri noktası için fiyat ve Ichimoku bileşenlerinin değerleri karşılaştırılarak alım-satım sinyalleri üretilir. Eğer fiyat, Senkou Span A ve Senkou Span B’nin maksimum değeriyle karşılaştırıldığında daha yüksekse ve Tenkan Sen, Kijun Sen’in üzerindeyse, alım sinyali üretilir. Eğer fiyat, Senkou Span A ve Senkou Span B’nin minimum değeriyle karşılaştırıldığında daha düşükse ve Tenkan Sen, Kijun Sen’in altındaysa, satım sinyali üretilir. Aksi takdirde, herhangi bir sinyal üretilmez. Bu şekilde, Ichimoku Kinko Hyo indikatörü kullanılarak belirli bir varlığın trendi ve potansiyel alım-satım fırsatları hakkında bilgi sağlanır ve alım-satım sinyalleri otomatik olarak belirlenir.\nIchimoku stratejisinin optimize edilmesi için kullanılan parametre aralıkları şöyledir:\n\ntenkan_range: Tenkan Sen (dönüş hattı) için kullanılacak dönem aralığıdır. Değerler 5 ile 9 arasında belirlenmiştir.\nkijun_range: Kijun Sen (standart hattı) için kullanılacak dönem aralığıdır. Değerler 20 ile 30 arasında 5’er birim artacak şekilde belirlenmiştir.\nsenkou_span_b_range: Senkou Span B (ön gösterge B) için kullanılacak dönem aralığıdır. Değerler 50 ile 100 arasında 10’ar birim artacak şekilde belirlenmiştir.\n\nÖrneğe göre en iyi kümülatif getiriyi sağlayan parametreler şunlardır:\nBest Params (Ichimoku): Tenkan=6, Kijun=20, Senkou Span B=50"
  },
  {
    "objectID": "posts/post_14/index.html#reytinglerin-hesaplanması",
    "href": "posts/post_14/index.html#reytinglerin-hesaplanması",
    "title": "İndikatör Ağırlıklarının Optimize Edilerek Teknik Reytinglerin Oluşturulması ve Al-Tut Stratejisinin Alt Edilmesi",
    "section": "Reytinglerin Hesaplanması",
    "text": "Reytinglerin Hesaplanması\n\nPandas DataFrame’in Güncellenmesi\nta_rating_df’i pivot_table yardımıyla aşağıdaki formata çevirelim ve yeni tabloyu pivot_ta_rating_df’e atayalım.\n\n\npivot_ta_rating_df = ta_rating_df.pivot_table(index='Date', columns='Indicator', values='Signal')\npivot_ta_rating_df.columns.name = None\n\n\n\n\nİndikatör Listesinin Oluşturulması\nDaha önce seçtiğimiz indikatörleri bir liste içerisine alalım.\n\nindicators = [\n    'Relative Strength Index',\n    'Weighted Moving Average',\n    'Moving Average Convergence Divergence',\n    'Average Directional Index',\n    'Bollinger Bands',\n    'Rate of Change',\n    'Ichimoku'\n]\n\n\n\nReyting Aralığının Oluşturulması\nAşağıdaki kriterleri signal_to_category adında bir fonksiyonda tanımlayalım.\n\n-1.0 \\(\\leq\\) reyting \\(\\leq\\) -0.5 ise Sat\n-0.5 \\(&lt;\\) reyting \\(&lt;\\) 0.5 ise Nötr\n0.5 \\(\\leq\\) reyting \\(\\leq\\) 1.0 ise Al\n\n\ndef signal_to_category(signal):\n    if -1.0 &lt;= signal &lt;= -0.5:\n        return -1\n    elif -0.5 &lt; signal &lt; 0.5:\n        return 0\n    elif 0.5 &lt;= signal &lt;= 1.0:\n        return 1\n    else:\n        return None\n\n\n\nİndikatör Ağırlıklarının Optimize Edilmesi\nTüm süreci optimize_weights_and_evaluate adındaki fonksiyonun içerisinde yürüteceğiz.\nFonksiyon, 5 adet parametre alıyor.\n\ndf: İlgili finansal varlığın (örneğimizde BTC-USD) tarihsel verilerinin bulunduğu df’tir.\npivot_df: Son oluşturulan pivot_ta_rating_df’tir.\nindicators: İndikatör listesidir.\ninitial_value: 100 birimin ne olduğuna bakılır. Parametrenin varsayılan değeri 100’dür.\nplot: Süreç tamamlandığında ekrana üç adet görsel basılır. İlk görsel, stratejinin nerelerde alım-satım sinyali ürettiğini gösterir. İkinci görsel, strateji ile al-tut stratejisini karşılaştırır. Üçüncü görsel ise indikatörlerin ağırlıklarını verir. Parametrenin varsayılan değeri False’tur.\n\n\ndef optimize_weights_and_evaluate(df, pivot_df, indicators, initial_value=100, plot=False):\n    results = []\n    best_cum_return = -np.inf\n    best_weights = None\n    best_strategy_df = None\n    buy_and_hold_cum_return = (df['Close'].pct_change().dropna() + 1).cumprod().iloc[-1] - 1\n\n    while best_cum_return &lt;= buy_and_hold_cum_return:\n        strategy_df = pivot_df.copy()\n\n        weights = (np.random.rand(len(indicators)) / np.random.rand(len(indicators)).sum())\n        weights = weights / weights.sum()\n        weight_dict = dict(zip(indicators, weights))\n        weighted_ratings = strategy_df.apply(lambda row: sum(row[indicator] * weight_dict[indicator] for indicator in indicators), axis=1)\n\n        strategy_df['Weighted Average Rating'] = weighted_ratings\n        strategy_df['Signal'] = strategy_df['Weighted Average Rating'].apply(signal_to_category)\n        strategy_df['Signal Updated'] = update_signals(strategy_df['Signal'].tolist())\n\n        if not any(signal in strategy_df['Signal Updated'].values for signal in [1, -1]):\n            continue\n\n        strategy_df['Close'] = df['Close'].reindex(strategy_df.index)\n        strategy_df = strategy_df.reset_index(drop=True)\n\n        strategy_df_ret_calc = strategy_df.copy()\n        strategy_df_ret_calc = strategy_df_ret_calc[strategy_df_ret_calc['Signal Updated'] != 0]\n        strategy_df_ret_calc['Strategy Return'] = strategy_df_ret_calc['Close'].pct_change()\n\n        for index, row in strategy_df_ret_calc.iterrows():\n            signal = row['Signal Updated']\n            if signal == 1:\n                strategy_df_ret_calc.loc[index, 'Strategy Return'] *= -1\n\n        strategy_df_ret_calc = strategy_df_ret_calc[['Strategy Return']]\n        strategy_df = pd.merge(strategy_df, strategy_df_ret_calc, how='left', left_index=True, right_index=True)\n        strategy_df['Strategy Return'] = strategy_df['Strategy Return'].fillna(0)\n        strategy_df.iloc[0, strategy_df.columns.get_loc('Strategy Return')] = np.nan\n\n        strategy_df['Buy and Hold Return'] = strategy_df['Close'].pct_change()\n\n        strategy_df['Strategy Cumulative Return'] = (1 + strategy_df['Strategy Return']).cumprod() - 1\n        strategy_df['Buy and Hold Cumulative Return'] = (1 + strategy_df['Buy and Hold Return']).cumprod() - 1\n\n        strategy_df.iloc[0, strategy_df.columns.get_loc('Strategy Cumulative Return')] = 0\n        strategy_df.iloc[0, strategy_df.columns.get_loc('Buy and Hold Cumulative Return')] = 0\n\n        strategy_df['Strategy Values'] = initial_value * (1 + strategy_df['Strategy Cumulative Return'])\n        strategy_df['Buy and Hold Values'] = initial_value * (1 + strategy_df['Buy and Hold Cumulative Return'])\n\n        strategy_df.index = df.index\n\n        non_neutral_df = strategy_df[strategy_df['Signal Updated'] != 0]\n        non_neutral_list = non_neutral_df['Signal Updated'].tolist()\n        strategy_cum_return = strategy_df['Strategy Cumulative Return'].iloc[-1]\n\n        results.append({\n            **weight_dict,\n            'Last Signal': non_neutral_list[-1],\n            'Strategy Cumulative Return': strategy_cum_return,\n            'Buy and Hold Cumulative Return': buy_and_hold_cum_return\n        })\n\n        if strategy_cum_return &gt; best_cum_return:\n            best_cum_return = strategy_cum_return\n            best_weights = weight_dict\n            best_strategy_df = strategy_df\n\n        if best_cum_return &gt; buy_and_hold_cum_return:\n            break\n\n    results_df = pd.DataFrame(results)\n\n    if plot:\n        plt.figure(figsize=(12, 6))\n        plt.plot(best_strategy_df.index, best_strategy_df['Close'], label='Close Price', color='black')\n\n        for date, row in best_strategy_df.iterrows():\n            signal = row['Signal Updated']\n            if signal == 1 or signal == -1:\n                plt.plot(\n                    date,\n                    row['Close'],\n                    marker='^' if signal == 1 else 'v',\n                    markersize=10,\n                    markerfacecolor='green' if signal == 1 else 'red',\n                    markeredgewidth=1,\n                    markeredgecolor='black'\n                )\n\n        plt.scatter([], [], marker='^', color='g', label='Buy Signal')\n        plt.scatter([], [], marker='v', color='r', label='Sell Signal')\n        plt.legend()\n\n        plt.title('Best Strategy Signals - Cumulative Return: {:.2%}'.format(best_cum_return))\n        plt.ylabel('Price')\n        plt.grid(True)\n        plt.show()\n\n        plt.figure(figsize=(12, 6))\n        plt.plot(\n            best_strategy_df.index,\n            best_strategy_df['Strategy Values'],\n            color='blue',\n            label='Strategy'\n        )\n        plt.plot(\n            best_strategy_df.index,\n            best_strategy_df['Buy and Hold Values'],\n            color='red',\n            label='Buy and Hold'\n        )\n        plt.axhline(y=100, color='black', linestyle='--', linewidth=2)\n        plt.title(f'Strategy vs. Buy and Hold Cumulative Returns\\nStrategy: {best_cum_return:.2%}, Buy and Hold: {buy_and_hold_cum_return:.2%}')\n        plt.legend()\n        plt.grid(True)\n        plt.show()\n\n        sorted_weights = {k: v for k, v in sorted(best_weights.items(), key=lambda item: item[1], reverse=True)}\n        indicators = list(sorted_weights.keys())\n        weights = list(sorted_weights.values())\n\n        plt.figure(figsize=(10, 6))\n        bars = plt.barh(indicators, weights, color='skyblue')\n        for bar, weight in zip(bars, weights):\n            plt.text(bar.get_width(), bar.get_y() + bar.get_height()/2, f'{weight:.3f}', va='center')\n        plt.xlabel('Weights')\n        plt.title('Optimized Weights for Indicators')\n        plt.gca().invert_yaxis()\n        plt.grid(axis='x')\n        plt.show()\n\n    return results_df, best_weights\n\noptimization_results, best_weights = optimize_weights_and_evaluate(df, pivot_ta_rating_df, indicators, plot=True)\n\nYukarıdaki fonksiyon, belirtilen teknik analiz indikatörleri için optimal ağırlıkları bulmak ve bu ağırlıkları kullanarak bir alım-satım stratejisini değerlendirmek amacıyla tasarlanmıştır. Fonksiyon, al-tut stratejisini alt edecek en iyi ağırlıkları bulana kadar çalışır. İşlem adımlarını özetlemek gerekirse:\n\nAğırlıkların Belirlenmesi: Rastgele ağırlıklar oluşturulur ve bunlar normalleştirilerek (toplamı 1 yapılarak) her bir indikatöre atanır.\nStratejinin Uygulanması: Her bir indikatör için ağırlıklı ortalama hesaplanır ve bu değerler belirlenen sinyal kategorilerine (sat, nötr, al) dönüştürülür. Sinyaller güncellenir ve bu sinyallerle strateji değerlendirilir.\nGetirilerin Hesaplanması: Sinyallere göre strateji getirisi ve al-tut getirisi hesaplanır. Her iki strateji için de kümülatif getiriler hesaplanır ve başlangıç değeri (örneğin, 100 birim) ile çarpılarak strateji ve al-tut için değerler belirlenir.\nEn İyi Ağırlıkların Bulunması: Her denemede bulunan strateji getirisi kaydedilir ve en yüksek getiriye sahip olan ağırlıklar en iyi ağırlıklar olarak belirlenir.\nSonuçların Görselleştirilmesi: En iyi strateji için getiriler ve sinyaller grafiklerle gösterilir. Ayrıca, en iyi ağırlıkların hangi indikatörler için olduğunu gösteren bir çubuk grafik oluşturulur.\n\nÖrneğe göre görsel çıktıları aşağıdaki gibidir.\n\n\n\noptimization_results çıktısına göre ilk denemede bulduğunu görebiliriz.\n\nEn uygun ağırlıkları görsel yerine best_weights çıktısında da görebiliriz.\n\nGelecek içeriklerde görüşmek dileğiyle."
  },
  {
    "objectID": "posts/post_3/index.html",
    "href": "posts/post_3/index.html",
    "title": "Anlık Veri Akışı ile Oy Öngörüsü",
    "section": "",
    "text": "Giriş\nSeçim öngörüleri için seçim anketlerini kullanabilir veya seçim sonucuna etkisi olan değişkenler ile modelleme yapabiliriz. Modellemeye seçim anketleri de dahil edilebilir pek tabi.\nYukarıdaki iki yönteme ek olarak, seçim akşamı gelen veri akışı da hangi adayın/partinin ne kadarlık bir oy alacağı noktasında öngörüde bulunmamıza yardımcı olabilir. Bu yöntem, diğer yöntemlere göre daha kısa vadelidir. Çünkü veriler seçim akşamı alınabilmektedir. Amacı ise daha akışın başında hem oy oranlarının gidişatını kontrol edebilmek hem de öngörüde bulunabilmektir. Peki, bu yöntemde başarılı bir öngörü nasıl olur? Bunu üç kritere bağlıyorum: Gerçek sonuca yakın, istikrarlı güncellemeye sahip ve öngörüsü olabildiğince erkenden yapılan. Nedenlerini açıklayayım.\n\nÖngörümüzün gerçek sonuca yakın olmasını isteriz. Gerçekten uzak bir öngörü kimseyi tatmin etmez.\nModel güncellenebilir ancak güncellenen modelin güven de vermesi gerekir. Oynaklık seviyesi yüksek bir güncelleme modele olan güveni düşürecektir.\nGerçeğe yakın bir öngörü veri akışının başında da yapılabilir sonunda da. Veri akışının başında yapılan gerçeğe yakın öngörü daha anlamlı olacaktır.\n\nYapacağımız uygulamada İstanbul ilini ve Anadolu Ajansı verilerini baz alacağız. Verileri 31 Mart 2024 akşamı izlediğim NOW kanalından aldım. Veri seti, açılan sandık oranı ve adayların oy oranları ile aralarındaki farkları içermektedir. aa_20240331_istanbul isimli JSON verisine buradan ulaşabilirsiniz.\n\n\nKullanılacak Kütüphaneler\n\nimport json\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nfrom scipy.stats import shapiro\nimport statsmodels.stats.diagnostic as dg\nimport matplotlib.pyplot as plt\n\n\n\nAçılan Sandık Oranlarına Göre Adayların Oy Oranları ve Aralarındaki Farklar\n\nwith open('aa_20240331_istanbul.json', 'r', encoding='utf-8') as file:\n    data = json.load(file)\n\ndf = pd.DataFrame(data['Data'])\n\nprint(df.head())\n\n\n\nplt.figure(figsize=(10, 6))\nplt.plot(\n    df['Açılan Sandık Oranı'],\n    df['Ekrem İmamoğlu'],\n    label='Ekrem İmamoğlu',\n    color='red',\n    linewidth=3\n)\nplt.plot(\n    df['Açılan Sandık Oranı'],\n    df['Murat Kurum'],\n    label='Murat Kurum',\n    color='orange',\n    linewidth=3\n)\nplt.xlabel('Açılan Sandık Oranı')\nplt.ylabel('Oy Oranı')\nplt.title('31 Mart 2024 Yerel Seçim - İstanbul')\nplt.text(\n    0.97,\n    -0.14,\n    'NOW kanalından alınan Anadolu Ajansı verileridir.',\n    color='gray',\n    fontsize=8,\n    fontstyle='italic',\n    ha='right',\n    transform=plt.gca().transAxes\n)\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\nAnadolu Ajansı geçmiş seçimlerde, gözlemlediğimiz üzere, veri akışını AKP ve ittifak ortaklarının adayları lehine başlatmıştır. 31 Mart 2024 yerel seçimlerinde ise CHP’nin adayı Ekrem İmamoğlu lehine başlamıştır. Anadolu Ajansı’nın geçmiş veri akışlarını göz önüne aldığımızda, bu başlangıç hem Ekrem İmamoğlu’nun seçimi kazanacağını hem de rakibi Murat Kurum’a fark atacağını göstermekteydi. Nitekim 11.55 puanlık fark ile öyle de oldu.\n\nplt.figure(figsize=(10, 6))\nplt.plot(\n    df['Açılan Sandık Oranı'],\n    df['Fark'],\n    color='blue',\n    linewidth=3\n)\nplt.xlabel('Açılan Sandık Oranı')\nplt.ylabel('Oy Oranı Farkı')\nplt.title('31 Mart 2024 Yerel Seçim - İstanbul')\nplt.text(\n    0.97,\n    -0.14,\n    'NOW kanalından alınan Anadolu Ajansı verileridir.\\nPozitif fark Ekrem İmamoğlu lehinedir.',\n    color='gray',\n    fontsize=8,\n    fontstyle='italic',\n    ha='right',\n    transform=plt.gca().transAxes\n)\nplt.grid(True)\nplt.show()\n\n\n\n\nModelin Geliştirilmesi\nVeri setimizde bulunan gözlem sayısı 50’dir ancak biz tamamını kullanmayacağız. Öngörülerimizi olabildiğince erken yapmalıyız. Bunun için açılan sandık oranının aşağı yukarı %50 olmasını bekleyebiliriz. Bu da 20 adet gözlem sayısına denk gelecektir. Oldukça küçük fakat kullanılamaz değil.\nÖngörü için regresyon modellerini kullanacağız.\n\ndependent_variable='Ekrem İmamoğlu'\nindependent_variable='Açılan Sandık Oranı'\nballot_box_rate=50\n\nmain_df = df[df[independent_variable] &lt;= ballot_box_rate][[independent_variable, dependent_variable]]\n\nRegresyon modelinde dependent_variable Ekrem İmamoğlu, independent_variable Açılan Sandık Oranı olacak. Burada Ekrem İmamoğlu oylarının açılan sandık oranlarına bağımlı olduğunu varsayıyoruz.\nBaşlamadan önce Ekrem İmamoğlu’nun oy grafiğini görelim.\n\nplt.figure(figsize=(10, 6))\nplt.plot(\n    df[independent_variable],\n    df[dependent_variable],\n    color='red',\n    linewidth=3\n)\nplt.axvline(x=main_df[independent_variable].iloc[-1], color='black', linestyle='--')\nplt.xlabel(f'{independent_variable}')\nplt.ylabel(f'{dependent_variable}')\nplt.title('31 Mart 2024 Yerel Seçim - İstanbul')\nplt.text(\n    0.97,\n    -0.14,\n    'NOW kanalından alınan Anadolu Ajansı verileridir.',\n    color='gray',\n    fontsize=8,\n    fontstyle='italic',\n    ha='right',\n    transform=plt.gca().transAxes\n)\nplt.grid(True)\nplt.show()\n\n\nGrafikte X ekseninde dikey bir çizgi bulunmaktadır. Bu çizginin sol tarafını kullanacak ve sağ tarafını hiç görmediğimizi varsayacağız.\nHer iki değişkenin de doğrusal olduğu regresyon modelini kurarak başlayalım.\n\ny = main_df[dependent_variable]\nX = main_df[independent_variable]\nX = sm.add_constant(X)\n\nmodel_linlin = sm.OLS(y, X).fit()\n\nprint(model_linlin.summary())\n\nTahminleri güven aralığı değerleri ile alalım.\n\nalpha=0.05\n\npredictions = model_linlin.get_prediction(X).summary_frame(alpha)\npredictions_mean = predictions['mean']\npredictions_lower = predictions['mean_ci_lower']\npredictions_upper = predictions['mean_ci_upper']\n\nHata terimlerini alalım.\n\nresiduals = model_linlin.resid\n\n\nSıfıra oldukça yakın p değerlerine sahip const kesme terimi ve Açılan Sandık Oranı değişkeni katsayılarının istatistiksel olarak anlamlı olduğunu söyleyebiliriz. Ayrıca, yaklaşık olarak %82’lik bir \\(R^2\\) yakaladık ki 0-1 ya da 0-100 aralığında değer aldığını düşünürsek bu iyi bir orandır. Regresyon modelindeki Açılan Sandık Oranı değişkeni sıfır olduğunda Ekrem İmamoğlu’nun oy oranı %49.3 olmaktadır. Buna ek olarak, Açılan Sandık Oranı değişkenindeki 1 puanlık artış Ekrem İmamoğlu oyunu ortalamada 0.0164 puan artırmaktadır.\n%95 güven aralığında gerçek ve tahmin edilen değerleri görelim.\n\nplt.figure(figsize=(10,6))\nplt.plot(X.iloc[:, 1], y, color='red', label='Gerçek')\nplt.plot(X.iloc[:, 1], predictions_mean, color='blue', label='Tahmin Edilen')\nplt.fill_between(X.iloc[:, 1], predictions_lower, predictions_upper, color='gray', alpha=0.3, label=f'%{100-alpha*100} Güven Aralığı')\nplt.xlabel(f'{independent_variable}')\nplt.ylabel(f'{dependent_variable}')\nplt.title('Lin-Lin Model')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\nModelimiz fena durmasa da varsayımları sağlayıp sağlamadığına bakalım.\nHata terimleri normal dağılmaktadır.\n\nsw_statistic, sw_p_value = shapiro(residuals)\nif sw_p_value &gt;= alpha:\n    print('Hata terimleri normal dağılıyor.')\nelse:\n    print('Hata terimleri normal dağılmıyor.')\n\nHata terimlerinde otokorelasyon bulunmamaktadır.\n\nbg_test_statistic, bg_p_value, _, _ = dg.acorr_breusch_godfrey(model_linlin, nlags=5)\nif bg_p_value &gt;= alpha:\n    print('Hata terimleri arasında otokorelasyon yoktur.')\nelse:\n    print('Hata terimleri arasında otokorelasyon vardır.')\n\nModelde, alpha değerini 0.01 aldığımızda değişen varyans bulunmamaktadır.\n\nbp_test_statistic, bp_p_value, _, _ = dg.het_breuschpagan(residuals, model_linlin.model.exog)\n\nif bp_p_value &lt;= 0.01: # alpha\n    print('Modelde değişen varyans vardır.')\nelse:\n    print('Modelde değişen varyans yoktur.')\n\nEkrem imamoğlu için oy öngörüsünde bulunalım.\n\nX_value = [1, 100]\n\nforecasts = model_linlin.get_prediction(X_value).summary_frame(alpha)\nforecasted_vote_mean = forecasts['mean']\nforecasted_vote_lower = forecasts['obs_ci_lower']\nforecasted_vote_upper = forecasts['obs_ci_upper']\nprint(f'Oy Öngörüsü: %{forecasted_vote_mean.item():.2f}\\nOy Öngörüsü Aralığı: %{forecasted_vote_lower.item():.2f} - %{forecasted_vote_upper.item():.2f}')\n\n%50.57 - %51.32 öngörü aralığında %50.95’lik bir oy öngörüsünde bulunabiliriz. Gerçek oy oranı %51.14’tür. Öngörü, gerçek oyun yaklaşık 0.19 puan kadar altında kalmıştır ancak fena olmadığını söyleyebiliriz. Bu öngörüyü açılan sandık oranı %49 iken yaptık.\nSonraki seçimlerde pratik bir şekilde kullanmak için buraya kadar yaptığımız modelleme işlemlerini bir fonksiyona dönüştürelim.\n\ndef modeling_and_forecasting(df, dependent_variable, independent_variable, ballot_box_rate=50, alphas=(0.01, 0.05, 0.1)):\n    main_df = df[df[independent_variable] &lt;= ballot_box_rate][[independent_variable, dependent_variable]]\n\n    y = main_df[dependent_variable]\n    X = main_df[independent_variable]\n    X = sm.add_constant(X)\n\n    model_linlin = sm.OLS(y, X).fit()\n\n    print(model_linlin.summary())\n\n    predictions = [model_linlin.get_prediction(X).summary_frame(alpha) for alpha in alphas]\n    predictions_mean = [pred['mean'] for pred in predictions]\n    predictions_lower = [pred['mean_ci_lower'] for pred in predictions]\n    predictions_upper = [pred['mean_ci_upper'] for pred in predictions]\n\n    residuals = model_linlin.resid\n\n    sw_statistic, sw_p_value = shapiro(residuals)\n    h_normal = ['Hata terimleri normal dağılıyor.' if sw_p_value &gt;= alpha else 'Hata terimleri normal dağılmıyor.' for alpha in alphas]\n\n    bg_test_statistic, bg_p_value, _, _ = dg.acorr_breusch_godfrey(model_linlin, nlags=5)\n    h_autocorrelation = ['Hata terimleri arasında otokorelasyon yoktur.' if bg_p_value &gt;= alpha else 'Hata terimleri arasında otokorelasyon vardır.' for alpha in alphas]\n\n    bp_test_statistic, bp_p_value, _, _ = dg.het_breuschpagan(residuals, model_linlin.model.exog)\n    h_heteroscedasticity = ['Modelde değişen varyans vardır.' if bp_p_value &lt;= alpha else 'Modelde değişen varyans yoktur.' for alpha in alphas]\n\n    assumptions_table = {\n        'Alfa': alphas,\n        'Hata Terimleri Normalliği': h_normal,\n        'Otokorelasyon': h_autocorrelation,\n        'Değişen varyans': h_heteroscedasticity\n    }\n\n    assumptions_table_df = pd.DataFrame(assumptions_table)\n\n    X_value = [1, 100]\n\n    forecasts = [model_linlin.get_prediction(X_value).summary_frame(alpha) for alpha in alphas]\n    forecasted_vote_mean = [forecast['mean'].item() for forecast in forecasts]\n    forecasted_vote_lower = [forecast['obs_ci_lower'].item() for forecast in forecasts]\n    forecasted_vote_upper = [forecast['obs_ci_upper'].item() for forecast in forecasts]\n\n    forecast_table = {\n        'Alfa': alphas,\n        'Oy Öngörüsü - Ortalama': forecasted_vote_mean,\n        'Oy Öngörüsü - Alt Sınır': forecasted_vote_lower,\n        'Oy Öngörüsü - Üst Sınır': forecasted_vote_upper\n    }\n\n    forecast_table_df = pd.DataFrame(forecast_table)\n\n    return assumptions_table_df, forecast_table_df\n\nassumptions_df, forecast_df = modeling_and_forecasting(\n    df,\n    dependent_variable='Ekrem İmamoğlu',\n    independent_variable='Açılan Sandık Oranı'\n)\n\n\n\nAçılan sandık oranı %60 olduğunda modeli kontrol etmeye başlayabiliriz. Bundan sonrasında %90’a kadar her yeni veri akışında modeli takip edeceğiz. Peki, nasıl?\nModelimizin bağımsız değişken tarafına kukla değişken ekleyeceğiz. Böylece modelde herhangi bir kırılım olup olmadığını inceleyeceğiz. Eğer kırılım varsa modeli kukla değişkenli kuracağız. Bunun yanında, RMSE (Root Mean Squared Error, Kök Ortalama Kare Hatası) değerlerini de hesaplayacağız.\n\\(\\mathrm{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\\)\n\n# Aşağıda pas geçilen varsayımlar bir önceki fonksiyonda olduğu gibi eklenebilir.\n\ndef update_modeling_and_forecasting(df, ballot_box_rate, alpha=0.1):\n\n    updated_df = df[df['Açılan Sandık Oranı'] &lt;= ballot_box_rate][['Açılan Sandık Oranı', 'Ekrem İmamoğlu']]\n\n    updated_df['Dummy'] = [0 if i &lt; 20 else 1 for i in range(len(updated_df))]\n\n    y = updated_df['Ekrem İmamoğlu']\n    X = updated_df[['Açılan Sandık Oranı','Dummy']]\n    X = sm.add_constant(X)\n\n    model_linlin = sm.OLS(y, X).fit()\n\n    dummy_p_value = model_linlin.pvalues['Dummy']\n    if dummy_p_value &lt;= alpha:\n        X_value = [1, 100, 1]\n        dummy_significance = 'Modele Dahil Edildi'\n    else:\n        X = X.drop(columns=['Dummy'])\n        model_linlin = sm.OLS(y, X).fit()\n        X_value = [1, 100]\n        dummy_significance = 'Modele Dahil Edilmedi'\n\n    forecasts = model_linlin.get_prediction(X_value).summary_frame(alpha)\n    forecasted_vote = forecasts['mean'].iloc[0]\n    forecasted_vote_lower = forecasts['obs_ci_lower'].iloc[0]\n    forecasted_vote_upper = forecasts['obs_ci_upper'].iloc[0]\n    rmse = np.sqrt(np.mean((y - model_linlin.predict(X))**2))\n\n    return forecasted_vote, forecasted_vote_lower, forecasted_vote_upper, dummy_significance, rmse\n\nmin_ballot_box_rate=60\nmax_ballot_box_rate=90\nballot_box_rate = df[(df['Açılan Sandık Oranı'] &gt;= min_ballot_box_rate) & (df['Açılan Sandık Oranı'] &lt;= max_ballot_box_rate)]['Açılan Sandık Oranı'].tolist()\n\nforecast_dfs = []\n\nfor turnout in ballot_box_rate:\n\n    forecasted_vote, forecasted_vote_lower, forecasted_vote_upper,  dummy_significance, rmse = update_modeling_and_forecasting(df, turnout)\n\n    forecast_df = pd.DataFrame({\n        'Açılan Sandık Oranı': [turnout],\n        'Oy Öngörüsü - Ortalama': [forecasted_vote],\n        'Oy Öngörüsü - Alt Sınır': [forecasted_vote_lower],\n        'Oy Öngörüsü - Üst Sınır': [forecasted_vote_upper],\n        'Kukla Değişken': [dummy_significance],\n        'RMSE': [rmse]\n    })\n\n    forecast_dfs.append(forecast_df)\n\nforecast_df = pd.concat(forecast_dfs, ignore_index=True)\n\nprint(forecast_df)\n\nAçılan sandık oranı %83.1 olduğunda istatistiksel olarak anlamlı bir kırılım yaşandığını görüyoruz. Bu noktada öngörülerimizi kukla değişkenli yapıyoruz. Açılan sandık oranı %60.4 iken 0.098 olan RMSE değerini açılan sandık oranı %83.1’e geldiğinde 0.076’ya düşürüyoruz. Bu model ile daha önce yapılan öngörüyü %50.84’e revize edebiliriz. Gerçek oy oranı %51.14’tür. Öngörü, gerçek oyun yaklaşık 0.3 puan kadar altında kalmıştır.\n\nGelecek içeriklerde görüşmek dileğiyle."
  },
  {
    "objectID": "posts/post_5/index.html",
    "href": "posts/post_5/index.html",
    "title": "Partilere Göre Milletvekillerinin Ortalama Yüzü: Türkiye Örneği",
    "section": "",
    "text": "Giriş\n1800’lü yıllarda Francis Galton, belirli bir grup insanda ortak olan yüz özelliklerini görselleştirmek amacıyla birçok farklı bireyin yüz fotoğraflarını tek bir fotoğraf filmi üzerine yansıtmış ve bu yüzlerin kompozit görüntülerini oluşturmuştur.\nBugün benzer bir yaklaşım, belirli bir grup insanın -örneğin, milletvekilleri gibi- ortak yüz özelliklerini incelemek amacıyla modern tekniklerle gerçekleştirilebilir.\nBu uygulamada, 28. dönem AKP, CHP, DEM ve MHP milletvekillerinin fotoğraflarını kullanarak parti ve cinsiyet kategorisinde ortalama bir yüz yaratacağız. Uygulamayı GitHub’ta bulunan şu repo yardımı ile yapacağız. Uygulamanın veri seti olan fotoğraflara ise burada bulunan tbmm_28 isimli klasör ile ulaşabilirsiniz.\n\n\nReponun Klonlanması\nÖncelikle repoyu proje klasörümüze klonlayalım.\n\ngit clone https://github.com/johnwmillr/facer.git Facer\n\nKlonlama işleminden sonra bulunduğumuz dizinde sadece Facer klasöründeki facer klasörünü bırakabiliriz. facer klasörünün içinde ise facer.py ve utils.py dosyaları kalabilir.\nYukarıdaki işlemden sonra aşağıdaki gibi zip’li dosyayı indirip unzip’liyor ve dosyayı bulunduğumuz dizinde açtığımız model isimli klasöre taşıyoruz.\n\ncurl -O http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\nbunzip2 shape_predictor_68_face_landmarks.dat.bz2\n\nmkdir model\nmv shape_predictor_68_face_landmarks.dat model\n\nTBMM’nin web sitesinden alınan görseller .jpe formatında olduğu için facer klasöründe bulunan facer.py dosyasındaki glob_image_files() fonksiyonuna .jpe uzantısını ekliyoruz.\nOrtalama bir yüze ulaşmak için uygulanan adımlar burada detaylı bir şekilde veriliyor.\n\n\nKullanılacak Kütüphaneler\n\nfrom facer.facer import load_images, detect_face_landmarks, create_average_face\nimport matplotlib.pyplot as plt\nimport os\n\n\n\nOrtalama Yüzün Hesaplanması\n\nfolders = [\n    './tbmm_28/akp/kadin',\n    './tbmm_28/chp/kadin',\n    './tbmm_28/dem/kadin',\n    './tbmm_28/mhp/kadin',\n    './tbmm_28/akp/erkek',\n    './tbmm_28/chp/erkek',\n    './tbmm_28/dem/erkek',\n    './tbmm_28/mhp/erkek'\n]\n\nfor folder in folders:\n    images = load_images(folder)\n\n    landmarks, faces = detect_face_landmarks(images)\n    average_face = create_average_face(faces, landmarks, save_image=False)\n\n    gender = 'kadin' if 'kadin' in folder else 'erkek'\n    party = folder.split('/')[2]\n    file_name = f'{party}_{gender}.jpg'\n\n    plt.imshow(average_face)\n    plt.axis('off')\n    plt.savefig(os.path.join('imgs', file_name))\n    plt.show()\n\n\nGelecek içeriklerde görüşmek dileğiyle."
  },
  {
    "objectID": "posts/post_7/index.html",
    "href": "posts/post_7/index.html",
    "title": "Reddit Başlıklarının Duygu Analizi: r/worldnews Örneği",
    "section": "",
    "text": "Giriş\nReddit platformundan verileri çekebilmek için öncelikle buradan bir uygulama oluşturarak OAuth2 anahtarlarını almamız gerekiyor ki API’a ulaşabilelim.\nAdım 1. are you a developer? create an app... butonuna tıklayalım.\nAdım 2. name alanına kullanıcı ismimizi yazalım, script’i seçelim ve redirect uri alanına http://localhost:8080 bilgisini girelim.\nAdım 3. create app butonuna tıklayalım.\nBize verilen personal use script ve secret bilgilerini kullanacağız.\n\n\n\nKullanılacak Kütüphaneler\n\nimport pandas as pd\nimport datetime as dt\nimport praw\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nimport matplotlib.pyplot as plt\n\n\n\nSubreddit Başlıklarının Çekilmesi\npraw kütüphanesini kullanarak Reddit API’ına erişim sağlıyoruz.\n\nreddit = praw.Reddit(\n    client_id='personel_use_script',\n    client_secret='secret',\n    user_agent='username'\n)\n\ntopics_dict isimli bir sözlük oluşturalım. Bu sözlük çekmek istediğimiz bilgileri içerecek.\n\ntopics_dict = {\n    'id':[],\n    'title':[],\n    'score':[],\n    'comms_num':[], # kullanmayacağız ama kalsın\n    'created':[]\n}\n\nVerileri r/worldnews isimli subreddit’ten çekeceğiz ve sözlüğü veri çerçevesine dönüştüreceğiz.\n\nfor submission in reddit.subreddit('worldnews').new(limit=None):\n    topics_dict['id'].append(submission.id)\n    topics_dict['title'].append(submission.title)\n    topics_dict['score'].append(submission.score)\n    topics_dict['comms_num'].append(submission.num_comments)\n    topics_dict['created'].append(submission.created)\n\ntopics_df = pd.DataFrame(topics_dict)\n\n\nTarih ve saat bilgisi timestamp formatındaki created kolonunda yer almaktadır. Bunu datetime formatına dönüştürmemiz gerekiyor.\n\ndef timestamp_to_datetime(created):\n    return dt.datetime.fromtimestamp(created)\n\ntopics_df['datetime'] = topics_df['created'].apply(timestamp_to_datetime)\n\n\n\n\nDuygu Skorlarının Elde Edilmesi ve Verilerin Görselleştirilmesi\n\nsia = SIA()\nresults = []\n\nfor datetime, line, score in zip(topics_df['datetime'], topics_df['title'], topics_df['score']):\n    pol_score = sia.polarity_scores(line)\n    pol_score['datetime'] = datetime\n    pol_score['headline'] = line\n    pol_score['score'] = score\n    results.append(pol_score)\n\nresults_df = pd.DataFrame.from_records(results)\n\n\nYukarıda, öncelikle SIA (SentimentIntensityAnalyzer) isimli duygu analizi aracını kullanabilmek için bir nesne oluşturduk. Sonrasında her bir başlığın duygu analizini yapmak için SIA’in polarity_scores metodunu kullandık. Bu metot, bir metnin duygusal içeriğini analiz eder ve dört farklı duygu ölçüsü verir: pos (olumlu), neg (olumsuz), neu (nötr) ve compound (bileşik, tüm duyguların birleşimi). Biz compound ile ilgileneceğiz. Tüm bilgileri daha önce boş bir liste olarak oluşturduğumuz results değişkenine gönderdik ve döngü bittikten sonra results listesini results_df isimli veri çerçevesine dönüştürdük.\nGörselleştirmeyi iki farklı şekilde yapabiliriz.\nBirincisi, Reddit skorları (beğeni) ile duygu skorlarını gösterebiliriz. Şu an gündemde olduğu için başlıklarda geçen Iran veya Israel için farklı bir renk tercih edebiliriz.\n\nplt.figure(figsize=(10,6))\nplt.scatter(results_df['compound'], results_df['score'], alpha=.1, color='gray', label='Other Headlines')\nfor index, row in results_df.iterrows():\n    if 'Iran' in row['headline'] or 'Israel' in row['headline']:\n        plt.scatter(row['compound'], row['score'], color='red')\nplt.xlabel('Compound Sentiment Score')\nplt.ylabel('Reddit Score')\nplt.title('r/worldnews: Sentiment Score vs. Reddit Score')\nplt.grid(True)\nplt.yscale('log')\nplt.legend(['Other Headlines', 'Headlines containing Iran or Israel'])\nplt.show()\n\n\nİkincisi, duygu skorlarının ortalamada nasıl değiştiğini bir zaman serisi olarak gösterebiliriz.\n\nresults_df['date'] = results_df['datetime'].dt.date\ndaily_avg_compound = results_df.groupby('date')['compound'].mean()\n\nplt.figure(figsize=(10,6))\nplt.plot(daily_avg_compound.index, daily_avg_compound, marker='o', markersize=8, color='r')\nplt.ylabel('Daily Average Compound Score')\nplt.title('r/worldnews: Daily Average Compound Score over Time')\nplt.grid(True)\nplt.show()\n\n\nGelecek içeriklerde görüşmek dileğiyle."
  },
  {
    "objectID": "posts/post_9/index.html",
    "href": "posts/post_9/index.html",
    "title": "TCMB/EVDS Verilerine Erişim (05/04/2024 Değişikliği Sonrası)",
    "section": "",
    "text": "Türkiye Cumhuriyet Merkez Bankası, yenievds@tcmb.gov.tr adresinden bir duyuru göndermişti. Mail almayanlar için bir kısmını aşağıda paylaşıyorum.\nÖnceki duyurumuzda EVDS nin web servis parametrelerinde güvenlik sebebiyle bir düzenleme yapılacağını belirtmiştik. URL adresinden giden “key” parametresinin artık http request header içinde gelmesi ve web servis kullanan uygulamalarınızda “key=xxxxxx” parametresini http request header olarak dönüştürmeniz gerektiği belirtilmişti. Bu düzenleme 05 Nisan 2024 Cuma saat 21:00 TSİ tarihinden itibaren geçerli olacaktır.\nURL adresinden gönderdiğimiz key parametresini artık headers içinden göndereceğiz."
  },
  {
    "objectID": "posts/post_9/index.html#tek-serili-örnek",
    "href": "posts/post_9/index.html#tek-serili-örnek",
    "title": "TCMB/EVDS Verilerine Erişim (05/04/2024 Değişikliği Sonrası)",
    "section": "Tek Serili Örnek",
    "text": "Tek Serili Örnek\n\napi_key = 'api_key'\n\nseries_code='TP.APIFON4' # TCMB Ağırlıklı Ortalama Fonlama Maliyeti\nstart_date='01-01-2011' # Başlangıç\nend_date='30-04-2024' # Bitiş\nfrequency='5' # Aylık\naggregationType='avg' # Ortalama\n\nparams = {\n    'series': series_code,\n    'startDate': start_date,\n    'endDate': end_date,\n    'frequency': frequency,\n    'aggregationTypes': aggregationType,\n    'type': 'json'\n}\n\nurl = f'https://evds2.tcmb.gov.tr/service/evds/{urlencode(params)}'\n\nresponse = requests.get(url=url, headers={'key': api_key})\n\n# print(response.request.headers)\n\nformatted_response = json.loads(response.content)\n\ndata = formatted_response['items']\ndf = pd.DataFrame(data)\n\ndf['Tarih'] = pd.to_datetime(df['Tarih']) + pd.offsets.Day(0)\ndf = df.drop(columns=['UNIXTIME'])\ndf.columns = ['Tarih', 'AOFM']\ndf['AOFM'] = pd.to_numeric(df['AOFM'])"
  },
  {
    "objectID": "posts/post_9/index.html#çok-serili-örnek",
    "href": "posts/post_9/index.html#çok-serili-örnek",
    "title": "TCMB/EVDS Verilerine Erişim (05/04/2024 Değişikliği Sonrası)",
    "section": "Çok Serili Örnek",
    "text": "Çok Serili Örnek\nEğer birden fazla seri ile çalışmak istiyorsak, seriler arasında - olacak şekilde giriş yapacağız.\n\nseries_code='TP.BISTTLREF.DUSUK-TP.BISTTLREF.YUKSEK-TP.BISTTLREF.KAPANIS' # Türk Lirası Gecelik Referans Faiz Oranı\nstart_date='14-06-2019' # Başlangıç\nend_date='30-04-2024' # Bitiş\nfrequency='5' # Aylık\n\nparams = {\n    'series': series_code,\n    'startDate': start_date,\n    'endDate': end_date,\n    'frequency': frequency,\n    'type': 'json'\n}\n\nurl = f'https://evds2.tcmb.gov.tr/service/evds/{urlencode(params)}'\n\nresponse = requests.get(url=url, headers={'key': api_key})\n\n# print(response.request.headers)\n\nformatted_response = json.loads(response.content)\n\ndata = formatted_response['items']\ndf = pd.DataFrame(data)\n\ndf['Tarih'] = pd.to_datetime(df['Tarih']) + pd.offsets.Day(0)\ndf = df.drop(columns=['UNIXTIME'])\nnumeric_columns = df.columns[df.columns != 'Tarih']\ndf[numeric_columns] = df[numeric_columns].apply(pd.to_numeric)\n\nGelecek içeriklerde görüşmek dileğiyle."
  }
]